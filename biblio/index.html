<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Bibliography - Deep Learning Systems: Algorithms, Compilers, and Processors for Large-Scale Production</title>
    <link href="../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../js/jquery-3.2.1.min.js"></script>
    <script src="../js/bootstrap-3.3.7.min.js"></script>
    <script src="../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Bibliography", url: "#_top", children: [
          ]},
        ];

    </script>
    <script src="../js/base.js"></script>
      <script src="../javascripts/config.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../bio/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../bio/" class="btn btn-xs btn-link">
        Author's Biography
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../ch10/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../ch10/" class="btn btn-xs btn-link">
        Chapter 10: Opportunities and Challenges
      </a>
    </div>
    
  </div>

    

    <h1 id="bibliography">Bibliography</h1>
<div id="abadi2016"></div>
<p>[ABC+16] M. Abadi, P. Barham, J. Chen, et al.
<a href="https://research.google/pubs/pub45381/">TensorFlow: a system for large-scale machine learning</a>.
<em>OSDI</em>, 2016.</p>
<div id="abadi2016-b"></div>
<p>[ACG+16] M. Abadi, A. Chu, I. Goodfellow, H. McMahan, I. Mironov, K. Talwar, and L. Zhang.
<a href="https://dl.acm.org/doi/abs/10.1145/2976749.2978318">Deep learning with differential privacy</a>.
<em>CCS</em>, Oct. 2016.</p>
<div id="abadi2016-c"></div>
<p>[AA16] M. Abadi and D. Andersen.
<a href="https://arxiv.org/abs/1610.06918">Learning to protect communications with adversarial neural cryptography</a>.
Oct. 2016.</p>
<div id="adebayo2018"></div>
<p>[AGM+18] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and B. Kim.
<a href="https://arxiv.org/abs/1810.03292">Sanity checks for saliency maps</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="abts2020"></div>
<p>[ARS+20] D. Abts, J. Ross, J. Sparling, et al.
<a href="https://ieeexplore.ieee.org/document/9138986">Think fast: a tensor streaming processor (TSP) for accelerating deep learning workloads</a>.
<em>ISCA</em>, Jun. 2020.</p>
<div id="agrawal2019"></div>
<p>[AMP+19] A. Agrawal, A. Modi, A. Passos, et al.
<a href="https://arxiv.org/pdf/1903.01855.pdf">TensorFlow Eager: a multi-stage, Python-embedded DSL for machine learning</a> (<a href="https://www.akshayagrawal.com/files/tfe_talk.pdf">slides</a>).
<em>MLSys</em>, Feb. 2019.</p>
<div id="ahmed2019"></div>
<p>[AAB+19] Z. Ahmed, S. Amizadeh, M. Bilenko, et al.
<a href="https://arxiv.org/abs/1905.05715">Machine learning at Microsoft with ML .NET</a>.
<em>SIGKDD</em>, Jul. 2019.</p>
<div id="al-fares2008"></div>
<p>[ALV08] M. Al-Fares, A. Loukissas, and A. Vahdat.
<a href="https://dl.acm.org/doi/10.1145/1402946.1402967">A scalable, commodity data center network architecture</a>.
<em>SIGCOMM</em>, Oct. 2008.</p>
<div id="alibaba-ml2020"></div>
<p>[Ali20] Alibaba.
<a href="https://www.alibabacloud.com/product/machine-learning">Machine Learning Platform for AI</a>.
2020.</p>
<div id="alammar2018"></div>
<p>[Ala18] J. Alammar.
<a href="http://jalammar.github.io/illustrated-transformer/">The illustrated transformer</a>.
June 2018.</p>
<div id="alistarh2018"></div>
<p>[AHJ+18] D. Alistarh, T. Hoefler, M. Johansson, S. Khirirat, N. Konstantinov, and C. Renggli.
<a href="https://arxiv.org/abs/1809.10505">The convergence of sparsified gradient methods</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="alvarez2015"></div>
<p>[AVG+15] L. Alvarez, L. Vilanova, M. Gonzalez, X. Martorell, N. Navarro, and E. Ayguade.
<a href="https://ieeexplore.ieee.org/document/6616543">Hardware-software coherence protocol for the coexistence of caches and local memories</a>.
<em>TC</em>, Jan. 2015.</p>
<div id="aws-inf12019"></div>
<p>[Ama19] Amazon.
<a href="https://aws.amazon.com/ec2/instance-types/inf1/">EC2 Inf1 Instances</a>.
2019.</p>
<div id="aws-reinvent2019"></div>
<p>[Ama19b] Amazon.
<a href="https://www.youtube.com/watch?v=17r1EapAxpk">AWS re:Invent 2019: deliver high performance ML inference with AWS Inferentia</a>.
Dec. 2019.</p>
<div id="amazon-sagemaker2020"></div>
<p>[Ama20] Amazon.
<a href="https://aws.amazon.com/sagemaker/">SageMaker</a>.
2020.</p>
<div id="amdahl1967"></div>
<p>[Amd67] G. Amdahl.
<a href="https://dl.acm.org/doi/10.1145/1465482.1465560">Validity of the single processor approach to achieving large scale computing capabilities</a>.
<em>AFIPS</em>, Apr. 1967.</p>
<div id="amd-epyc2019"></div>
<p>[Amd19] Amd.
<a href="https://www.amd.com/en/products/cpu/amd-epyc-7742">EPYC 7742</a>.
2019.</p>
<div id="amodei2015"></div>
<p>[AAB+15] D. Amodei, R. Anubhai, E. Battenberg, et al.
<a href="https://arxiv.org/abs/1512.02595">Deep Speech 2: end-to-end speech recognition in English and Mandarin</a>.
<em>ICML</em>, Dec. 2015.</p>
<div id="amodei2016"></div>
<p>[AC16] D. Amodei and J. Clark.
<a href="https://openai.com/blog/faulty-reward-functions/">Faulty reward functions in the wild</a>.
<em>OpenAI</em>, Dec. 2016.</p>
<div id="amodei2018"></div>
<p>[DH18] A. Dario and D. Hernandez.
<a href="https://openai.com/blog/ai-and-compute/">AI and compute</a>.
<em>OpenAI</em>, May 2018.</p>
<div id="antoniou2019"></div>
<p>[AES19] A. Antoniou, H. Edwards, and A. Storkey.
<a href="https://arxiv.org/abs/1810.09502">How to train your MAML</a>.
<em>ICLR</em>, Mar. 2019.</p>
<div id="arik2019"></div>
<p>[AP19] S. Arik and T. Pfister.
<a href="https://arxiv.org/abs/1902.06292">ProtoAttend: attention-based prototypical learning</a>.
Sep. 2019.</p>
<div id="arivazhagan2019"></div>
<p>[ABF+19] N. Arivazhagan, A. Bapna, O. Firat, et al.
<a href="https://arxiv.org/abs/1907.05019">Massively multilingual neural machine translation in the wild: findings and challenges</a>.
July 2019.</p>
<div id="arjovsky2017"></div>
<p>[ACB17] M. Arjovsky, S. Chintala, and L. Bottou.
<a href="https://arxiv.org/abs/1701.07875">Wasserstein GAN</a>.
Jan. 2017.</p>
<div id="ashby2011"></div>
<p>[ADC11] T. Ashby, P. Diaz, and M. Cintra.
<a href="https://ieeexplore.ieee.org/document/5492677">Software-based cache coherence with hardware-assisted selective self-invalidations using Bloom filters</a>.
<em>TC</em>, Apr. 2011.</p>
<div id="ashkiani2018"></div>
<p>[AFO18] S. Ashkiani, M. Farach-Colton, and J. Owens.
<a href="https://arxiv.org/abs/1710.11246">A dynamic hash table for the GPU</a>.
<em>IPDPS</em>, May 2018.</p>
<div id="athalye2018"></div>
<p>[ACW18] A. Athalye, N. Carlini, and D. Wagner.
<a href="https://arxiv.org/abs/1802.00420">Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples</a>.
<em>ICML</em>, Jul. 2018.</p>
<div id="ba2016"></div>
<p>[BKH16] J. Ba, J. Kiros, and G. Hinton.
<a href="https://arxiv.org/abs/1607.06450">Layer normalization</a>.
July 2016.</p>
<div id="bacoyannis2018"></div>
<p>[BGJ+18] V. Bacoyannis, V. Glukhov, T. Jin, J. Kochems, and D. Song.
<a href="https://arxiv.org/abs/1811.09549">Idiosyncrasies and challenges of data driven learning in electronic trading</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="baidu-kunlun2020"></div>
<p>[Bai20] Baidu.
<a href="https://cloud.baidu.com/">Kunlun</a>.
2020.</p>
<div id="bai2018"></div>
<p>[BKK18] S. Bai, J. Kolter, and V. Koltun.
<a href="https://arxiv.org/abs/1803.01271">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</a>.
Mar. 2018.</p>
<div id="bai2019"></div>
<p>[BKK19] S. Bai, J. Kolter, and V. Koltun.
<a href="https://arxiv.org/abs/1909.01377">Deep equilibrium models</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="bay2006"></div>
<p>[BTV06] H. Bay, T. Tuytelaars, and L. Van Gool.
<a href="https://www.vision.ee.ethz.ch/ surf/eccv06.pdf">SURF: speeded up robust features</a>.
<em>ECCV</em>, 2006.</p>
<div id="balaprakash2019"></div>
<p>[BES+19] P. Balaprakash, R. Egele, M. Salim, V. Vishwanath, F. Xia, T. Brettin, and R. Stevens.
<a href="https://arxiv.org/abs/1909.00311">Scalable reinforcement learning based neural architecture search for cancer deep learning research</a>.
<em>SC</em>, Nov. 2019.</p>
<div id="balunovic2020"></div>
<p>[BV20] M. Balunovic and M. Vechev.
<a href="https://openreview.net/forum?id=SJxSDxrKDr">Adversarial training and provable defenses: bridging the gap</a>.
<em>ICLR</em>, Feb. 2020.</p>
<div id="barroso2018"></div>
<p>[BHR18] L. Barroso, U. Holze, and P. Ranganathan.
<a href="https://www.morganclaypool.com/doi/abs/10.2200/S00874ED3V01Y201809CAC046">The datacenter as a computer: designing warehouse-scale machines</a>.
<em>M\&amp;C</em>, Oct. 2018.</p>
<div id="belletti2019"></div>
<p>[BLK+19] F. Belletti, K. Lakshmanan, W. Krichene, et al.
<a href="https://arxiv.org/abs/1905.09874">Scaling up collaborative filtering data sets through randomized fractal expansions</a>.
Apr. 2019.</p>
<div id="bengio2012"></div>
<p>[Ben12] Y. Bengio.
<a href="https://arxiv.org/abs/1206.5533">Practical recommendations for gradient-based training of deep architectures</a>.
<em>NNs: Tricks of the Trade</em>, Sep. 2012.</p>
<div id="berner2019"></div>
<p>[BBC+19] C. Berner, G. Brockman, B. Chan, et al.
<a href="https://arxiv.org/abs/1912.06680">Dota 2 with large scale deep reinforcement learning</a>.
Dec. 2019.</p>
<div id="berg2019"></div>
<p>[BCC+19] D. Berg, R. Chirravuri, R. Cledat, S. Goyal, F. Hamad, and V. Tuulos.
<a href="https://netflixtechblog.com/open-sourcing-metaflow-a-human-centric-framework-for-data-science-fa72e04a5d9">Open-sourcing Metaflow, a human-centric framework for data science</a>.
<em>Netflix Tech Blog</em>, Dec. 2019.</p>
<div id="ray2019"></div>
<p>[Ber19] Berkeley.
<a href="https://rise.cs.berkeley.edu/projects/ray/">Ray</a>.
2019.</p>
<div id="binkowski2020"></div>
<p>[BDD+20] M. Binkowski, J. Donahue, S. Dieleman, et al.
<a href="https://openreview.net/forum?id=r1gfQgSFDr">High fidelity speech synthesis with adversarial networks</a>.
<em>ICLR</em>, Apr. 2020.</p>
<div id="blanchard2020"></div>
<p>[BHH20] P. Blanchard, D. Higham, and N. Higham
<a href="https://academic.oup.com/imajna/advance-article/doi/10.1093/imanum/draa038/5893596">Accurately computing the log-sum-exp and softmax functions</a>.
<em>J. Num. Analysis</em>, Aug. 2020.</p>
<div id="blundell2015"></div>
<p>[BCK+15] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra.
<a href="https://arxiv.org/abs/1505.05424">Weight uncertainty in neural networks</a>.
<em>ICML</em>, July 2015.</p>
<div id="bolukbasi2016"></div>
<p>[BCZ+16] T. Bolukbasi, K. Chang, J. Zou, V. Saligrama, and A. Kalai.
<a href="https://papers.nips.cc/book/advances-in-neural-information-processing-systems-29-2016">Man is to computer programmer as woman is to homemaker? Debiasing word embeddings</a>.
<em>NeurIPS</em>, Dec. 2016.</p>
<div id="bonawitz2017"></div>
<p>[BIK+17] K. Bonawitz, V. Ivanov, B. Kreuter, et al.
<a href="https://dl.acm.org/doi/10.1145/3133956.3133982">Practical secure aggregation for privacy-preserving machine learning</a>.
<em>CCS</em>, Oct. 2017.</p>
<div id="bondhugula2008"></div>
<p>[BHR+08] U. Bondhugula, A. Hartono, J. Ramanujam, and P. Sadayappan.
<a href="https://dl.acm.org/doi/10.1145/1379022.1375595">A practical automatic polyhedral parallelizer and locality optimizer</a>.
<em>SIGPLAN</em>, June 2008.</p>
<div id="bondhugula2016"></div>
<p>[BAC16] U. Bondhugula, A. Acharya, and A. Cohen.
<a href="https://dl.acm.org/doi/10.1145/2896389">The Pluto+ algorithm: A practical approach for parallelization and locality optimization of affine loop nests</a>.
<em>TOPLAS</em>, Apr. 2016.</p>
<div id="botev2017"></div>
<p>[BLB17] A. Botev, G. Lever, and D. Barber.
<a href="https://ieeexplore.ieee.org/document/7966082">Nesterov's accelerated gradient and momentum as approximations to regularised update descent</a>.
<em>IJCNN</em>, Jul. 2017.</p>
<div id="boyd2018"></div>
<p>[BCD+18] T. Boyd, Y. Cao, S. Das, T. Joerg, and J. Lebar.
<a href="https://medium.com/tensorflow/pushing-the-limits-of-gpu-performance-with-xla-53559db8e473">Pushing the limits of GPU performance with XLA</a>.
Nov. 2018.</p>
<div id="bromley1993"></div>
<p>[BGL+93] J. Bromley, I. Guyon, Y. LeCun, E. Sackinger, and R. Shah.
<a href="https://dl.acm.org/doi/10.5555/2987189.2987282">Signature verification using a ``Siamese'' time delay neural network</a>.
<em>NeurIPS</em>, Dec. 1993.</p>
<div id="brovman2019"></div>
<p>[Bro19] Y. Brovman.
<a href="https://tech.ebayinc.com/engineering/complementary-item-recommendations-at-ebay-scale/">Complementary item recommendations at eBay scale</a>.
Feb. 2019.</p>
<div id="brown2020"></div>
<p>[BMR+20] T. Brown, B. Mann, N. Ryder, M. Subbiah, et al.
<a href="https://arxiv.org/abs/2005.14165">Language models are few-shot learners</a>.
May 2020.</p>
<div id="bucila2006"></div>
<p>[BCN06] C. Bucila, R. Caruana, and A. Niculescu-Mizil.
<a href="https://www.cs.cornell.edu/ caruana/compression.kdd06.pdf">Model compression</a>.
<em>SIGKDD</em>, Aug. 2006.</p>
<div id="burda2018"></div>
<p>[BEP+18] Y. Burda, H. Edwards, D. Pathak, A. Storkey, T. Darrell, and A. Efros.
<a href="https://arxiv.org/abs/1808.04355">Large-scale study of curiosity-driven learning</a>.
Aug. 2018.</p>
<div id="cai2019"></div>
<p>[CZH19] H. Cai, L. Zhu, and S. Han.
<a href="https://arxiv.org/abs/1812.00332">ProxylessNAS: direct neural architecture search on target task and hardware</a>.
<em>ICLR</em>, Feb. 2019.</p>
<div id="cambier2020"></div>
<p>[CBG+20] L. Cambier, A. Bhiwandiwalla, T. Gong, O. H. Elibol, M. Nekuii, and H. Tang.
<a href="https://openreview.net/forum?id=Bkxe2AVtPS">Shifted and squeezed 8-bit floating point format for low-precision training of deep neural networks</a>.
<em>ICLR</em>, Jan. 2020.</p>
<div id="cao2018"></div>
<p>[HSW+18] Z. Cao, G. Hidalgo, T. Simon, S. Wei, and Y. Sheikh.
<a href="https://arxiv.org/abs/1812.08008">OpenPose: realtime multi-person 2D pose estimation using part affinity fields</a>.
<em>CVPR</em>, Dec. 2018.</p>
<div id="caspi2017"></div>
<p>[CLN+17] I. Caspi, G. Leibovich, G. Novik, and S. Endrawis.
<a href="https://github.com/NervanaSystems/coach">Reinforcement Learning Coach</a>.
Dec. 2017.</p>
<div id="castro2018"></div>
<p>[CMG+18] P. Castro, S. Moitra, C. Gelada, S. Kumar, and M. Bellemare.
<a href="https://github.com/google/dopamine">Dopamine: a research framework for deep reinforcement learning</a>.
Dec. 2018.</p>
<div id="chan2016"></div>
<p>[CJL+16] W. Chan, N. Jaitly, Q. Le, and O. Vinyals.
<a href="https://ieeexplore.ieee.org/document/7472621">Listen, attend and spell: a neural network for large vocabulary conversational speech recognition</a>.
<em>ICASSP</em>, 2016.</p>
<div id="chang2020"></div>
<p>[CFL20] O. Chang, L. Flokas, and H. Lipson.
<a href="https://openreview.net/forum?id=H1lma24tPB">Principled weight initialization for hypernetworks</a>.
<em>ICLR</em>, Feb. 2020.</p>
<div id="chaudhari2017"></div>
<p>[CCS+17] P. Chaudhari, A. Choromanska, S. Soatto, et al.
<a href="https://arxiv.org/abs/1611.01838">Entropy-SGD: biasing gradient descent into wide valleys</a>.
<em>ICLR</em>, Mar. 2017.</p>
<div id="chawla2011"></div>
<p>[CBH+11] N. Chawla, K. Bowyer, L. Hall, and W. Kegelmeyer.
<a href="https://arxiv.org/abs/1106.1813">SMOTE: synthetic minority over-sampling technique</a>.
<em>JAIR</em>, June 2011.</p>
<div id="chebotar2019"></div>
<p>[CHM+19] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox.
<a href="https://arxiv.org/abs/1810.05687">Closing the sim-to-real loop: adapting simulation randomization with real world experience</a>.
<em>ICRA</em>, May 2019.</p>
<div id="chen2016"></div>
<p>[CXZ+16] T. Chen, B. Xu, C. Zhang, and C. Guestrin.
<a href="https://arxiv.org/abs/1604.06174">Training deep nets with sublinear memory cost</a>.
Apr. 2016.</p>
<div id="chen2016-c"></div>
<p>[CES16] Y. Chen, J. Emer, and V. Sze.
<a href="https://ieeexplore.ieee.org/document/7551407">Eyeriss: a spatial architecture for energy-efficient dataflow for convolutional neural networks</a>.
<em>ISCA</em>, June 2016.</p>
<div id="chen2016-d"></div>
<p>[CG16] T. Chen and C. Guestrin.
<a href="https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf">XGBoost: a scalable tree boosting system</a>.
<em>SIGKDD</em>, Aug. 2016.</p>
<div id="chen2017"></div>
<p>[CPS+17] L. Chen, G. Papandreou, F. Schroff, and H. Adam.
<a href="https://arxiv.org/abs/1706.05587">Rethinking Atrous convolution for semantic image segmentation</a>.
June 2017.</p>
<div id="chen2017-b"></div>
<p>[CES17] Y. Chen, J. Emer, and V. Sze.
<a href="https://ieeexplore.ieee.org/abstract/document/7948671">Using dataflow to optimize energy efficiency of deep neural network accelerators</a>.
<em>MICRO</em>, June 2017.</p>
<div id="chen2018"></div>
<p>[CMJ+18] T. Chen, T. Moreau, Z. Jiang, et al.
<a href="https://www.usenix.org/conference/osdi18/presentation/chen">TVM: an automated end-to-end optimizing compiler for deep learning</a>.
<em>OSDI</em>, 2018.</p>
<div id="chen2019"></div>
<p>[CYC19] C. Chen, C. Yang, and H. Cheng.
<a href="https://arxiv.org/abs/1809.02839">Efficient and robust parallel DNN training through model parallelism on multi-GPU platform</a>.
Oct. 2019.</p>
<div id="chen2019-b"></div>
<p>[CZZ+19] C. Chen, M. Zhang, M. Zhang, Y. Liu, Y. Li, and S. Ma.
<a href="http://www.thuir.cn/group/~mzhang/publications/WSDM2019ChenChong.pdf">Social attentional memory network: modeling aspect- and friend-level differences in recommendation</a>.
<em>WSDM</em>, Jan. 2019.</p>
<div id="chen2019-c"></div>
<p>[CZL+19] Q. Chen, H. Zhao, W. Li, P. Huang, and W. Ou.
<a href="https://arxiv.org/abs/1905.06874">Behavior sequence transformer for e-commerce recommendation in Alibaba</a>.
<em>DLP-KDD</em>, Aug. 2019.</p>
<div id="chen2020"></div>
<p>[CMF+20] B. Chen, T. Medini, J. Farwell, S. Gobriel, C. Tai, and A. Shrivastava.
<a href="https://arxiv.org/abs/1903.03129">SLIDE : in defense of smart algorithms over hardware acceleration for large-scale deep learning systems</a>.
<em>MLSys</em>, Mar. 2020.</p>
<div id="chen2019-d"></div>
<p>[CYE+19] Y. Chen, T. Yang, J. Emer, and V. Sze.
<a href="https://arxiv.org/abs/1807.07928">Eyeriss v2: a flexible accelerator for emerging deep neural networks on mobile devices</a>.
<em>JETCAS</em>, June 2019.</p>
<div id="cheng2016"></div>
<p>[CKH+16] H. Cheng, L. Koc, J. Harmsen, et al.
<a href="https://arxiv.org/abs/1606.07792">Wide and deep learning for recommender systems</a>.
<em>DLRS</em>, Sep. 2016.</p>
<div id="chetlur2014"></div>
<p>[CWV+14] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro, and E. Shelhamer.
<a href="https://arxiv.org/abs/1410.0759">cuDNN: efficient primitives for deep learning</a>.
Dec. 2014.</p>
<div id="choi2017"></div>
<p>[CCK+17] Y. Choi, M. Choi, M. Kim, J. Ha, S. Kim, and J. Choo.
<a href="https://arxiv.org/abs/1711.09020">StarGAN: unified generative adversarial networks for multi-domain image-to-image translation</a>.
<em>CVPR</em>, Nov. 2017.</p>
<div id="choi2018"></div>
<p>[CWV+18] J. Choi, Z. Wang, S. Venkataramani, P. Chuang, V. Srinivasan, and K. Gopalakrishnan.
<a href="https://arxiv.org/abs/1805.06085">PACT: parameterized clipping activation for quantized neural networks</a>.
July 2018.</p>
<div id="chollet2016"></div>
<p>[Cho16] F. Chollet.
<a href="https://arxiv.org/abs/1610.02357">Xception: deep learning with depthwise separable convolutions</a>.
<em>CVPR</em>, Oct. 2016.</p>
<div id="choma2018"></div>
<p>[CB18] N. Choma and J. BrunaY.
<a href="https://www.nersc.gov/assets/Uploads/BDC-08-Nicholas-Choma-Big-Data-Summit-slides.pdf">Graph neural networks for neutrino classification</a>.
<em>Big Data Summit</em>, Feb. 2018.</p>
<div id="chung2014"></div>
<p>[CGC+14] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio.
<a href="https://arxiv.org/abs/1412.3555">Empirical evaluation of gated recurrent neural networks on sequence modeling</a>.
Dec. 2014.</p>
<div id="chung2018"></div>
<p>[CFO+18] E. Chung, J. Fowers, K. Ovtcharov, et al.
<a href="https://ieeexplore.ieee.org/document/8344479">Serving DNNs in real time at datacenter scale with project Brainwave</a>.
<em>MICRO</em>, Mar. 2018.</p>
<div id="cicek2016"></div>
<p>[CAL+16] O. Cicek, A. Abdulkadir, S. Lienkamp, T. Brox, and O. Ronneberger.
<a href="https://arxiv.org/abs/1606.06650">3D U-Net: learning dense volumetric segmentation from sparse annotation</a>.
<em>MICCAI</em>, June 2016.</p>
<div id="cortex2020"></div>
<p>[Cor20] Cortex.
<a href="https://github.com/cortexlabs/cortex">Deploy machine learning models in production</a>.
2020.</p>
<div id="covington2016"></div>
<p>[CAS16] P. Covington, J. Adams, and E. Sargin.
<a href="https://research.google/pubs/pub45530/">Deep neural networks for YouTube recommendations</a>.
<em>RecSys</em>, Sep. 2016.</p>
<div id="dai2019"></div>
<p>[DB19] W. Dai and D. Berleant.
<a href="https://arxiv.org/abs/1907.03626">Benchmarking contemporary deep learning hardware and frameworks: a survey of qualitative metrics</a>.
<em>CogMI</em>, Dec. 2019.</p>
<div id="das2016"></div>
<p>[DAM+16] D. Das, S. Avancha, D. Mudigere, et al.
<a href="https://arxiv.org/abs/1602.06709">Distributed deep learning using synchronous stochastic gradient descent</a>.
Feb. 2016.</p>
<div id="dally2017"></div>
<p>[Dal17] B. Dally.
<a href="https://ip.cadence.com/uploads/presentations/1000AM_Dally_Cadence_ENN.pdf">High-performance hardware for machine learning</a>.
<em>ENN</em>, Feb. 2017.</p>
<div id="das2018"></div>
<p>[DMM+18] D. Das, N. Mellempudi, D. Mudigere, et al.
<a href="https://arxiv.org/abs/1802.00930">Mixed precision training of convolutional neural networks using integer operations</a>.
<em>ICLR</em>, Feb. 2018.</p>
<div id="dauphin2014"></div>
<p>[DPG+14] Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio.
<a href="https://arxiv.org/abs/1406.2572">Identifying and attacking the saddle point problem in high-dimensional non-convex optimization</a>.
<em>NeurIPS</em>, Dec. 2014.</p>
<div id="dave2019"></div>
<p>[DKA+19] S. Dave, Y. Kim, S. Avancha, K. Lee, and A. Shrivastava.
<a href="https://dl.acm.org/doi/abs/10.1145/3358198">DMazeRunner: executing perfectly nested loops on dataflow accelerators</a>.
<em>TECS</em>, Oct. 2019.</p>
<div id="dawnbench2020"></div>
<p>[Daw20] DAWNBench.
<a href="https://dawn.cs.stanford.edu/benchmark/">DAWNBench: an end-to-end deep learning benchmark and competition</a>.
2020.</p>
<div id="decrema2019"></div>
<p>[DCJ19] M. Dacrema, P. Cremonesi, and D. Jannach.
<a href="https://arxiv.org/abs/1907.06902">Are we really making much progress? A worrying analysis of recent neural recommendation approaches</a>.
<em>RecSys</em>, Sep. 2019.</p>
<div id="deepbench2019"></div>
<p>[Dee19] DeepBench.
<a href="https://github.com/baidu-research/DeepBench">Benchmarking deep learning operations on different hardware</a>.
2019.</p>
<div id="dennard1974"></div>
<p>[DGY+74] R. Dennard, F. Gaensslen, H. Yu, V. Rideout, E. Bassous, and A. LeBlanc.
<a href="https://ieeexplore.ieee.org/document/1050511">Design of ion-implanted MOSFET's with very small physical dimensions</a>.
<em>JSSC</em>, Oct. 1974.</p>
<div id="dennis2019"></div>
<p>[DAM+19] D. Dennis, D. Acar, V. Mandikal, V. Sadasivan, H. Simhadri, V. Saligrama, and P. Jain.
<a href="https://papers.nips.cc/paper/9451-shallow-rnn-accurate-time-series-classification-on-resource-constrained-devices.pdf">Shallow RNNs: a method for accurate time-series classification on tiny devices</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="devlin2017"></div>
<p>[Dev17] J. Devlin.
<a href="https://arxiv.org/abs/1705.01991">Sharp models on dull hardware: fast and accurate neural machine translation decoding on the CPU</a>.
May 2017.</p>
<div id="devlin2018"></div>
<p>[DCL+18] J. Devlin, M. Chang, K. Lee, and K. Toutanova.
<a href="https://arxiv.org/abs/1810.04805">BERT: pre-training of deep bidirectional transformers for language understanding</a>.
Oct. 2018.</p>
<div id="dhillon2018"></div>
<p>[DAL+18] G. Dhillon, K. Azizzadenesheli, Z. Lipton, et al.
<a href="https://arxiv.org/abs/1803.01442">Stochastic activation pruning for robust adversarial defense</a>.
<em>ICLR</em>, Mar. 2018.</p>
<div id="dinechin2019"></div>
<p>[dDF+19] F. de Dinechin, L. Forget, J. Muller, and Y. Uguen.
<a href="https://dl.acm.org/doi/10.1145/3316279.3316285">Posits: the good, the bad and the ugly</a>.
<em>CoNGA</em>, Mar. 2019.</p>
<div id="ding2019"></div>
<p>[DSK+19] Y. Ding, J. Sohn, M. Kawczynski, et al.
<a href="https://www.ncbi.nlm.nih.gov/pubmed/30398430">A deep learning model to predict a diagnosis of Alzheimer disease by using F-FDG PET of the brain</a>.
<em>Radiology</em>, Feb. 2019.</p>
<div id="dinh2017"></div>
<p>[DPB+17] L. Dinh, R. Pascanu, S. Bengio, and Y. Bengio.
<a href="https://arxiv.org/abs/1703.04933">Sharp minima can generalize for deep nets</a>.
<em>ICML</em>, Aug. 2017.</p>
<div id="doctor2019"></div>
<p>[DWO+19] Z. Doctor, D. Wysocki, R. O'Shaughnessy, D. Holz, and B. Farr.
<a href="https://arxiv.org/abs/1911.04424">Black hole coagulation: modeling hierarchical mergers in black hole populations</a>.
Nov. 2019.</p>
<div id="domhan2020"></div>
<p>[DDV+20] T. Domhan, M. Denkowski, D. Vilar, X. Niu, F. Hieber, and K. Heafield.
<a href="https://arxiv.org/abs/2008.04885">The Sockeye 2 neural machine translation toolkit at AMTA 2020</a>.
Aug. 2020.</p>
<div id="dong2019"></div>
<p>[Don19] L. Dong.
<a href="https://tech.ebayinc.com/engineering/odm/">eBay's hyperscale platforms</a>.
Sep. 2019.</p>
<div id="dong2019-b"></div>
<p>[DYC+19] Z. Dong, Z. Yao, Y. Cai, D. Arfeen, A. Gholami, M. Mahoney, and K. Keutzer.
<a href="https://arxiv.org/abs/1911.03852">HAWQ-V2: Hessian aware trace-weighted quantization of neural networks</a>.
Nov. 2019.</p>
<div id="dozat2016"></div>
<p>[Doz16] T. Dozat.
<a href="https://openreview.net/forum?id=OM0jvwB8jIp57ZJjtNEZ">Incorporating Nesterov momentum into Adam</a>.
<em>ICLR</em>, May 2016.</p>
<div id="dryden2019"></div>
<p>[DMM+19] N. Dryden, N. Maruyama, T. Moon, T. Benson, M. Snir, and B. Van Essen.
<a href="https://dl.acm.org/doi/10.1145/3295500.3356207">Channel and filter parallelism for large-scale CNN training</a>.
<em>SC</em>, Nov. 2019.</p>
<div id="du2020"></div>
<p>[DJS20] M. Du, R. Jia, and D. Song.
<a href="https://openreview.net/forum?id=SJx0q1rtvS">Robust anomaly detection and backdoor attack detection via differential privacy</a>.
<em>ICLR</em>, Feb. 2020.</p>
<div id="duchi2011"></div>
<p>[DHS11] J. Duchji, E. Hazan, and Y. Singer.
<a href="http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">Adaptive subgradient methods for online learning and stochastic optimization</a>.
<em>JMLR</em>, July 2011.</p>
<div id="efrati2020"></div>
<p>[Efr20] A. Efrati.
<a href="https://www.theinformation.com/articles/ai-startups-proliferate-as-businesses-look-for-savings">AI startups proliferate as businesses look for savings</a>.
<em>The Information</em>, Aug. 2020.</p>
<div id="elango2018"></div>
<p>[ERR+18] V. Elango, N. Rubin, M. Ravishankar, H. Sandanagobalane, and V. Grover.
<a href="https://dl.acm.org/doi/10.1145/3211346.3211354">Diesel: DSL for linear algebra and neural net computations on GPUs</a>.
<em>MAPL</em>, June 2018.</p>
<div id="eider2018"></div>
<p>[Eid18] Eider.
<a href="https://nips.cc/Expo/Conferences/2018/Schedule?demo_id=4">Expo Demo</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="eisenman2018"></div>
<p>[ENG+18] A. Eisenman, M. Naumov, D. Gardner, M. Smelyanskiy, S. Pupyrev, K. Hazelwood, A. Cidon, and S. Katti.
<a href="https://arxiv.org/abs/1811.05922">Bandana: using non-volatile memory for storing deep learning models</a>.
Nov. 2018.</p>
<div id="erez2015"></div>
<p>[ETT15] T. Erez, Y. Tassa, and E. Todorov.
<a href="https://ieeexplore.ieee.org/document/7139807">Simulation tools for model-based robotics: comparison of Bullet, Havok, MuJoCo, ODE and PhysX</a>.
<em>ICRA</em>, May 2015.</p>
<div id="esmaeilzadeh2011"></div>
<p>[EBA+11] H. Esmaeilzadeh, E. Blem, R. S. Amant, K. Sankaralingam, and D. Burger.
<a href="https://ieeexplore.ieee.org/document/6307773">Dark silicon and the end of multicore scaling</a>.
<em>ISCA</em>, June 2011.</p>
<div id="evans2016"></div>
<p>[EG16] R. Evans and J. Gao.
<a href="https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40">DeepMind AI reduces Google data centre cooling bill by 40 percent</a>.
July 2016.</p>
<div id="pytorch-glowir2018"></div>
<p>[Fac18] Facebook.
<a href="https://github.com/pytorch/glow/blob/master/docs/IR.md">Glow IR</a>.
Oct. 2018.</p>
<div id="pytorch-glow2020"></div>
<p>[Fac20] Facebook.
<a href="https://github.com/pytorch/glow">Compiler for neural network hardware accelerators</a>.
Feb. 2020.</p>
<div id="farshchi2019"></div>
<p>[FHY19] F. Farshchi, Q. Huang, and H. Yun.
<a href="https://arxiv.org/abs/1903.06495">Integrating NVIDIA deep learning accelerator (NVDLA) with RISC-V SoC on FireSim</a>.
<em>EMC2</em>, Dec. 2019.</p>
<div id="feldman2019"></div>
<p>[Fel19] M. Feldman.
<a href="https://www-nextplatform-com.cdn.ampproject.org/c/s/www.nextplatform.com/2019/12/19/ai-recommendation-systems-get-a-gpu-makeover/amp/">AI recommendation systems get a GPU makeover</a>.
2018.</p>
<div id="feldman2019-b"></div>
<p>[Fel19b] A. Feldman.
<a href="https://www.cerebras.net/cerebras-deploys-the-cs-1-the-industrys-fastest-ai-computer-at-argonne-national-lab/">Cerebras deploys the CS-1, the industry's fastest AI computer, at Argonne National Lab</a>.
Nov. 2019.</p>
<div id="felzenszwalb2010"></div>
<p>[FGM+10] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan.
<a href="https://ieeexplore.ieee.org/document/5255236">Object detection with discriminatively trained part-based models</a>.
<em>PAMI</em>, Sep. 2010.</p>
<div id="fey2020"></div>
<p>[Fey20] M. Fey.
<a href="https://pytorch-geometric.readthedocs.io/en/latest/">PyTorch geometric documentation</a>.
2020.</p>
<div id="fey2019"></div>
<p>[FL19] M. Fey and J. Lenssen.
<a href="https://arxiv.org/abs/1903.02428">Fast graph representation learning with PyTorch geometric</a>.
Mar. 2019.</p>
<div id="finn2017"></div>
<p>[FAL17] C. Finn, P. Abbeel, and S. Levine.
<a href="https://arxiv.org/abs/1703.03400">Model-agnostic meta-learning for fast adaptation of deep networks</a>.
<em>ICML</em>, July 2017.</p>
<div id="firoiu2017"></div>
<p>[FWT11] V. Firoiu, W. Whitney, and J. Tenenbaum.
<a href="https://arxiv.org/abs/1702.06230">Beating the world's best at Super Smash Bros. with deep reinforcement learning</a>.
May 2017.</p>
<div id="flennerhag2020"></div>
<p>[FRP+20] S. Flennerhag, A. Rusu, R. Pascanu, F. Visin, H. Yin, and R. Hadsell.
<a href="https://openreview.net/forum?id=rkeiQlBFPB">Meta-learning with warped gradient descent</a>.
<em>ICLR</em>, Apr. 2020.</p>
<div id="frankle2019"></div>
<p>[FC19] J. Frankle and M. Carbin.
<a href="https://arxiv.org/abs/1803.03635">The lottery ticket hypothesis: finding sparse, trainable neural networks</a>.
<em>ICLR</em>, Mar. 2019.</p>
<div id="frigo1999"></div>
<p>[FLP+99] M. Frigo, C. E. Leiserson, H. Prokop, and S. Ramachandran.
<a href="https://www.computer.org/csdl/proceedings-article/focs/1999/04090285/12OmNwE9OA0">Cache-oblivious algorithms</a>.
1999.</p>
<div id="gabor1946"></div>
<p>[Gab46] D. Gabor.
<a href="https://ieeexplore.ieee.org/document/5298517">Theory of communication. Part 1: the analysis of information</a>.
<em>Radio &amp; Comm. Eng.</em>, Nov. 1946.</p>
<div id="gale2020"></div>
<p>[GZY+20] T. Gale, M. Zaharia, C. Young, and Erich Elsen.
<a href="https://arxiv.org/abs/2006.10901">Sparse GPU kernels for deep learning</a>.
June 2020.</p>
<div id="gauci2019"></div>
<p>[GCL+19] J. Gauci, E. Conti, Y. Liang, et al.
<a href="https://arxiv.org/abs/1811.00260">Horizon: Facebook's open source applied reinforcement learning platform</a>.
Sep. 2019.</p>
<div id="gebru2020"></div>
<p>[GMV+20] T. Gebru, J. Morgenstern, B. Vecchione, J. Vaughan, H. Wallach, H. Daume III, and K. Crawford.
<a href="https://arxiv.org/abs/1803.09010">Datasheets for datasets</a>.
Mar, 2019.</p>
<div id="gehring2017"></div>
<p>[GAG+17] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. Dauphin.
<a href="https://arxiv.org/abs/1705.03122">Convolutional sequence to sequence learning</a>.
<em>ICML</em>, May 2017.</p>
<div id="geirhos2018"></div>
<p>[GRM+18] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. Wichmann, and W. Brendel.
<a href="https://arxiv.org/abs/1811.12231">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</a>.
Nov. 2018.</p>
<div id="gentry2009"></div>
<p>[Gen09] C. Gentry.
<a href="https://crypto.stanford.edu/craig/craig-thesis.pdf">A fully homomorphic encryption scheme</a>.
Sep. 2009.</p>
<div id="georganas2018"></div>
<p>[GAB+18] E. Georganas, S. Avancha, K. Banerjee, D. Kalamkar, G. Henry, H. Pabst, and A. Heinecke.
<a href="https://arxiv.org/abs/1808.05567">Anatomy Of high-performance deep learning convolutions on SIMD architectures</a>.
<em>SC</em>, Aug. 2018.</p>
<div id="gers1999"></div>
<p>[GSC99] F. Gers, J. Schmidhuber, and F. Cummins.
<a href="https://ieeexplore.ieee.org/document/818041">Learning to forget: continual prediction with LSTM</a>.
<em>ICANN</em>, Sep. 1999.</p>
<div id="gharakhanian2017"></div>
<p>[Gha17] A. Gharakhanian.
<a href="https://www.kdnuggets.com/2017/01/generative-adversarial-networks-hot-topic-machine-learning.html">Generative adversarial networks-hot topic in machine learning</a>.
<em>KDnuggets</em>, Jan. 2017.</p>
<div id="gholami2018"></div>
<p>[GAJ+18] A. Gholami, A. Azad, P. Jin, K. Keutzer, and A. Buluc.
<a href="https://arxiv.org/abs/1712.04432">Integrated model, batch, and domain parallelism in training neural networks</a>.
<em>SPAA</em>, July 2018.</p>
<div id="ghose2019"></div>
<p>[GLH+19] S. Ghose, T. Li, N. Hajinazar, D. Cali, and O. Mutlu.
<a href="https://arxiv.org/abs/1902.07609">Understanding the interactions of workloads and DRAM types: a comprehensive experimental study</a>.
Oct. 2019.</p>
<div id="ginsburg2020"></div>
<p>[GCH+20] B. Ginsburg, P. Castonguay, O. Hrinchuk, et al.
<a href="https://arxiv.org/abs/1905.11286">Stochastic gradient methods with layer-wise adaptive moments for training of deep networks</a>.
Feb. 2020.</p>
<div id="glorot2011"></div>
<p>[GBB11] X. Glorot, A. Bordes, and Y. Bengio.
<a href="http://proceedings.mlr.press/v15/glorot11a.html">Deep sparse rectifier neural networks</a>.
<em>AISTATS</em>, 2011.</p>
<div id="glorot2010"></div>
<p>[GB10] X. Glorot and Y. Bengio.
<a href="http://proceedings.mlr.press/v9/glorot10a.html">Understanding the difficulty of training deep feedforward neural networks</a>.
<em>AISTATS</em>, 2010.</p>
<div id="goodfellow2014"></div>
<p>[GPM+14] I. Goodfellow, J. Pouget-Abadie, M. Mirza, et al.
<a href="https://arxiv.org/abs/1406.2661">Generative adversarial networks</a>.
Jun. 2014.</p>
<div id="mlir2019"></div>
<p>[Goo19] Google.
<a href="https://blog.tensorflow.org/2019/04/mlir-new-intermediate-representation.html">MLIR: a new intermediate representation and compiler framework</a>.
Apr. 2019.</p>
<div id="google-dev2020"></div>
<p>[Goo20] Google.
<a href="https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space">Embeddings: translating to a lower-dimensional space</a>.
2020.</p>
<div id="differential-privacy2020"></div>
<p>[Goo20b] Google.
<a href="https://github.com/google/differential-privacy">C++ differential privacy library</a>.
Feb. 2020.</p>
<div id="tensorflow-index2020"></div>
<p>[Goo20c] Google.
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/compiler/xla/g3doc/index.md">TensorFlow XLA index</a>.
Feb. 2020.</p>
<div id="aiplatform2020"></div>
<p>[Goo20d] Google.
<a href="https://cloud.google.com/ai-platform/">AI Platform</a>.
2020.</p>
<div id="tensorflow-tfx2020"></div>
<p>[Goo20e] Google.
<a href="https://www.tensorflow.org/tfx/">TensorFlow Extended (TFX) is an end-to-end platform for deploying production ML pipelines</a>.
2020.</p>
<div id="tcav2020"></div>
<p>[Goo20f] Google.
<a href="https://github.com/tensorflow/tcav">TensorFlow TCAV</a>.
2020.</p>
<div id="tensorflow-xla2020"></div>
<p>[Goo20g] Google.
<a href="https://www.tensorflow.org/xla/operation_semantics">TensorFlow-XLA Operation Semantics</a>.
2020.</p>
<div id="goto2008"></div>
<p>[Gvd08] K. Goto, and R. van de Geijn.
<a href="https://dl.acm.org/doi/10.1145/1356052.1356053">Anatomy of high-performance matrix multiplication</a>.
<em>TOMS</em>, May 2008.</p>
<div id="gc-azure2019"></div>
<p>[Gra19] GraphCore.
<a href="https://www.graphcore.ai/posts/microsoft-and-graphcore-collaborate-to-accelerate-artificial-intelligence">Microsoft and Graphcore collaborate to accelerate artificial intelligence</a>.
2019.</p>
<div id="ipu2020"></div>
<p>[Gra20] Graphcore.
<a href="https://www.graphcore.ai/mk2-ipu-m2000-white-paper">Intelligent processing unit</a>.
July 2020.</p>
<div id="greff2017"></div>
<p>[GSK+17] K. Greff, R. Srivastava, J. Koutn\'{\i}k, B. Steunebrink, and J. Schmidhuber.
<a href="https://ieeexplore.ieee.org/document/7508408">LSTM: a search space dyssey</a>.
<em>TNNLS</em>, Oct. 2017.</p>
<div id="griewank2000"></div>
<p>[GW00] A. Griewank and A. Walther.
<a href="https://dl.acm.org/doi/10.1145/347837.347846">Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation</a>.
<em>TOMS</em>, Mar. 2000.</p>
<div id="guan2019"></div>
<p>[GMY+19] H. Guan, A. Malevich, J. Yang, Jongsoo Park, and H. Yuen.
<a href="https://arxiv.org/abs/1911.02079">Post-training 4-bit quantization on embedding tables</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="gui2019"></div>
<p>[GWY+19] S. Gui, H. Wang, C. Yu, H. Yang, Z. Wang, and J. Liu.
<a href="https://arxiv.org/abs/1902.03538">Model compression with adversarial robustness: a unified optimization framework</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="guildai2020"></div>
<p>[Gui20] GuildAI.
<a href="https://guild.ai/">The ML Engineering Platform</a>.
2020.</p>
<div id="gunning2017"></div>
<p>[Gun17] D. Gunning.
<a href="https://www.darpa.mil/attachments/XAIProgramUpdate.pdf">Explainable Artificial Intelligence (XAI)</a>.
<em>DARPA</em>, Nov. 2017.</p>
<div id="gupta2020"></div>
<p>[GPV+20] P. Gupta, N. Puri, S. Verma, D. Kayastha, S. Deshmukh, B. Krishnamurthy, and S. Singh.
<a href="https://openreview.net/forum?id=SJgzLkBKPB">Explain your move: understanding agent actions using focused feature saliency</a>.
<em>ICLR</em>, 2020.</p>
<div id="guo2017"></div>
<p>[GTY+17] H. Guo, R. Tang, Y. Ye, Z. Li, and X. He.
<a href="https://arxiv.org/pdf/1703.04247.pdf">DeepFM: a factorization-machine based neural network for CTR prediction</a>.
Mar. 2017.</p>
<div id="gustafson2017"></div>
<p>[Gus17] J. Gustafson.
<a href="https://posithub.org/docs/Posits4.pdf">Posit arithmetic</a>.
2017.</p>
<div id="habana2019"></div>
<p>[Hab19] Habana Labs.
<a href="https://habana.ai/wp-content/uploads/pdf/habana_labs_goya_whitepaper.pdf">Goya inference platform white paper</a>.
Aug. 2019.</p>
<div id="hls2019"></div>
<p>[Hab19b] Habana Labs.
<a href="https://habana.ai/wp-content/uploads/2019/06/HLS-1-Datasheet.pdf">System-1</a>.
June 2019.</p>
<div id="han2016"></div>
<p>[HKK16] D. Han, J. Kim, and J. Kim.
<a href="https://arxiv.org/abs/1610.02915">Deep pyramidal residual networks</a>.
<em>CVPR</em>, Oct. 2016.</p>
<div id="han2017"></div>
<p>[HPN+17] S. Han, J. Pool, S. Narang, et al.
<a href="https://arxiv.org/abs/1607.04381">DSD: dense-sparse-dense training for deep neural networks</a>.
<em>ICLR</em>, Feb. 2017.</p>
<div id="hard2019"></div>
<p>[HRM+19] A. Hard, K. Rao, R. Mathews, et al.
<a href="https://arxiv.org/abs/1811.03604">Federated learning for mobile keyboard prediction</a>.
Feb. 2019.</p>
<div id="harlap2018"></div>
<p>[HNP+18] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger, and P. Gibbons.
<a href="https://arxiv.org/abs/1806.03377">PipeDream: fast and efficient pipeline parallel DNN training</a>.
June 2018.</p>
<div id="hartmann2018"></div>
<p>[Har18] F. Hartmann.
<a href="https://florian.github.io/federated-learning-firefox/">Federated learning for Firefox</a>.
Aug. 2018.</p>
<div id="hassan2018"></div>
<p>[Has18] M. Hassan.
<a href="https://neurohive.io/wp-content/uploads/2018/10/AlexNet-1.png">AlexNet-1.png</a>.
2018.</p>
<div id="hazelwood2018"></div>
<p>[Haz18] K. Hazelwood.
<a href="https://atscaleconference.com/videos/scale-2018-applied-machine-learning-at-facebook-an-infrastructure-perspective/">Applied machine learning at Facebook: an infrastructure perspective</a>.
Sep. 2018.</p>
<div id="hazelwood2018-b"></div>
<p>[HBB+18] K. Hazelwood, S. Bird, D. Brooks, et al.
<a href="https://ieeexplore.ieee.org/document/8327042">Applied machine learning at Facebook: a datacenter infrastructure perspective</a>.
<em>HPCA</em>, Feb. 2018.</p>
<div id="hazelwood2020"></div>
<p>[Haz20] K. Hazelwood.
<a href="https://www.youtube.com/watch?v=DCERuwn2IaI&amp;t=6m13s">Deep learning: it's not all about recognizing cats and dogs</a>.
<em>SAIS</em>, June 2020.</p>
<div id="he2008"></div>
<p>[HBG+08] H. He, Y. Bai, E. A. Garcia, and S. Li.
<a href="https://ieeexplore.ieee.org/document/4633969">ADASYN: adaptive synthetic sampling approach for imbalanced learning</a>.
<em>IJCNN</em>, June 2008.</p>
<div id="he2015"></div>
<p>[HZR+15] K. He, X. Zhang, S. Ren, and J. Sun.
<a href="https://arxiv.org/abs/1512.03385">Deep residual learning for image recognition</a>.
<em>CVPR</em>, Dec. 2015.</p>
<div id="he2015-b"></div>
<p>[HZR+15] K. He, X. Zhang, S. Ren, and J. Sun.
<a href="https://arxiv.org/abs/1502.01852">Delving deep into rectifiers: surpassing human-level performance on ImageNet classification</a>.
<em>ICCV</em>, Feb. 2015.</p>
<div id="he2015-c"></div>
<p>[HZR+15] K. He, X. Zhang, S. Ren, and J. Sun.
<a href="https://arxiv.org/abs/1406.4729">Spatial pyramid pooling in deep convolutional networks for visual recognition</a>.
Apr. 2015.</p>
<div id="he2017"></div>
<p>[HGD+17] K. He, G. Gkioxari, P. Dollar, and R. Girshick.
<a href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a>.
<em>ICCV</em>, Mar. 2017.</p>
<div id="he2017-b"></div>
<p>[HLZ+17] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, and T. Chua.
<a href="https://arxiv.org/pdf/1708.05031.pdf">Neural collaborative filtering</a>.
<em>ICIWWW</em>, Apr. 2017.</p>
<div id="he2019"></div>
<p>[HSP+19] Y. He, T. Sainath, R. Prabhavalkar, et al.
<a href="https://arxiv.org/abs/1811.06621">Streaming end-to-end speech recognition for mobile devices</a>.
<em>ICASSP</em>, Apr. 2019.</p>
<div id="he2019-b"></div>
<p>[HLL+19] Y. He, J. Lin, Z. Liu, H. Wang, L. Li, and S. Han.
<a href="https://arxiv.org/abs/1802.03494">AMC: AutoML for model compression and acceleration on mobile devices</a>.
<em>ECCV</em>, Jan. 2019.</p>
<div id="hegde2019"></div>
<p>[HAP+19] K. Hegde, H. Asghari-Moghaddam, M. Pellauer, et al.
<a href="https://dl.acm.org/doi/10.1145/3352460.3358275">ExTensor: an accelerator for sparse tensor algebra</a>.
<em>MICRO</em>, Oct. 2019.</p>
<div id="henderson2019"></div>
<p>[HIB+19] P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger.
<a href="https://arxiv.org/abs/1709.06560">Deep Reinforcement Learning that Matters</a>.
Jan. 2019.</p>
<div id="hendrycks2016"></div>
<p>[HG16] D. Hendrycks and K. Gimpel.
<a href="https://arxiv.org/pdf/1606.08415.pdf">Gaussian error linear units (GELUs)</a>.
June 2016.</p>
<div id="hermann2017"></div>
<p>[HDB17] J. Hermann and M. Del Balso.
<a href="https://eng.uber.com/michelangelo/">Meet Michelangelo: Uber's machine learning platform</a>.
Sep. 2017.</p>
<div id="hessel2017"></div>
<p>[HMv+17] M. Hessel, J. Modayil, H. van Hasselt, et al.
<a href="https://arxiv.org/abs/1710.02298">Rainbow: combining improvements in deep reinforcement learning</a>.
<em>AAAI</em>, Oct. 2017.</p>
<div id="highlander2015"></div>
<p>[HR15] T. Highlander and A. Rodriguez.
<a href="http://www.bmva.org/bmvc/2015/papers/paper160/paper160.pdf">Very efficient training of convolutional neural networks using fast Fourier transform and overlap-and-add</a>.
<em>BMVA</em>, Sep. 2015.</p>
<div id="hinton2012"></div>
<p>[HSS12] G. Hinton, N. Srivastava, and K. Swersky.
<a href="https://www.cs.toronto.edu/ tijmen/csc321/slides/lecture_slides_lec6.pdf">RMSProp: divide the gradient by a running average of its recent magnitude</a>.
<em>Coursera</em>, 2012.</p>
<div id="hinton2015"></div>
<p>[HVD15] G. Hinton, O. Vinyals, and J. Dean.
<a href="https://arxiv.org/abs/1503.02531">Distilling the knowledge in a neural network</a>.
Mar. 2015.</p>
<div id="hitaj2017"></div>
<p>[HAP17] B. Hitaj, G. Ateniese, and F. Perez-Cruz.
<a href="https://arxiv.org/abs/1702.07464">Deep models under the GAN: information leakage from collaborative deep learning</a>.
<em>SIGSAC CCS</em>, Sep. 2017.</p>
<div id="hochreiter1997"></div>
<p>[HS97] S. Hochreiter and J. Schmidhuber.
<a href="https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.1.1">Flat minima</a>.
<em>Neural Comp.</em>, Jan. 1997.</p>
<div id="hochreiter1997-b"></div>
<p>[HS97] S. Hochreiter and J. Schmidhuber.
<a href="https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735">Long short-term memory</a>.
<em>Neural Comp.</em>, Nov. 1997.</p>
<div id="hoffer2017"></div>
<p>[HHS17] E. Hoffer, I. Hubara, and D. Soudry.
<a href="https://arxiv.org/abs/1705.08741">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</a>.
<em>NeurIPS</em>, Dec. 2017.</p>
<div id="holler2019"></div>
<p>[HM19] A. Holler and M. Mui.
<a href="https://eng.uber.com/michelangelo-model-representation/">Evolving Michelangelo model representation for flexibility at scale</a>.
Oct. 2019.</p>
<div id="hooker2019"></div>
<p>[HEK+19] S. Hooker, D. Erhan, P. Kindermans, and B. Kim.
<a href="https://arxiv.org/abs/1806.10758">A benchmark for interpretability methods in deep neural networks</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="hornik1989"></div>
<p>[HSW89] K. Hornik, M. Stinchcombe, and H. White.
<a href="https://www.sciencedirect.com/science/article/abs/pii/0893608089900208">Multilayer feedforward networks are universal approximators</a>.
<em>NNs</em>, Mar. 1989.</p>
<div id="horowitz2014"></div>
<p>[Hor14] M. Horowitz.
<a href="https://ieeexplore.ieee.org/document/6757323">1.1 Computing's energy problem (and what we can do about it)</a>.
<em>ISSCC</em>, Feb. 2014.</p>
<div id="hou2019"></div>
<p>[Hou19] J. Hou.
<a href="https://www.qualcomm.com/news/onq/2019/07/30/new-research-quantization-could-revolutionize-power-efficient-ai">New research on quantization could revolutionize power-efficient AI</a>.
July 2019.</p>
<div id="howard2017"></div>
<p>[HZC+17] A. G. Howard, M. Zhu, B. Chen, et al.
<a href="https://arxiv.org/abs/1704.04861">MobileNets: efficient convolutional neural networks for mobile vision applications</a>.
Apr. 2017.</p>
<div id="hu2019-b"></div>
<p>[HSA+19] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu.
<a href="https://arxiv.org/abs/1709.01507">Squeeze-and-excitation networks</a>.
<em>CVPR</em>, May 2019.</p>
<div id="hu2019"></div>
<p>[HLG+19] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and J. Leskovec.
<a href="https://openreview.net/forum?id=HJlWWJSFDH">Strategies for pre-training graph neural networks</a>.
<em>ICLR</em>, Sep. 2019.</p>
<div id="hua2019"></div>
<p>[HZS+19] W. Hua, Y. Zhou, C. Sa, Z. Zhang, and G. Suh.
<a href="https://arxiv.org/abs/1805.12549">Channel gating neural networks</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="huang2016"></div>
<p>[HLv+16] G. Huang, Z. Liu, L. van der Maaten, and K. Weinberger.
<a href="https://arxiv.org/abs/1608.06993">Densely connected convolutional networks</a>.
<em>CVPR</em>, Aug. 2016.</p>
<div id="huang2017"></div>
<p>[HLP+17] X. Huang, Y. Li, O. Poursaeed, J. Hopcroft, and S. Belongie.
<a href="https://arxiv.org/abs/1612.04357">Stacked generative adversarial networks</a>.
<em>CVPR</em>, June 2017.</p>
<div id="huang2019"></div>
<p>[HCB+19] Y. Huang, Y. Cheng, A. Bapna, et al.
<a href="https://arxiv.org/abs/1811.06965">GPipe: efficient training of giant neural networks using pipeline parallelism</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="huang2019-b"></div>
<p>[HDS+19] D. Huang, P. Dhariwal, D. Song, and I. Sutskever.
<a href="https://openreview.net/forum?id=r1xwKoR9Y7">GamePad: a learning environment for theorem proving</a>.
<em>ICLR</em>, 2019.</p>
<div id="huawei-9102019"></div>
<p>[Hua19] Huawei.
<a href="https://e.huawei.com/us/products/cloud-computing-dc/atlas/ascend-910">Ascend 910 AI processor</a>.
2019.</p>
<div id="hughes2015"></div>
<p>[Hug15] C. Hughes.
<a href="https://www.morganclaypool.com/doi/abs/10.2200/S00647ED1V01Y201505CAC032">Single-instruction multiple-data execution</a>.
<em>M\&amp;C</em>, May 2015.</p>
<div id="hwang2014"></div>
<p>[HS14] K. Hwang and W. Sung.
<a href="https://ieeexplore.ieee.org/document/6986082">Fixed-point feedforward deep neural network design using weights +1, 0, and -1</a>.
<em>SiPS</em>, Oct. 2014.</p>
<div id="iandola2016"></div>
<p>[IHM+16] F. Iandola, S. Han, M. Moskewicz, K. Ashraf, W. Dally, and K. Keutzer.
<a href="https://arxiv.org/abs/1602.07360">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</a>.
<em>CVPR</em>, Feb. 2016.</p>
<div id="ibm2020"></div>
<p>[Ibm20] IBM.
<a href="https://newsroom.ibm.com/2020-08-17-IBM-Reveals-Next-Generation-IBM-POWER10-Processor">IBM reveals next-generation IBM POWER10 processor</a>.
Aug. 2020.</p>
<div id="intel2018"></div>
<p>[Int18] Intel.
<a href="https://nervanasystems.github.io/distiller/knowledge_distillation.html">Knowledge Distillation</a>.
2018.</p>
<div id="intel-xeon2019"></div>
<p>[Int19] Intel.
<a href="https://newsroom.intel.com/news/next-generation-intel-xeon-scalable-processors-deliver-breakthrough-platform-performance-56-processor-cores/">Next-generation Intel Xeon Scalable processors to deliver breakthrough platform performance with up to 56 processor cores</a>.
Aug. 2019.</p>
<div id="intel-aurora2019"></div>
<p>[Int19b] Intel.
<a href="https://newsroom.intel.com/wp-content/uploads/sites/11/2019/11/Aurora-Supercomputer.pdf">Aurora SuperComputer</a>.
Nov. 2019.</p>
<div id="intel-xeon2020"></div>
<p>[Int20] Intel.
<a href="https://newsroom.intel.com/news/2020-ces-intel-news-livestream-replay/">Innovation through intelligence</a>.
Jan. 2020.</p>
<div id="intel-isa2020"></div>
<p>[Int20b] Intel.
<a href="https://software.intel.com/content/dam/develop/public/us/en/documents/architecture-instruction-set-extensions-programming-reference.pdf">Intel architecture instruction set extensions and future features programming reference</a>.
June 2020.</p>
<div id="intel-zoo2020"></div>
<p>[Int20c] Intel.
<a href="https://software.intel.com/en-us/ai/analytics-zoo">Analytics zoo</a>.
2020.</p>
<div id="ioffe2015"></div>
<p>[IS15] S. Ioffe and C. Szegedy.
<a href="https://arxiv.org/abs/1502.03167">Batch normalization: accelerating deep network training by reducing internal covariate shift</a>.
Feb. 2015.</p>
<div id="ioffe2017"></div>
<p>[Iof17] S. Ioffe.
<a href="https://arxiv.org/abs/1702.03275">Batch renormalization: towards reducing minibatch dependence in batch-normalized models</a>.
<em>NeurIPS</em>, Dec. 2017.</p>
<div id="isola2016"></div>
<p>[IZZ+16] P. Isola, J. Zhu, T. Zhou, and A. Efros.
<a href="https://arxiv.org/abs/1611.07004">Image-to-image translation with conditional adversarial networks</a>.
<em>CVPR</em>, Nov. 2016.</p>
<div id="ivakhnenko1971"></div>
<p>[Iva71] A. Ivakhnenko.
<a href="https://ieeexplore.ieee.org/document/4308320">Polynomial theory of complex systems</a>.
<em>SMC</em>, Oct. 1971.</p>
<div id="izmailov2019"></div>
<p>[IPG+19] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov, and A. Wilson.
<a href="https://arxiv.org/abs/1803.05407">Averaging weights leads to wider optima and better generalization</a>.
<em>UAI</em>, Feb. 2019.</p>
<div id="jadhav2019"></div>
<p>[Jad19] A. Jadhav.
<a href="https://towardsdatascience.com/https-medium-com-aishwaryajadhav-applications-of-graph-neural-networks-1420576be574">Applications of graph neural networks</a>.
Feb. 2019.</p>
<div id="jain2019"></div>
<p>[JJN+19] P. Jain, A. Jain, A. Nrusimha, A. Gholami, P. Abbeel, K. Keutzer, I. Stoica, and J. Gonzalez.
<a href="https://arxiv.org/abs/1910.02653">Checkmate: breaking the memory wall with optimal tensor rematerialization</a>.
Oct. 2019.</p>
<div id="janner2019"></div>
<p>[JFZ+19] M. Janner, J. Fu, M. Zhang, and S. Levine.
<a href="https://arxiv.org/abs/1906.08253">When to trust your model: model-based policy optimization</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="jauk2019"></div>
<p>[JYS19] D. Jauk, D. Yang, and M. Schulz.
<a href="https://dl.acm.org/doi/10.1145/3295500.3356185">Predicting faults in high performance computing systems: an in-depth survey of the state-of-the-practice</a>.
<em>SC</em>, Nov. 2019.</p>
<div id="jax2020"></div>
<p>[Jax20] Jax.
<a href="https://github.com/google/jax">Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more</a>.
Feb. 2020.</p>
<div id="jia2018"></div>
<p>[JJM+18] Y. Jia, M. Johnson, W. Macherey, et al.
<a href="https://arxiv.org/abs/1811.02050">Leveraging weakly supervised data to improve end-to-end speech-to-text translation</a>.
<em>ICASSP</em>, Nov. 2018.</p>
<div id="jia2019"></div>
<p>[JWB+19] Y. Jia, R. Weiss, F. Biadsy, W. Macherey, M. Johnson, Z. Chen, and Y. Wu.
<a href="https://arxiv.org/abs/1904.06037">Direct speech-to-speech translation with a sequence-to-sequence model</a>.
Apr. 2019.</p>
<div id="jia2018-b"></div>
<p>[JZW+18] Y. Jia, Y. Zhang, R. Weiss, et al.
<a href="https://arxiv.org/abs/1806.04558">Transfer learning from speaker verification to multispeaker text-to-speech synthesis</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="jia2018-c"></div>
<p>[JZA18] Z. Jia, M. Zaharia, and A. Aiken.
<a href="https://arxiv.org/abs/1807.05358">Beyond data and model parallelism for deep neural networks</a>.
<em>ML</em>, July 2018.</p>
<div id="jiao2020"></div>
<p>[JHJ+20] Y. Jiao, L. Han, R. Jin, et al.
<a href="https://ieeexplore.ieee.org/xpl/conhome/1000708/all-proceedings">12nm programmable convolution-efficient neural-processing-unit chip achieving 825 TOPS</a>.
<em>ISSCC</em>, Feb. 2020.</p>
<div id="jin2018"></div>
<p>[JGK18] P. Jin, B. Ginsburg, and K. Keutzer.
<a href="https://openreview.net/pdf?id=S1Yt0d1vG">Spatially parallel convolution</a>.
<em>ICLR</em>, 2018.</p>
<div id="johnson2018"></div>
<p>[Joh18] J. Johnson.
<a href="https://arxiv.org/abs/1811.01721">Rethinking floating point for deep learning</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="johnson2018-b"></div>
<p>[JS18] M. Johnson and B. Stevens.
<a href="https://www.nature.com/articles/d41586-018-02053-7">Pruning hypothesis comes of age</a>.
<em>Nature</em>, Feb. 2018.</p>
<div id="jordon2019"></div>
<p>[JYv19] J. Jordon, J. Yoon, and M. van der Schaar.
<a href="https://openreview.net/forum?id=S1zk9iRqF7">PATE-GAN: generating synthetic data with differential privacy guarantees</a>.
<em>ICLR</em>, Feb. 2019.</p>
<div id="jouppi2017"></div>
<p>[JYP+17] N. Jouppi, C. Young, N. Patil, D. Patterson, et al.
<a href="https://arxiv.org/abs/1704.04760">In-datacenter performance analysis of a tensor processing unit</a>.
<em>ISCA</em>, June 2017.</p>
<div id="jouppi2020"></div>
<p>[JYK+20] N. Jouppi, D. Yoon, G. Kurian, S. Li, N. Patil, J. Laudon, C. Young, and D. Patterson.
<a href="https://cacm.acm.org/magazines/2020/7/245702-a-domain-specific-supercomputer-for-training-deep-neural-networks/">A domain-specific supercomputer for training deep neural networks</a>.
<em>CACM</em>, July 2020.</p>
<div id="jozefowicz2015"></div>
<p>[JZS15] R. Jozefowicz, W. Zaremba, and I. Sutskever.
<a href="https://dl.acm.org/doi/10.5555/3045118.3045367">An empirical exploration of recurrent network architectures</a>.
<em>ICML</em>, July 2015.</p>
<div id="kaji2019"></div>
<p>[KZK+19] D. Kaji, J. Zech, J. Kim, S. Cho, N. Dangayach, A. Costa, and E. Oermann.
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6373907/">An attention based deep learning model of clinical events in the intensive care unit</a>.
Feb. 2019.</p>
<div id="kalchbrenner2018"></div>
<p>[KES+18] N. Kalchbrenner, E. Elsen, K. Simonyan, et al.
<a href="https://arxiv.org/abs/1802.08435">Efficient neural audio synthesis</a>.
June 2018.</p>
<div id="kalamkar2019"></div>
<p>[KMM+19] D. Kalamkar, D. Mudigere, N. Mellempudi, et al.
<a href="https://arxiv.org/abs/1905.12322">A study of bfloat16 for deep learning training</a>.
June 2019.</p>
<div id="kaplan2020"></div>
<p>[KMH+20] J. Kaplan, S. McCandlish, T. Henighan, T. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei.
<a href="https://arxiv.org/abs/2001.08361">Scaling laws for neural language models</a>.
Jan. 2020.</p>
<div id="karandikar2018"></div>
<p>[KBA18] S. Karandikar, D. Biancolin, and A. Amid.
<a href="https://fires.im/">FireSim</a>.
2018.</p>
<div id="karita2019"></div>
<p>[KCH+19] S. Karita, N. Chen, T. Hayashi, et al.
<a href="https://arxiv.org/abs/1909.06317">A comparative study on transformer vs RNN in speech applications</a>.
Sep. 2019.</p>
<div id="karpathy2019"></div>
<p>[Kar19] A. Karpathy.
<a href="https://karpathy.github.io/2019/04/25/recipe/">A recipe for training neural networks</a>.
Apr. 2019.</p>
<div id="karras2019"></div>
<p>[KLA19] T. Karras, S. Laine, and T. Aila.
<a href="https://arxiv.org/abs/1812.04948">A style-based generator architecture for generative adversarial networks</a>.
<em>CVPR</em>, Mar. 2019.</p>
<div id="katariya2019"></div>
<p>[KR19]  S. Katariya and A. Ramani.
<a href="https://tech.ebayinc.com/engineering/ebays-transformation-to-a-modern-ai-platform/">eBay's transformation to a modern AI platform</a>.
Dec. 2019.</p>
<div id="keskar2017"></div>
<p>[KMN+17] N. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. Tang.
<a href="https://arxiv.org/abs/1609.04836">On large-batch training for deep learning: generalization gap and sharp minima</a>.
<em>ICLR</em>, Apr. 2017.</p>
<div id="keskar2017-b"></div>
<p>[KS17] N. Keskar and R. Socher.
<a href="https://arxiv.org/abs/1712.07628">Improving generalization performance by switching from Adam to SGD</a>.
Dec. 2017.</p>
<div id="kim2005"></div>
<p>[KDT+05] J. Kim, W. Dally, B. Towles, and A. Gupta.
<a href="https://ieeexplore.ieee.org/document/1431575">Microarchitecture of a high-radix router</a>.
<em>ISCA</em>, June 2005.</p>
<div id="kim2008"></div>
<p>[KDS+08] J. Kim, W. Dally, S. Scott, and D. Abts.
<a href="https://ieeexplore.ieee.org/document/4556717">Technology-driven, highly-scalable dragonfly topology</a>.
<em>ISCA</em>, June 2008.</p>
<div id="kim2018"></div>
<p>[KWG+18] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, and R. Sayres.
<a href="https://arxiv.org/abs/1711.11279">Interpretability beyond feature attribution: quantitative testing with concept activation vectors (TCAV)</a>.
<em>ICML</em>, June 2018.</p>
<div id="kim2019"></div>
<p>[KKS+19] C. Kim, S. Kang, D. Shin, S. Choi, Y. Kim, and H. Yoo.
<a href="https://ieeexplore.ieee.org/document/8662447">A 2.1TFLOPS/W mobile deep RL accelerator with transposable PE array and experience compression</a>.
<em>ISSCC</em>, Feb. 2019.</p>
<div id="kingma2017"></div>
<p>[KB17] D. Kingma and J. Ba.
<a href="https://arxiv.org/abs/1412.6980">Adam: a method for stochastic optimization</a>.
<em>ICLR</em>, Jan. 2017.</p>
<div id="kjolstad2017"></div>
<p>[KKC+17] F. Kjolstad, S. Kamil, S. Chou, D. Lugato, and S. Amarasinghe.
<a href="http://tensor-compiler.org/kjolstad-oopsla17-tensor-compiler.pdf">The tensor algebra compiler</a>.
<em>OOPSLA</em>, Oct. 2017.</p>
<div id="klambauer2017"></div>
<p>[KUM+17] G. Klambauer, T. Unterthiner, A. Mayr, and S. Hochreiter.
<a href="https://arxiv.org/abs/1706.02515">Self-normalizing neural networks</a>.
<em>NeurIPS</em>, Dec. 2017.</p>
<div id="intel-sc2019"></div>
<p>[Kod19] R. Koduri.
<a href="https://newsroom.intel.com/news-releases/intel-unveils-new-gpu-architecture-optimized-for-hpc-ai-oneapi/">Intel unveils new GPU architecture with high-performance computing and AI acceleration, and oneAPI software stack with unified and scalable abstraction for heterogeneous architectures</a>.
<em>Intel HPC Dev. Conf.</em>, Nov. 2019.</p>
<div id="komuravelli2015"></div>
<p>[KSA+15] R. Komuravelli, M. Sinclair, J. Alsop, et al.
<a href="https://ieeexplore.ieee.org/abstract/document/7284106">Stash: have your scratchpad and cache it too</a>.
<em>ISCA</em>, Oct. 2015.</p>
<div id="konecny2017"></div>
<p>[KMY+17] J. Konecny, H. McMahan, F. Yu, P. Richtarik, A. Suresh, and D. Bacon.
<a href="https://arxiv.org/abs/1610.05492">Federated learning: strategies for improving communication efficiency</a>.
Oct. 2017.</p>
<div id="kosson2020"></div>
<p>[KCV+20] A. Kosson, V. Chiley, A. Venigalla, J. Hestness, and U. Koster.
<a href="https://arxiv.org/abs/2003.11666">Pipelined backpropagation at scale: training large models without batches</a>.
Mar. 2020.</p>
<div id="koster2017"></div>
<p>[KWW+17] U. Koster, T. Webb, X. Wang, et al.
<a href="https://arxiv.org/abs/1711.02213">Flexpoint: an adaptive numerical format for efficient training of deep neural networks</a>.
<em>NeurIPS</em>, Dec. 2017.</p>
<div id="kouw2019"></div>
<p>[KL19] W. Kouw and M. Loog.
<a href="https://arxiv.org/abs/1812.11806">An introduction to domain adaptation and transfer learning</a>.
Jan. 2019.</p>
<div id="kraska2018"></div>
<p>[KBC+18] T. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis.
<a href="https://arxiv.org/abs/1712.01208">The case for learned index structures</a>.
Apr. 2018.</p>
<div id="krizhevsky2012"></div>
<p>[KSH12] A. Krizhevsky, I. Sutskever, and G. Hinton.
<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet classification with deep convolutional neural networks</a>.
<em>NeurIPS</em>, Dec. 2012.</p>
<div id="krizhevsky2014"></div>
<p>[Kri14] A. Krizhevsky.
<a href="https://arxiv.org/abs/1404.5997">One weird trick for parallelizing convolutional neural networks</a>.
Apr. 2014.</p>
<div id="kuchaiev2018"></div>
<p>[KGG+18] O. Kuchaiev, B. Ginsburg, I. Gitman, et al.
<a href="https://arxiv.org/abs/1805.10387">Mixed-precision training for NLP and speech recognition with OpenSeq2Seq</a>.
Nov. 2018.</p>
<div id="laguna2019"></div>
<p>[LMM+19] I. Laguna, R. Marshall, K. Mohror, M. Ruefenacht, A. Skjellum, and N. Sultana.
<a href="https://dl.acm.org/doi/10.1145/3295500.3356176">A large-scale study of MPI usage in open-source HPC applications</a>.
<em>SC</em>, Nov. 2019.</p>
<div id="lan2019"></div>
<p>[LCG+19] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut.
<a href="https://arxiv.org/abs/1909.11942">ALBERT: a lite BERT for self-supervised learning of language representations</a>.
Sep. 2019.</p>
<div id="larsen2019"></div>
<p>[LS19] R. Larsen and T. Shpeisman.
<a href="https://research.google/pubs/pub48051/">TensorFlow graph optimizations</a>.
2019.</p>
<div id="lattner2004"></div>
<p>[LA04] C. Lattner and V. Adve.
<a href="https://llvm.org/pubs/2004-01-30-CGO-LLVM.html">LLVM: a compilation framework for lifelong program analysis \&amp; transformation</a>.
<em>CGO</em>, Mar. 2004.</p>
<div id="lattner2019"></div>
<p>[LP19] C. Lattner and J. Pienaar.
<a href="https://drive.google.com/file/d/1hUeAJXcAXwz82RXA5VtO5ZoH8cVQhrOK/view">MLIR primer: a compiler infrastructure for the end of Moore's Law</a>.
<em>CGO</em>, Feb. 2019.</p>
<div id="lavin2016"></div>
<p>[LG16] A. Lavin and S. Gray.
<a href="https://arxiv.org/abs/1509.09308">Fast algorithms for convolutional neural networks</a>.
<em>CVPR</em>, Sep. 2015.</p>
<div id="lecun2016"></div>
<p>[Lec16] Y. Lecun.
<a href="https://www.youtube.com/watch?v=IbjF5VjniVE">RI seminar: Yann LeCun : the next frontier in AI: unsupervised learning</a>.
Nov. 2016.</p>
<div id="lecun1998"></div>
<p>[LBB+98] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner.
<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">Gradient-based learning applied to document recognition</a>.
<em>IEEE</em>, Nov. 1998.</p>
<div id="lecun1989"></div>
<p>[LDS89] Y. Lecun, J. Denker, and S. Solla.
<a href="https://papers.nips.cc/paper/250-optimal-brain-damage">Optimal brain damage</a>.
<em>NeurIPS</em>, 1989.</p>
<div id="lecuyer2019"></div>
<p>[LAG+19] M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana.
<a href="https://ieeexplore.ieee.org/abstract/document/8835364">Certified robustness to adversarial examples with differential privacy</a>.
<em>S\&amp;P</em>, May 2019.</p>
<div id="ledig2016"></div>
<p>[LTH+16] C. Ledig, L. Theis, F. Huszar, et al.
<a href="https://arxiv.org/abs/1609.04802">Photo-realistic single image super-resolution using a generative adversarial network</a>.
<em>CVPR</em>, Sep. 2016.</p>
<div id="lee2017"></div>
<p>[LMC+17] E. Lee, D. Miyashita, E. Chai, B. Murmann, and S. Wong.
<a href="https://ieeexplore.ieee.org/document/7953288">LogNet: energy-efficient neural networks using logarithmic computation</a>.
<em>ICASSP</em>, Mar. 2017.</p>
<div id="lee2019"></div>
<p>[LLH+19] J. Lee, J. Lee, D. Han, J. Lee, G. Park, and H. Yoo.
<a href="https://ieeexplore.ieee.org/document/8662302">7.7 LNPU: a 25.3TFLOPS/W sparse deep-neural-network learning processor with fine-grained mixed precision of FP8-FP16</a>.
<em>ISSCC</em>, Feb. 2019.</p>
<div id="lee2019-c"></div>
<p>[LMR+19] K. Lee, S. Maji, A. Ravichandran, and S. Soatto.
<a href="https://arxiv.org/abs/1904.03758">Meta-learning with differentiable convex optimization</a>.
<em>CVPR</em>, Apr. 2019.</p>
<div id="lepikhin2020"></div>
<p>[LLX+20] D. Lepikhin, H. Lee, Y. Xu, et al.
<a href="https://arxiv.org/abs/2006.16668">GShard: scaling giant models with conditional computation and automatic sharding</a>.
June 2020.</p>
<div id="leverich2007"></div>
<p>[LAS+07] J. Leverich, H. Arakida, A. Solomatnikov, A. Firoozshahian, M. Horowitz, and C. Kozyrakis.
<a href="https://dl.acm.org/doi/10.1145/1273440.1250707">Comparing memory systems for chip multiprocessors</a>.
<em>ISCA</em>, June 2007.</p>
<div id="leviathan2018"></div>
<p>[LM18] Y. Leviathan and Y. Matias.
<a href="https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html">Google Duplex: an AI system for accomplishing real-world tasks over the phone</a>.
May 2018.</p>
<div id="li2019"></div>
<p>[LSZ+19] T. Li, A. Sahu, M. Zaheer, M. Sanjabi, A. Talwalkar, and V. Smith.
<a href="https://arxiv.org/abs/1812.06127">Federated optimization in heterogeneous networks</a>.
Sep. 2019.</p>
<div id="li2019-b"></div>
<p>[LCH+19] X. Li, S. Chen, X. Hu, and J. Yang.
<a href="https://arxiv.org/pdf/1801.05134.pdf">Understanding the disharmony between dropout and batch normalization by variance shift</a>.
<em>CVPR</em>, Jan. 2019.</p>
<div id="liang2018"></div>
<p>[LKH+18] D. Liang, R. Krishnan, M. Hoffman, and T. Jebara.
<a href="https://arxiv.org/abs/1802.05814">Variational autoencoders for collaborative tiltering</a>.
<em>IW3C2</em>, Feb. 2018.</p>
<div id="lillicrap2019"></div>
<p>[LHP+19] T. Lillicrap, J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra.
<a href="https://arxiv.org/abs/1509.02971">Continuous control with deep reinforcement learning</a>.
July 2019.</p>
<div id="lin2019-b"></div>
<p>[LGH19] J. Lin, C. Gan, and S. Han.
<a href="https://arxiv.org/abs/1904.08444">Defensive quantization: when efficiency meets robustness</a>.
<em>ICLR</em>, Apr. 2019.</p>
<div id="lin2016"></div>
<p>[LGH+16] T. Lin, P.Doll\'ar, R. Girshick, K. He, B. Hariharan, and S. Belongie.
<a href="https://arxiv.org/abs/1612.03144">Feature pyramid networks for object detection</a>.
<em>CVPR</em>, Dec. 2016.</p>
<div id="lin2017"></div>
<p>[LGG+17] T. Lin, P. Goyal, R. Girshick, K. He, and P. Dollar.
<a href="https://arxiv.org/abs/1708.02002">Focal loss for dense object detection</a>.
<em>ICCV</em>, Aug. 2017.</p>
<div id="lin2019"></div>
<p>[LSP+19] T. Lin, S. Stich, K. Patel, and M. Jaggi.
<a href="https://arxiv.org/abs/1808.07217">Don't use large mini-batches, use local SGD</a>.
June 2019.</p>
<div id="lin2018"></div>
<p>[LHM+18] Y. Lin, S. Han, H. Mao, Y. Wang, and W. Dally.
<a href="https://arxiv.org/abs/1712.01887">Deep gradient compression: reducing the communication bandwidth for distributed training</a>.
<em>ICLR</em>, Feb. 2018.</p>
<div id="lindstrom2018"></div>
<p>[LHL+18] P. Lindstrom, J. Hittinger, M. Larsen, S. Lloyd, and M. Salasoo.
<a href="http://helper.ipam.ucla.edu/publications/bdcws2/bdcws2_15049.pdf">Alternatives to IEEE: NextGen number formats for scientific computing</a>.
<em>IPAM</em>, Oct. 2018.</p>
<div id="liu2018"></div>
<p>[LRS+18] G. Liu, F. Reda, K. Shih, T. Wang, A. Tao, and B. Catanzaro.
<a href="https://arxiv.org/abs/1804.07723">Image inpainting for irregular holes using partial convolutions</a>.
<em>ECCV</em>, Apr. 2018.</p>
<div id="liu2018-b"></div>
<p>[LDR+18] L. Liu, S. Dean, E. Rolf, M. Simchowitz, and M. Hardt.
<a href="https://arxiv.org/abs/1803.04383">Delayed impact of fair machine learning</a>.
<em>ICML</em>, Apr. 2018.</p>
<div id="liu2018-c"></div>
<p>[LPH+18] X. Liu, J. Pool, S. Han, and W. Dally.
<a href="https://arxiv.org/abs/1802.06367">Efficient sparse Winograd convolutional neural networks</a>.
<em>ICLR</em>, Feb. 2018.</p>
<div id="liu2019-f"></div>
<p>[LSY19] H. Liu, K. Simonyan, and Y. Yang.
<a href="https://arxiv.org/abs/1806.09055">DARTS: differentiable architecture search</a>.
<em>ICLR</em>, Apr. 2019.</p>
<div id="liu2019"></div>
<p>[LJH+19] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han.
<a href="https://arxiv.org/abs/1908.03265">On the variance of the adaptive learning rate and beyond</a>.
Aug. 2019.</p>
<div id="liu2019-e"></div>
<p>[LZL+19] L. Liu, J. Zhu, Z. Li, Y. Lu, Y. Deng, J. Han, S. Yin, and S. Wei.
<a href="https://dl.acm.org/doi/10.1145/3357375">A survey of coarse-grained reconfigurable architecture and design: taxonomy, challenges, and applications</a>.
<em>CSUR</em>, Oct. 2019.</p>
<div id="liu2015"></div>
<p>[LAE+15] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu, and A. Berg.
<a href="https://arxiv.org/abs/1512.02325">SSD: single shot multibox detector</a>.
<em>ECCV</em>, Dec. 2015.</p>
<div id="liu2019-c"></div>
<p>[LOG+19] Y. Liu, M. Ott, N. Goyal, et al.
<a href="https://arxiv.org/abs/1907.11692">RoBERTa: a robustly optimized BERT pretraining approach</a>.
July 2019.</p>
<div id="liu2019-d"></div>
<p>[LSZ+19] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell.
<a href="https://arxiv.org/abs/1810.05270">Rethinking the value of network pruning</a>.
<em>ICLR</em>, Mar. 2019.</p>
<div id="polyhedral2020"></div>
<p>[Llv20] LLVM.
<a href="https://github.com/llvm/llvm-project/blob/master/mlir/docs/Rationale/RationaleSimplifiedPolyhedralForm.md">MLIR: the case for a simplified polyhedral form</a>.
2020.</p>
<div id="long2014"></div>
<p>[LSD14] J. Long, E. Shelhamer, and T. Darrell.
<a href="https://arxiv.org/abs/1411.4038">Fully convolutional networks for semantic segmentation</a>.
<em>CVPR</em>, Nov. 2014.</p>
<div id="lorica2019"></div>
<p>[Lor19] B. Lorica.
<a href="https://www.oreilly.com/content/one-simple-graphic-researchers-love-pytorch-and-tensorflow/?fbclid=IwAR3kYmlyD7zky37IYFu0cafQn7yemhl8P-7MNyB30z0q5RDzxcTOrP8kxDk">One simple graphic: researchers love PyTorch and TensorFlow</a>.
July 2019.</p>
<div id="loshchilov2017"></div>
<p>[LH17] I. Loshchilov and F. Hutter.
<a href="https://arxiv.org/abs/1608.03983">SGDR: stochastic gradient descent with warm restarts</a>.
<em>ICLR</em>, May 2017.</p>
<div id="loshchilov2019"></div>
<p>[LH19] I. Loshchilov and F. Hutter.
<a href="https://arxiv.org/abs/1711.05101">Decoupled weight decay regularization</a>.
<em>ICLR</em>, Jan. 2019.</p>
<div id="lovely2019"></div>
<p>[Lov19] S. Lovely.
<a href="https://cordcutting.com/blog/how-many-titles-are-available-on-netflix-in-your-country/">How many titles are available on Netflix in your country?</a>.
May 2019.</p>
<div id="luong2015"></div>
<p>[LPM15] M. Luong, H. Pham, and C. Manning.
<a href="https://arxiv.org/abs/1508.04025">Effective approaches to attention-based neural machine translation</a>.
Aug. 2015.</p>
<div id="lym2019"></div>
<p>[LCZ+19] S. Lym, E. Choukse, S. Zangeneh, W. Wen, S. Sanghavi, and M. Erez.
<a href="https://dl.acm.org/doi/10.1145/3295500.3356156">PruneTrain: fast neural network training by dynamic sparse model reconfiguration</a>.
<em>SC</em>, Nov. 2019.</p>
<div id="ma2019"></div>
<p>[MYM+19] L. Ma, Z. Yang, Y. Miao, J. Xue, M. Wu, L. Zhou, and Y. Dai.
<a href="https://www.microsoft.com/en-us/research/publication/neugraph-parallel-deep-neural-network-computation-on-large-graphs/">NeuGraph: parallel deep neural network computation on large graphs</a>.
<em>ATC</em>, July 2019.</p>
<div id="madry2019"></div>
<p>[MMS+19] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu.
<a href="https://arxiv.org/abs/1706.06083">Towards deep learning models resistant to adversarial attacks</a>.
<em>ICLR</em>, Sep. 2019.</p>
<div id="mao2017"></div>
<p>[MHP+17] H. Mao, S. Han, J. Pool, W. Li, X. Liu, Y. Wang, and W. Dally.
<a href="https://arxiv.org/abs/1705.08922">Exploring the regularity of sparse structure in convolutional neural networks</a>.
<em>NeurIPS</em>, Dec. 2017.</p>
<div id="masters2018"></div>
<p>[ML18] D. Masters and C. Luschi.
<a href="https://arxiv.org/abs/1804.07612">Revisiting small batch training for deep neural networks</a>.
Apr. 2018.</p>
<div id="mccandlish2018"></div>
<p>[MKA+18] S. McCandlish, J. Kaplan, D. Amodei, et al.
<a href="https://arxiv.org/abs/1812.06162">An empirical model of large-batch training</a>.
Feb. 2017.</p>
<div id="mcmahan2017"></div>
<p>[MMR+17] H. McMahan, E. Moore, D. Ramage, S. Hampson, and B. Arcas.
<a href="https://arxiv.org/abs/1602.05629">Communication-efficient learning of deep networks from decentralized data</a>.
Feb. 2017.</p>
<div id="mellempudi2019"></div>
<p>[MSD+19] N. Mellempudi, S. Srinivasan, D. Das, and B. Kaul.
<a href="https://arxiv.org/abs/1905.12334">Mixed precision training with 8-bit floating point</a>.
May 2019.</p>
<div id="meng2017"></div>
<p>[MC17] D. Meng and H. Chen.
<a href="https://arxiv.org/abs/1705.09064">MagNet: a two-pronged defense against adversarial examples</a>.
<em>CCS</em>, Sep. 2017.</p>
<div id="merity2019"></div>
<p>[Mer19] S. Merity.
<a href="https://arxiv.org/abs/1911.11423">Single headed attention RNN: stop thinking with your head</a>.
Nov. 2019.</p>
<div id="metaflow2019"></div>
<p>[Met19] MetaFlow.
<a href="https://metaflow.org/">A framework for real-life data science</a>.
2019.</p>
<div id="metaflow-aws2019"></div>
<p>[Met19b] Metaflow.
<a href="https://docs.metaflow.org/metaflow-on-aws/metaflow-on-aws">Metaflow on AWS</a>.
2019.</p>
<div id="michel2019"></div>
<p>[MLN19] P. Michel, O. Levy, and G. Neubig.
<a href="https://arxiv.org/abs/1905.10650">Are sixteen heads really better than one?</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="mlnet2020"></div>
<p>[Mic20] Microsoft.
<a href="https://docs.microsoft.com/en-us/dotnet/machine-learning/">ML.NET Documentation</a>.
2020.</p>
<div id="microsoft-azure2020"></div>
<p>[Mic20b] Microsoft.
<a href="https://azure.microsoft.com/en-us/services/cognitive-services/">Azure Cognitive services</a>.
2020.</p>
<div id="migacz2017"></div>
<p>[Mig17] S. Migacz.
<a href="https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">8-bit inference with TensorRT</a>.
<em>GTC</em>, May 2017.</p>
<div id="mikami2019"></div>
<p>[MSU+19] H. Mikami, H. Suganuma, P. U-chupala, Y. Tanaka, and Y. Kageyama.
<a href="https://arxiv.org/abs/1811.05233">Massively distributed SGD: ImageNet/ResNet-50 training in a flash</a>.
Mar. 2019.</p>
<div id="mikolov2013"></div>
<p>[MSC+13] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean.
<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed representations of words and phrases and their compositionality</a>.
<em>NeurIPS</em>, Dec. 2013.</p>
<div id="milletari2016"></div>
<p>[MNA16] F. Milletari, N. Navab, and S. Ahmadi.
<a href="https://arxiv.org/abs/1606.04797">V-Net: fully convolutional neural networks for volumetric medical image segmentation</a>.
<em>3DV</em>, June 2016.</p>
<div id="mirhoseini2018"></div>
<p>[MGP+18] A. Mirhoseini, A. Goldie, H. Pham, B. Steiner, Q. Le, and J. Dean.
<a href="https://openreview.net/pdf?id=Hkc-TeZ0W">A hierarchical model for device placement</a>.
<em>ICLR</em>, 2018.</p>
<div id="mirzadeh2019"></div>
<p>[MFL+19] S. Mirzadeh, M. Farajtabar, A. Li, N. Levine, A. Matsukawa, and H. Ghasemzadeh.
<a href="https://arxiv.org/abs/1902.03393">Improved knowledge distillation via teacher assistant</a>.
<em>AAAI</em>, Dec. 2019.</p>
<div id="mitchell2019"></div>
<p>[MWZ+19] M. Mitchell, S. Wu, A. Zaldivar, et al.
<a href="https://arxiv.org/abs/1810.03993">Model cards for model reporting</a>.
Jan. 2019.</p>
<div id="mitliagkas2016"></div>
<p>[MZH+16] I. Mitliagkas, C. Zhang, S. Hadjis, and C. Re.
<a href="https://arxiv.org/abs/1605.09774">Asynchrony begets momentum, with an application to deep learning</a>.
<em>Comm., Control, and Comp.</em>, Nov. 2016.</p>
<div id="mlflow2020"></div>
<p>[Mlf20] MLFlow.
<a href="https://mlflow.org/">An open source platform for the machine learning lifecycle</a>.
2020.</p>
<div id="mlperf2018"></div>
<p>[Mlp18] MLPerf.
<a href="https://mlperf.org/">MLPerf</a>.
2018.</p>
<div id="mnih2016"></div>
<p>[MBM+16] V. Mnih, A. Badia, M. Mirza, et al.
<a href="https://arxiv.org/abs/1602.01783">Asynchronous methods for deep reinforcement learning</a>.
<em>ICML</em>, June 2016.</p>
<div id="mnih2013"></div>
<p>[KSe+13] V. Mnih, K. Kavukcuoglu, D. Silver, et al.
<a href="https://arxiv.org/abs/1312.5602">Playing atari with deep reinforcement learning</a>.
Dec. 2013.</p>
<div id="mnih2015"></div>
<p>[MKS+15] V. Mnih, K. Kavukcuoglu, D. Silver, et al.
<a href="https://www.nature.com/articles/nature14236">Human-level control through deep reinforcement learning</a>.
<em>Nature</em>, Feb. 2015.</p>
<div id="moore1965"></div>
<p>[Moo65] G. Moore.
<a href="https://ieeexplore.ieee.org/document/4785860">Cramming more components onto integrated circuits</a>.
<em>Electronics</em>, Apr. 1965.</p>
<div id="moore1975"></div>
<p>[Moo75] G. Moore.
<a href="https://ieeexplore.ieee.org/document/4804410">Progress in digital integrated electronics</a>.
<em>Technical Digest</em>, Sep. 1975.</p>
<div id="mor2020"></div>
<p>[MPG+20] R. Mor, E. Peterfreund, M. Gavish, and A. Globerson.
<a href="https://openreview.net/forum?id=BkgzMCVtPB">Optimal strategies against generative attacks</a>.
<em>ICLR</em>, Feb. 2020.</p>
<div id="morcos2019"></div>
<p>[MYP+19] A. Morcos, H. Yu, M. Paganini, and Y. Tian.
<a href="https://arxiv.org/abs/1906.02773">One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="morgan2019"></div>
<p>[Mor19] T. Morgan.
<a href="https://www.nextplatform.com/2019/09/03/nvidia-shows-off-tech-chops-with-rc18-inference-chip/">Nvidia shows off tech chops with RC18 inference chip</a>.
<em>Next Platform</em>, Sep. 2019.</p>
<div id="moritz2018"></div>
<p>[MNW+18] P. Moritz, R. Nishihara, S. Wang, et al.
<a href="https://arxiv.org/abs/1712.05889">Ray: a distributed framework for emerging AI applications</a>.
<em>OSDI</em>, Sep. 2018.</p>
<div id="mosic2017"></div>
<p>[Mos17] R. Mosic.
<a href="https://medium.com/@ranko.mosic/reinforcement-learning-based-trading-application-at-jp-morgan-chase-f829b8ec54f2">Deep reinforcement learning based trading application at JP Morgan Chase</a>.
July 2017.</p>
<div id="munkhdalai2017"></div>
<p>[MY17] T. Munkhdalai and H. Yu.
<a href="https://arxiv.org/abs/1703.00837">Meta networks</a>.
<em>ICML</em>, June 2017.</p>
<div id="nagel2019"></div>
<p>[NvB+19] M. Nagel, M. van Baalen, T. Blankevoort, and M. Welling.
<a href="https://arxiv.org/abs/1906.04721">Data-free quantization through weight equalization and bias correction</a>.
<em>CVPR</em>, Nov. 2019.</p>
<div id="nagy2018"></div>
<p>[NIG+18] D. Nagy, G. Indalecio, A. Garcia-Loureiro, M. Elmessary, K. Kalna, and N. Seoane.
<a href="https://ieeexplore.ieee.org/document/8288601">FinFET versus gate-all-around nanowire FET: performance, scaling, and variability</a>.
<em>EDS</em>, Feb. 2018.</p>
<div id="nakkiran2019-b"></div>
<p>[Nak19] P. Nakkiran.
<a href="https://arxiv.org/abs/1901.00532">Adversarial robustness may be at odds with simplicity</a>.
Jan. 2019.</p>
<div id="nakkiran2020"></div>
<p>[NKB+20] P. Nakkiran, G. Kaplun, Y. Bansal, T. Yang, B. Barak, and I. Sutskever.
<a href="https://openreview.net/pdf?id=B1g5sA4twr">Deep double descent: where bigger models and more data hurt</a>.
<em>ICLR</em>, Apr. 2020.</p>
<div id="narayanan2019"></div>
<p>[Nar19] N. Narayanan.
<a href="https://www.cs.princeton.edu/ arvindn/talks/MIT-STS-AI-snakeoil.pdf">How to recognize AI snake oil</a>.
2019.</p>
<div id="nassif2019"></div>
<p>[NSA+19] A. Nassif, I. Shahin, I. Attili, M. Azzeh, and K. Shaalan.
<a href="https://ieeexplore.ieee.org/document/8632885">Speech recognition using deep neural networks: a systematic review</a>.
<em>Access</em>, 2019.</p>
<div id="naumov2019"></div>
<p>[NMS+19] M. Naumov, D. Mudigere, H. Shi, et al.
<a href="https://arxiv.org/pdf/1906.00091.pdf">Deep learning recommendation model for personalization and recommendation systems</a>.
May 2019.</p>
<div id="naumov2020"></div>
<p>[NKM+20] M. Naumov, J. Kim, D. Mudigere, et al.
<a href="https://arxiv.org/abs/2003.09518">Deep learning training in Facebook data centers: design of scale-up and scale-out systems</a>.
Mar. 2020.</p>
<div id="nayak2019"></div>
<p>[Nay19] P. Nayak.
<a href="https://www.blog.google/products/search/search-language-understanding-bert/">Understanding searches better than ever before</a>.
Oct. 2019.</p>
<div id="neftci2019"></div>
<p>[NMZ19] E. Neftci, H. Mostafa, and F. Zenke.
<a href="https://ieeexplore.ieee.org/document/8891809">Surrogate gradient learning in spiking neural networks: bringing the power of gradient-based optimization to spiking neural networks</a>.
<em>SPM</em>, Nov. 2019.</p>
<div id="neal1995"></div>
<p>[Nea95] R. Neal.
<a href="http://www.cs.toronto.edu/pub/radford/thesis.pdf">Bayesian learning for neural networks</a>.
Ph.D. Thesis, University of Toronto, 1995.</p>
<div id="nimbix2020"></div>
<p>[Nim20] Nimbix.
<a href="https://www.nimbix.net/groq">Groq tensor streaming processors</a>.
2020.</p>
<div id="novikova2017"></div>
<p>[NDC+17] J. Novikova, O. Dusek, A. Curry, and V. Rieser.
<a href="https://arxiv.org/abs/1707.06875">Why we need new evaluation metrics for NLG</a>.
July 2017.</p>
<div id="nurvitadhi2019"></div>
<p>[NKJ+19] E. Nurvitadhi, D. Kwon, A. Jafari, et al.
<a href="https://ieeexplore.ieee.org/document/8735536">Why compete when you can work together: FPGA-ASIC integration for persistent RNNs</a>.
<em>FCCM</em>, May 2019.</p>
<div id="nvidia-sass2015"></div>
<p>[Nvi15] Nvidia.
<a href="https://docs.nvidia.com/nsight-visual-studio-edition/4.6/Nsight_Visual_Studio_Edition_User_Guide.htm">PTX and SASS assembly debugging</a>.
2015.</p>
<div id="nvidia-rapids2020"></div>
<p>[Nvi20] Nvidia.
<a href="https://developer.nvidia.com/rapids">RAPIDS</a>.
2020.</p>
<div id="nvidia-t42020"></div>
<p>[Nvi20b] Nvidia.
<a href="https://www.nvidia.com/en-us/data-center/tesla-t4/">T4</a>.
2020.</p>
<div id="nvidia-perf2020"></div>
<p>[Nvi20c] Nvidia.
<a href="https://developer.nvidia.com/deep-learning-performance-training-inference">Data center deep learning product performance</a>.
July 2020.</p>
<div id="olah2018"></div>
<p>[OSJ+18] C. Olah, A. Satyanarayan, I. Johnson, S. Carter, L. Schubert, K. Ye, and A. Mordvintsev.
<a href="https://distill.pub/2018/building-blocks/">The building blocks of interpretability</a>.
2018.</p>
<div id="ojala2002"></div>
<p>[OPM02] T. Ojala, M. Pietik\"ainen, and T. Maenpaa.
<a href="https://dl.acm.org/doi/10.1109/TPAMI.2002.1017623">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</a>.
<em>PAMI</em>, July 2002.</p>
<div id="openai2018"></div>
<p>[Ope18] OpenAI.
<a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">Kinds of RL algorithms</a>.
2018.</p>
<div id="orr1999"></div>
<p>[Orr99] G. Orr.
<a href="https://www.willamette.edu/ gorr/classes/cs449/momrate.html">Momentum and Learning Rate Adaptation</a>.
{Willamette University}, 1999.</p>
<div id="padmanabhan2019"></div>
<p>[Pad19] S. Padmanabhan.
<a href="https://tech.ebayinc.com/research/building-a-product-catalog-ebays-university-machine-learning-competition/">Building a product catalog: eBay's university machine learning competition</a>.
Oct. 2019.</p>
<div id="paganini2018"></div>
<p>[PdN18] M. Paganini, L. de Oliveira, and B. Nachman.
<a href="https://arxiv.org/abs/1705.02355">Accelerating science with generative adversarial networks: an application to 3D particle showers in multi-layer calorimeters</a>.
<em>PRL</em>, Jan. 2018.</p>
<div id="pan2010"></div>
<p>[PY10] S. Pan and Q. Yang.
<a href="https://ieeexplore.ieee.org/document/5288526">A survey on transfer learning</a>.
<em>TKDE</em>, Oct. 2010.</p>
<div id="papernot2016"></div>
<p>[PMW+16] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami.
<a href="https://arxiv.org/abs/1511.04508">Distillation as a defense to adversarial perturbations against deep neural networks</a>.
<em>S\&amp;P</em>, Mar. 2016.</p>
<div id="park2019"></div>
<p>[PCZ+19] D. Park, W. Chan, Y. Zhang, C. Chiu, B. Zoph, E. Cubuk, and Q. Le.
<a href="https://arxiv.org/abs/1904.08779">SpecAugment: a simple data augmentation method for automatic speech recognition</a>.
Apr. 2019.</p>
<div id="park2018"></div>
<p>[PNB+18] J. Park, M. Naumov, P. Basu, S. Deng, et al.
<a href="https://arxiv.org/pdf/1811.09886.pdf">Deep learning inference in Facebook data centers: characterization, performance optimizations and hardware implications</a>.
Nov. 2018.</p>
<div id="pedram2017"></div>
<p>[PRH+17] A. Pedram, S. Richardson, M. Horowitz, S. Galal, and S. Kvatinsky.
<a href="https://ieeexplore.ieee.org/document/7479518">Dark memory and accelerator-rich system optimization in the dark silicon era</a>.
<em>D\&amp;T</em>, May 2016.</p>
<div id="pellauer2019"></div>
<p>[PSC+19] M. Pellauer, Y. Shao, J. Clemons, et al.
<a href="https://dl.acm.org/doi/10.1145/3297858.3304025">Buffets: an efficient and composable storage idiom for explicit decoupled data orchestration</a>.
<em>ASPLOS</em>, Apr. 2019.</p>
<div id="pennington2014"></div>
<p>[PSM14] J. Pennington, R. Socher, and C. Manning.
<a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: global vectors for word representation</a>.
<em>EMNLP</em>, 2014.</p>
<div id="pham2018"></div>
<p>[PGZ+18] H. Pham, M. Guan, B. Zoph, Q. V. Le, and J. Dean.
<a href="https://arxiv.org/abs/1802.03268">Efficient neural architecture search via parameter sharing</a>.
Feb. 2018.</p>
<div id="Phi2018"></div>
<p>[Phi18] M. Phi.
<a href="https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21">Illustrated guide to LSTM's and GRU's: a step by step explanation.</a>
<em>TDS</em>. Sep. 2018.</p>
<div id="ping2017"></div>
<p>[PPG+17] W. Ping, K. Peng, A. Gibiansky, S. Arik, A. Kannan, S. Narang, J. Raiman, and J.Miller.
<a href="https://arxiv.org/abs/1710.07654">Deep Voice 3: scaling text-to-speech with convolutional sequence learning</a>.
Oct. 2017.</p>
<div id="ping2018"></div>
<p>[PPC18] W. Ping, K. Peng, and J. Chen.
<a href="https://arxiv.org/abs/1807.07281">ClariNet: parallel wave generation in end-to-end text-to-speech</a>.
July 2018.</p>
<div id="pollack1999"></div>
<p>[Pol99] F. Pollack.
<a href="https://dl.acm.org/doi/10.5555/320080.320082">New microarchitecture challenges in the coming generations of CMOS process technologies</a>.
<em>MICRO</em>, Nov. 1999.</p>
<div id="prabhakar2017"></div>
<p>[PZK+17] R. Prabhakar, Y. Zhang, D. Koeplinger, et al.
<a href="https://dl.acm.org/doi/10.1145/3140659.3080256">Plasticine: a reconfigurable architecture for parallel patterns</a>.
<em>SIGARCH</em>, June 2017.</p>
<div id="pratap2018"></div>
<p>[PHX+18] V. Pratap, A. Hannun, Q. Xu, et al.
<a href="https://arxiv.org/abs/1812.07625">wav2letter++: the fastest open-source speech recognition system</a>.
Dec. 2018.</p>
<div id="qian1999"></div>
<p>[Qia99] N. Qian.
<a href="https://www.ncbi.nlm.nih.gov/pubmed/12662723">On the momentum term in gradient descent learning algorithms</a>.
Jan. 1999.</p>
<div id="radford2015"></div>
<p>[RMC15] A. Radford, L. Metz, and S. Chintala.
<a href="https://arxiv.org/abs/1511.06434">Unsupervised representation learning with deep convolutional generative adversarial networks</a>.
<em>ICIGP</em>, Nov. 2015.</p>
<div id="radford2019"></div>
<p>[RWC+19] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever.
<a href="https://openai.com/blog/better-language-models/">Language models are unsupervised multitask learners</a>.
2019.</p>
<div id="ragan-kelley2013"></div>
<p>[RBA+13] J. Ragan-Kelley, C. Barnes, A. Adams, S. Paris, F. Durand, and S. Amarasinghe.
<a href="https://dl.acm.org/doi/10.1145/2491956.2462176">Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines</a>.
<em>PLDI</em>, June 2013.</p>
<div id="raffel2019"></div>
<p>[RSR+19] C. Raffel, N. Shazeer, A. Roberts, et al.
<a href="https://arxiv.org/abs/1910.10683">Exploring the limits of transfer learning with a unified text-to-text transformer</a>.
Oct. 2019.</p>
<div id="rakelly2019"></div>
<p>[RZQ+19] K. Rakelly, A. Zhou, D. Quillen, C. Finn, and S. Levine.
<a href="https://arxiv.org/abs/1903.08254">Efficient off-policy meta-reinforcement learning via probabilistic context variables</a>.
Mar. 2019.</p>
<div id="rastegari2016"></div>
<p>[ROR+16] M. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi.
<a href="https://arxiv.org/abs/1603.05279">XNOR-Net: ImageNet classification using binary convolutional neural networks</a>.
<em>ECCV</em>, Sep. 2016.</p>
<div id="raza2019"></div>
<p>[RD19] S. Raza and C. Ding.
<a href="https://www.sciencedirect.com/science/article/pii/S1574013718302120">Progress in context-aware recommender systems-an overview</a>.
Jan. 2019.</p>
<div id="real2019"></div>
<p>[RAH+19] E. Real, A. Aggarwal, Y. Huang, and Q. Le.
<a href="https://arxiv.org/abs/1802.01548">Regularized evolution for image classifier architecture search</a>.
<em>AAAI</em>, Feb. 2019.</p>
<div id="reddi2019"></div>
<p>[RKK19] S. Reddi, S. Kale, and S. Kumar.
<a href="https://arxiv.org/abs/1904.09237">On the convergence of Adam and beyond</a>.
<em>ICLR</em>, Apr. 2019.</p>
<div id="redmon2016"></div>
<p>[RDG+16] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi.
<a href="https://arxiv.org/pdf/1506.02640.pdf">You only look once: unified, real-time object detection</a>.
<em>CVPR</em>, 2016.</p>
<div id="redmon2018"></div>
<p>[RF18] J. Redmon and A. Farhadi.
<a href="https://arxiv.org/abs/1804.02767">YOLOv 3: an incremental improvement</a>.
Apr. 2018.</p>
<div id="ren2015"></div>
<p>[RHG+15] S. Ren, K. He, R. Girshick, and J. Sun.
<a href="https://arxiv.org/abs/1506.01497">Faster R-CNN: towards real-time object detection with region proposal networks</a>.
<em>NeurIPS</em>, Dec. 2015.</p>
<div id="renggli2019"></div>
<p>[RAA+19] C. Renggli, S. Ashkboos, M. Aghagolzadeh, D. Alistarh, and T. Hoefler.
<a href="https://arxiv.org/abs/1802.08021">SparCML: high-performance sparse communication for machine learning</a>.
<em>SC</em>, Aug. 2019.</p>
<div id="rodriguez2018"></div>
<p>[RKL+18] A. Rodriguez, T. Kacprzak, A. Lucchi, et al.
<a href="https://arxiv.org/abs/1801.09070">Fast cosmic web simulations with generative adversarial networks</a>.
<em>CompAC</em>, Nov. 2018.</p>
<div id="rogers2009"></div>
<p>[RKB+09] B. Rogers, A. Krishna, G. Bell, K. Vu, X. Jiang, and Y. Solihin.
<a href="https://dl.acm.org/doi/10.1145/1555815.1555801">Scaling the bandwidth wall: challenges in and avenues for CMP scaling</a>.
<em>SIGARCH</em>, Jun. 2009.</p>
<div id="rolnick2019"></div>
<p>[RDK+19] D. Rolnick, P. Donti, L. Kaack, et al.
<a href="https://arxiv.org/abs/1906.05433">Tackling climate change with machine learning</a>.
Nov. 2019.</p>
<div id="rolnick2019-b"></div>
<p>[RDK+19] D. Rolnick, P. Donti, L. Kack, et al.
<a href="https://www.climatechange.ai/NeurIPS2019_workshop.html">Tackling climate change with machine learning workshop</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="ronneberger2015"></div>
<p>[RFB15] O. Ronneberger, P. Fischer, and T. Brox.
<a href="https://arxiv.org/abs/1505.04597">U-Net convolutional networks for biomedical image segmentation</a>.
May 2015.</p>
<div id="rosset2020"></div>
<p>[Ros20] C. Rosset.
<a href="https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/">Turing-NLG: a 17-billion-parameter language model by Microsoft</a>.
Feb. 2020.</p>
<div id="roune2019"></div>
<p>[RXT19] B. Roune and XLA Team.
<a href="https://drive.google.com/file/d/1AfNoznbCIejQLErblXF7CV_Wk4cJypuC/view">Compiling ML with XLA</a>.
Feb. 2019.</p>
<div id="roy2019"></div>
<p>[RJP19] K. Roy, A. Jaiswal, and P. Panda.
<a href="https://doi.org/10.1038/s41586-019-1677-2">Towards spike-based machine intelligence with neuromorphic computing</a>.
<em>Nature</em>, 2019.</p>
<div id="ruder2017"></div>
<p>[Rud17] S. Ruder.
<a href="https://arxiv.org/abs/1706.05098">An overview of multi-task learning in deep neural networks</a>.
June 2017.</p>
<div id="rupp2020"></div>
<p>[Rup20] K. Rupp.
<a href="https://github.com/karlrupp/microprocessor-trend-data">Microprocessor trend data</a>.
2020.</p>
<div id="russakovsky2015"></div>
<p>[RDS+15] O. Russakovsky, J. Deng, H. Su, et al.
<a href="http://www.image-net.org/challenges/LSVRC/">Large scale visual recognition challenge</a>.
<em>IJCV</em>, 2015.</p>
<div id="rusu2019"></div>
<p>[RRS+19] A. Rusu, D. Rao, J. Sygnowski, O. Vinyals, R. Pascanu, S. Osindero, and R. Hadsell.
<a href="https://arxiv.org/abs/1807.05960">Meta-learning with latent embedding optimization</a>.
<em>ICLR</em>, Mar. 2019.</p>
<div id="samsung2016"></div>
<p>[Sam16] Samgsung.
<a href="https://news.samsung.com/global/samsung-begins-mass-producing-worlds-fastest-dram-based-on-newest-high-bandwidth-memory-hbm-interface">Samsung begins mass producing world's fastest DRAM-based on newest high bandwidth memory (HBM) interface</a>.
2016.</p>
<div id="sanders2009"></div>
<p>[SST09] P. Sanders, J. Speck, and J. Traff.
<a href="https://www.sciencedirect.com/science/article/pii/S0167819109000957">Two-tree algorithms for full bandwidth broadcast, reduction and scan</a>.
Sep. 2009.</p>
<div id="sanh2019"></div>
<p>[SDC+19] V. Sanh, L. Debut, J. Chaumond, and T. Wolf.
<a href="https://arxiv.org/abs/1910.01108">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</a>.
Oct. 2019.</p>
<div id="sanh2019-b"></div>
<p>[San19] V. Sanh.
<a href="https://medium.com/huggingface/distilbert-8cf3380435b5">Smaller, faster, cheaper, lighter: introducing DistilBERT, a distilled version of BERT</a>.
<em>Medium</em>, Aug. 2019.</p>
<div id="sasaki2019"></div>
<p>[Sas19] K. Sasaki.
<a href="https://www.lewuathe.com/federated-learning-with-tensorflow.html">Federated Learning with TensorFlow</a>.
2019.</p>
<div id="sato2017"></div>
<p>[SYP17] K. Sato, C. Young, and D. Patterson.
<a href="https://cloud.google.com/blog/products/gcp/an-in-depth-look-at-googles-first-tensor-processing-unit-tpu">An in-depth look at Google's first Tensor Processing Unit (TPU)</a>.
May 2017.</p>
<div id="scarselli2009"></div>
<p>[SGT+09] F. Scarselli, M. Gori, A. Tsoi, M. Hagenbuchner, and G. Monfardini.
<a href="https://ieeexplore.ieee.org/document/4700287">The graph neural network model</a>.
<em>TNNLS</em>, Jan. 2009.</p>
<div id="schalkwyk2019"></div>
<p>[Sch19] J. Schalkwyk.
<a href="https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html">An all-neural on-device speech recognizer</a>.
Mar. 2019.</p>
<div id="schrittwieser2020"></div>
<p>[SAH+20] J. Schrittwieser, I. Antonoglou, T. Hubert, et al.
<a href="https://arxiv.org/abs/1911.08265">Mastering Atari, Go, Chess and Shogi by planning with a learned model</a>.
Feb. 2020.</p>
<div id="schroff2015"></div>
<p>[SKP15] F. Schroff, D. Kalenichenko, and J. Philbin.
<a href="https://arxiv.org/abs/1503.03832">FaceNet: a unified embedding for face recognition and clustering</a>.
<em>CVPR</em>, Mar. 2015.</p>
<div id="schulman2017"></div>
<p>[SLM+17] J. Schulman, S. Levine, P. Moritz, M. Jordan, and P. Abbeel.
<a href="https://arxiv.org/abs/1502.05477">Trust region policy optimization</a>.
Apr. 2017.</p>
<div id="seide2014"></div>
<p>[SFD+14] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu.
<a href="https://www.microsoft.com/en-us/research/publication/1-bit-stochastic-gradient-descent-and-application-to-data-parallel-distributed-training-of-speech-dnns/">1-bit stochastic gradient descent and application to data-parallel distributed training of speech DNNs</a>.
<em>Int' Speech Comm. Association</em>, Sep. 2014.</p>
<div id="sergeev2018"></div>
<p>[SDB18] A. Sergeev and M. Del Balso.
<a href="https://arxiv.org/abs/1802.05799">Horovod: fast and easy distributed deep learning in TensorFlow</a>.
Feb. 2018.</p>
<div id="sennrich2015"></div>
<p>[SHB15] R. Sennrich, B. Haddow, and A. Birch.
<a href="https://arxiv.org/abs/1508.07909">Neural machine translation of rare words with subword units</a>.
Aug. 2015.</p>
<div id="seo2016"></div>
<p>[SKF+16] M. Seo, A. Kembhavi, A. Farhadi, and H. Hajishirzi.
<a href="https://arxiv.org/abs/1611.01603">Bidirectional attention flow for machine comprehension</a>.
Nov. 2016.</p>
<div id="shallue2019"></div>
<p>[SLA+19] C. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and G. Dahl.
<a href="https://arxiv.org/abs/1811.03600">Measuring the effects of data parallelism on neural network training</a>.
<em>JMLR</em>, July 2019.</p>
<div id="sharan2018"></div>
<p>[SWR18] Y. Sharan, H. Wang, and S. Rath.
<a href="https://tech.ebayinc.com/research/gui-testing-powered-by-deep-learning/">GUI testing powered by deep learning</a>.
<em>eBay Tech Blog</em>, June 2018.</p>
<div id="shazeer2018"></div>
<p>[SCP+18] N. Shazeer, Y. Cheng, N. Parmar, et al.
<a href="https://github.com/tensorflow/mesh">Mesh-TensorFlow: deep learning for supercomputers</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="shen2017"></div>
<p>[SPW+17] J. Shen, R. Pang, R. Weiss, et al.
<a href="https://arxiv.org/abs/1712.05884">Natural TTS synthesis by conditioning WaveNet on Mel Spectrogram predictions</a>.
<em>ICASSP</em>, Dec. 2017.</p>
<div id="shen2019"></div>
<p>[SDY+19] S. Shen, Z. Dong, J. Ye, L. Ma, Z. Yao, A. Gholami, M. Mahoney, and K. Keutzer.
<a href="https://arxiv.org/abs/1909.05840">Q-BERT: Hessian based ultra low precision quantization of BERT</a>.
Sep. 2019.</p>
<div id="sheth2018"></div>
<p>[She18] R. Sheth.
<a href="https://cloud.google.com/blog/products/ai-machine-learning/introducing-pytorch-across-google-cloud">Introducing PyTorch across Google Cloud</a>.
Oct. 2018.</p>
<div id="shickel2019"></div>
<p>[SLA+19] B. Shickel, T. Loftus, L. Adhikari, T. Ozrazgat-Baslanti, A. Bihorac, and P. Rashidi.
<a href="https://www.nature.com/articles/s41598-019-38491-0">DeepSOFA: a continuous acuity score for critically ill patients using clinically interpretable deep learning</a>.
Feb. 2019.</p>
<div id="shoeybi2019"></div>
<p>[SPP+19] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro.
<a href="https://arxiv.org/abs/1909.08053">Megatron LM training multi billion parameter language models using model parallelism</a>.
Oct. 2019.</p>
<div id="shpeisman2019"></div>
<p>[SL19] T. Shpeisman and C. Lattner.
<a href="https://www.youtube.com/watch?v=qzljG6DKgic&amp;t=22m20s">MLIR: multi-level intermediate representation for compiler infrastructure</a>.
Apr. 2019.</p>
<div id="silver2016"></div>
<p>[SHM+16] D. Silver, A. Huang, C. Maddison, et al.
<a href="https://www.nature.com/articles/nature16961">Mastering the game of Go with deep neural networks and tree search</a>.
<em>Nature</em>, Jan. 2016.</p>
<div id="silver2017"></div>
<p>[SSS+17] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, et al.
<a href="https://www.nature.com/articles/nature24270">Mastering the game of Go without human knowledge</a>.
<em>Nature</em>, Oct. 2017.</p>
<div id="silver2018"></div>
<p>[SSS+18] D. Silver, J. Schrittwieser, K. Simonyan, et al.
<a href="https://science.sciencemag.org/content/362/6419/1140.full">A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</a>.
<em>Science</em>, Dec. 2018.</p>
<div id="simonyan2014"></div>
<p>[SZ14] K. Simonyan and A. Zisserman.
<a href="https://arxiv.org/abs/1409.1556">Very deep convolutional networks for large-scale image recognition</a>.
Sep. 2014.</p>
<div id="smith2017"></div>
<p>[Smi17] L. Smith.
<a href="https://arxiv.org/abs/1506.01186">Cyclical learning rates for training neural networks</a>.
<em>WACV</em>, Apr. 2017.</p>
<div id="snell2017"></div>
<p>[SSZ17] J. Snell, K. Swersky, and R. Zemel.
<a href="https://arxiv.org/abs/1703.05175">Prototypical networks for few-shot learning</a>.
<em>NeurIPS</em>, Dec. 2017.</p>
<div id="steinwart2019"></div>
<p>[Ste19] I. Steinwart.
<a href="https://arxiv.org/abs/1903.11482">A sober look at neural network initializations</a>.
Sep. 2019.</p>
<div id="stephens2019"></div>
<p>[Ste19b] N. Stephens.
<a href="https://community.arm.com/developer/ip-products/processors/b/ml-ip-blog/posts/bfloat16-processing-for-neural-networks-on-armv8_2d00_a">BFloat16 processing for neural networks on Armv8-A</a>.
Aug. 2019.</p>
<div id="stooke2019"></div>
<p>[SA19] A. Stooke and P. Abbeel.
<a href="https://arxiv.org/abs/1803.02811">Accelerated methods for deep reinforcement learning</a>.
Jan. 2019.</p>
<div id="straw2019"></div>
<p>[SPE19] A. Straw, A. Procter, and R. Earhart.
<a href="https://conferences.oreilly.com/artificial-intelligence/ai-ny-2019/public/schedule/detail/75130">nGraph: unlocking next-generation performance with deep learning compilers</a>.
2019.</p>
<div id="sukhbaatar2019"></div>
<p>[SGB+19] S. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin.
<a href="https://arxiv.org/abs/1905.07799">Adaptive attention span in transformers</a>.
May 2019.</p>
<div id="sun2019"></div>
<p>[SCC+19] X. Sun, J. Choi, C. Chen, et al.
<a href="https://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks">Hybrid 8-bit floating point (HFP8) training and inference for deep neural networks</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="sun2019-b"></div>
<p>[SWL+19] Y. Sun, S. Wang, Y. Li, S. Feng, H. Tian, H. Wu, and H. Wang.
<a href="https://arxiv.org/abs/1907.12412">ERNIE 2.0: a continual pre-training framework for language understanding</a>.
2019.</p>
<div id="sun2020"></div>
<p>[SAD+20] Y. Sun, N. Agostini, S. Dong, and D. Kaeli.
<a href="https://arxiv.org/abs/1911.11313">Summarizing CPU and GPU design trends with product data</a>.
2020.</p>
<div id="sutskever2014"></div>
<p>[SVL14] I. Sutskever, O. Vinyals, and Q. Le.
<a href="https://arxiv.org/abs/1409.3215">Sequence to sequence learning with neural networks</a>.
<em>NeurIPS</em>, Dec. 2014.</p>
<div id="sze2017"></div>
<p>[SCY+17] V. Sze, Y. Chen, T. Yang, and J. Emer.
<a href="https://ieeexplore.ieee.org/document/8114708">Efficient processing of deep neural networks: a tutorial and survey</a>.
<em>Proc. IEEE</em>, Dec. 2017.</p>
<div id="sze2020"></div>
<p>[SCY+20] V. Sze, Y. Chen, T. Yang, and J. Emer.
<a href="https://www.morganclaypool.com/doi/abs/10.2200/S01004ED1V01Y202004CAC050">Efficient processing of deep neural networks</a>.
<em>M\&amp;C</em>, June 2020.</p>
<div id="szegedy2014"></div>
<p>[SLJ+14] C. Szegedy, W. Liu, Y. Jia, et al.
<a href="https://arxiv.org/abs/1409.4842">Going deeper with convolutions</a>.
<em>CVPR</em>, Sep. 2014.</p>
<div id="szegedy2015"></div>
<p>[SVI+15] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.
<a href="https://arxiv.org/abs/1512.00567">Rethinking the Inception architecture for computer vision</a>.
<em>CVPR</em>, Dec. 2015.</p>
<div id="szegedy2014-b"></div>
<p>[SZS+14] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus.
<a href="https://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a>.
Feb. 2014.</p>
<div id="synced2017"></div>
<p>[Syn17] Synced.
<a href="https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129">A brief overview of attention mechanism</a>.
<em>Medium</em>, Sep. 2017.</p>
<div id="tan2019"></div>
<p>[TPL19] M. Tan, R. Pang, and Q. Le.
<a href="https://arxiv.org/abs/1911.09070">EfficientDet: scalable and efficient object detection</a>.
Nov. 2019.</p>
<div id="tan2019-b"></div>
<p>[TL19] M. Tan and Q. Le.
<a href="https://arxiv.org/abs/1905.11946">EfficientNet: rethinking model scaling for convolutional neural networks</a>.
May 2019.</p>
<div id="tassa2018"></div>
<p>[TYD+18] Y. Tassa, Y, Doron, A. Muldal, et al.
<a href="https://arxiv.org/abs/1801.00690">DeepMind control suite</a>.
Jan. 2018.</p>
<div id="tavarageri2016"></div>
<p>[TKT+16] S. Tavarageri, W. Kim, J. Torrellas, and P. Sadayappan.
<a href="https://ieeexplore.ieee.org/document/7839700">Compiler support for software cache coherence</a>.
<em>HiPC</em>, Dec. 2016.</p>
<div id="terry2019"></div>
<p>[Ter19] Terry.
<a href="https://devblogs.microsoft.com/cppblog/inlining-decisions-in-visual-studio/">Inlining decisions in visual studio</a>.
July 2019.</p>
<div id="thakur2005"></div>
<p>[TRG05] R. Thakur, R. Rabenseifner, and W. Gropp.
<a href="https://dl.acm.org/doi/10.1177/1094342005051521">Optimization of collective communication operations in MPICH</a>.
<em>HiPC</em>, Feb. 2005.</p>
<div id="thompson2020"></div>
<p>[TGL+20] N. Thompson, K. Greenewald, K. Lee, and G. Manso.
<a href="https://arxiv.org/abs/2007.05558">The computational limits of deep learning</a>.
July 2020.</p>
<div id="tramer2018"></div>
<p>[TKP+18] F. Tramer, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel.
<a href="https://arxiv.org/abs/1705.07204">Ensemble adversarial training: attacks and defenses</a>.
<em>ICLR</em>, July 2018.</p>
<div id="tsai2018"></div>
<p>[TAN+18] H. Tsai, S. Ambrogio, P. Narayanan, R. Shelby, and G. Burr.
<a href="https://iopscience.iop.org/article/10.1088/1361-6463/aac8a5">Recent progress in analog memory-based accelerators for deep learning</a>.
<em>J. Phys. D: Appl. Phys</em>, June 2018.</p>
<div id="tsang2018"></div>
<p>[Tsa18] S. Tsang.
<a href="https://towardsdatascience.com/yolov1-you-only-look-once-object-detection-e1f3ffec8a89">Review: YOLOv1 - you only look once (object detection)</a>.
<em>TDS</em>, Oct. 2018.</p>
<div id="tsipras2019"></div>
<p>[TSE+19] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry.
<a href="https://arxiv.org/abs/1805.12152">Robustness may be at odds with accuracy</a>.
<em>ICLR</em>, Sep. 2019.</p>
<div id="tvm2019"></div>
<p>[Tvm19] TVM.
<a href="https://tvm.apache.org/2019/03/18/tvm-apache-announcement">TVM deep learning compiler joins Apache Software Foundation</a>.
Mar. 2019.</p>
<div id="relay-ir2019"></div>
<p>[Tvm19] TVM.
<a href="https://docs.tvm.ai/dev/relay_intro.html">Introduction to Relay IR</a>.
2019.</p>
<div id="vandenoord2016"></div>
<p>[vKK+16] A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu.
<a href="https://arxiv.org/abs/1601.06759">Pixel recurrent neural networks</a>.
Jan. 2016.</p>
<div id="vandenoord2016-b"></div>
<p>[vDZ+16] A. van den Oord, S. Dieleman, H. Zen, et al.
<a href="https://arxiv.org/abs/1609.03499">WaveNet: a generative model for raw audio</a>.
Sep. 2016.</p>
<div id="vandenoord2017"></div>
<p>[vLB+17] A. van den Oord, Y. Li, I. Babuschkin, et al.
<a href="https://arxiv.org/abs/1711.10433">Parallel WaveNet: fast high-fidelity speech synthesis</a>.
Nov. 2017.</p>
<div id="valin2019"></div>
<p>[VS19] J. Valin and J. Skoglund.
<a href="https://arxiv.org/abs/1810.11846">LPCNet: improving neural speech synthesis through linear prediction</a>.
<em>ICASSP</em>, May 2019.</p>
<div id="vasilache2018"></div>
<p>[VZT+18] N. Vasilache, O. Zinenko, T. Theodoridis, et al.
<a href="https://arxiv.org/abs/1802.04730">Tensor Comprehensions: framework-agnostic high-performance machine learning abstractions</a>.
<em>ICASSP</em>, May 2019.</p>
<div id="vaswani2017"></div>
<p>[VSP+17] A. Vaswani, N. Shazeer, N. Parmar, et al.
<a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a>.
<em>NeurIPS</em>, Dec. 2017.</p>
<div id="venkatesan2019"></div>
<p>[VSZ+19] R. Venkatesan, Y. Shao, B. Zimmer, et al.
<a href="https://ieeexplore.ieee.org/document/8875657">A 0.11 PJ/OP, 0.32-128 TOPS, scalable multi-chip-module-based deep neural network accelerator designed with a high-productivity VLSI methodology</a>.
<em>HCS</em>, Aug. 2019.</p>
<div id="villmow2018"></div>
<p>[Vil18] M. Villmow.
<a href="https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=s8822-optimizing+nmt+with+tensorrt">Optimizing NMT with TensorRT</a>.
Mar. 2018.</p>
<div id="vinyals2014"></div>
<p>[VTB+14] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan.
<a href="https://arxiv.org/abs/1411.4555">Show and tell: a neural image caption generator</a>.
<em>CVPR</em>, Nov. 2014.</p>
<div id="vinyals2017"></div>
<p>[VBL+17] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra.
<a href="https://arxiv.org/abs/1606.04080">Matching networks for one shot learning</a>.
<em>NeurIPS</em>, Dec. 2017.</p>
<div id="vinyals2019"></div>
<p>[VBC+19] O. Vinyals, I. Babuschkin, J. Chung, et al.
<a href="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/">AlphaStar: mastering the real-time strategy game StarCraft II</a>.
Dec. 2019.</p>
<div id="vladimirov2019"></div>
<p>[VAK19] A. Vladimirov, R. Asai, and V. Karpusenko.
<a href="https://colfaxresearch.com/second-edition-of-parallel-programming-and-optimization-with-intel-xeon-phi-coprocessors/">Parallel programming and optimization with Intel Xeon Phi coprocessors</a>.
Jan. 2019.</p>
<div id="walsh2013"></div>
<p>[Wal13] C. Walsh.
<a href="https://www.nature.com/articles/502172a">Peter Huttenlocher (1931-2013)</a>.
<em>Nature</em>, Oct. 2013.</p>
<div id="wang2018"></div>
<p>[SMH+18] A.Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. Bowman.
<a href="https://arxiv.org/abs/1804.07461">GLUE: a multi-task benchmark and analysis platform for natural language understanding</a>.
Apr. 2018.</p>
<div id="wang2020"></div>
<p>[WYL+20] H. Wang, J. Yang, H. Lee, and S. Han.
<a href="https://arxiv.org/abs/1812.02734">Learning to design circuits</a>.
Jan. 2020.</p>
<div id="wang2018-b"></div>
<p>[WCB+18] N. Wang, J. Choi, D. Brand, C. Chen, and K. Gopalakrishnan.
<a href="https://arxiv.org/abs/1812.08011">Training deep neural networks with 8-bit floating point numbers</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="wang2019"></div>
<p>[WVP+19] G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and I. Stoica.
<a href="https://arxiv.org/pdf/1910.04940.pdf">Blink: fast and generic collectives for distributed ML</a>.
Oct. 2019.</p>
<div id="wang2017"></div>
<p>[WYZ+17] J. Wang, L. Yu, W. Zhang, Y. Gong, Y. Xu, B. Wang, P. Zhang, and D. Zhang.
<a href="https://arxiv.org/abs/1705.10513">IRGAN: a minimax game for unifying generative and discriminative information retrieval models</a>.
<em>SIGIR</em>, May 2017.</p>
<div id="wang2019-c"></div>
<p>[WML+19] Y. Wang, A. Mohamed, D. Le, et al.
<a href="https://arxiv.org/abs/1910.09799">Transformer-based acoustic modeling for hybrid speech recognition</a>.
Oct. 2019.</p>
<div id="wang2019-d"></div>
<p>[WYK+19] Y. Wang, Q. Yao, J. Kwok, and L. Ni.
<a href="https://arxiv.org/abs/1904.05046">Generalizing from a few examples: a survey on few-shot learning</a>.
<em>Comp. Surveys</em>, May 2019.</p>
<div id="wang2019-e"></div>
<p>[WWB19] Y. Wang, G. Wei, and D. Brooks.
<a href="https://arxiv.org/abs/1907.10701">Benchmarking TPU, GPU, and CPU platforms for deep learning</a>.
Oct. 2019.</p>
<div id="wang2017-c"></div>
<p>[WSS+17] Y. Wang, R. Skerry-Ryan, D. Stanton, et al.
<a href="https://arxiv.org/abs/1703.10135">Tacotron: towards end-to-end speech synthesis</a>.
Mar. 2017.</p>
<div id="wang2019-f"></div>
<p>[WWS+19] Y. Wang, Q. Wang, S. Shi, X. He, Z. Tang, K. Zhao, and X. Chu.
<a href="https://arxiv.org/abs/1909.06842">Benchmarking the performance and power of AI accelerators for AI training</a>.
Nov. 2019.</p>
<div id="wei2018"></div>
<p>[WSA18] R. Wei, L. Schwartz, and V. Adve.
<a href="https://openreview.net/pdf?id=ryG6xZ-RZ">DLVM: a modern compiler infrastructure for deep learning systems</a>.
<em>ICLR</em>, Apr. 2018.</p>
<div id="wen2016"></div>
<p>[WWW+16] W. Wen, C. Wu, Y. Wang, Y. Chen, and H. Li.
<a href="https://arxiv.org/abs/1608.03665">Learning structured sparsity in deep neural networks</a>.
<em>NeurIPS</em>, Dec. 2016.</p>
<div id="wen2017"></div>
<p>[WXY+17] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li.
<a href="https://arxiv.org/abs/1705.07878">TernGrad: ternary gradients to reduce communication in distributed deep learning</a>.
<em>NeurIPS</em>, Dec. 2017.</p>
<div id="weng2017"></div>
<p>[Wen17] L. Weng.
<a href="https://lilianweng.github.io/lil-log/2017/08/20/from-GAN-to-WGAN.html">From GAN to WGAN</a>.
Aug. 2017.</p>
<div id="wikimedia2011"></div>
<p>[Wik11] Wikimedia.
<a href="https://commons.wikimedia.org/wiki/File:Kernel_Machine.svg">Kernel Machine.svg</a>.
2011.</p>
<div id="wikimedia2012"></div>
<p>[Wik12] Wikimedia.
<a href="https://commons.wikimedia.org/wiki/File:Cart--pendulum.svg">Cart-pendulum.svg</a>.
2012.</p>
<div id="wikimedia2015"></div>
<p>[Wik15] Wikimedia.
<a href="https://commons.wikimedia.org/wiki/File:Typical_cnn.png">Typical cnn.png</a>.
2015.</p>
<div id="wikimedia2017"></div>
<p>[Wik17] Wikimedia.
<a href="https://commons.wikimedia.org/wiki/File:MnistExamples.png">MnistExamples.png</a>.
2017.</p>
<div id="wikimedia2018"></div>
<p>[Wik18] Wikimedia.
<a href="https://commons.wikimedia.org/wiki/File:Spectrogram--19thC.png">Spectrogram-19thC.png</a>.
2018.</p>
<div id="wiki-apple-a13-2019"></div>
<p>[Wik19] Wikipedia.
<a href="https://en.wikipedia.org/wiki/Apple_A13">Apple A13</a>.
2019.</p>
<div id="wiki-authorsguild2020"></div>
<p>[Wik20] Wikipedia.
<a href="https://en.wikipedia.org/wiki/Authors_Guild,_Inc._v._Google,_Inc.">Authors Guild, Inc. v. Google, Inc.</a>
Feb. 2020.</p>
<div id="wiki-rankbrain2020"></div>
<p>[Wik20b] Wikipedia.
<a href="https://en.wikipedia.org/wiki/RankBrain">RankBrain</a>.
Feb. 2020.</p>
<div id="williams2009"></div>
<p>[WWP09] S. Williams, A. Waterman, and D. Patterson.
<a href="https://dl.acm.org/doi/10.1145/1498765.1498785">Roofline: an insightful visual performance model for multicore architectures</a>.
<em>ACM</em>, Apr. 2009.</p>
<div id="wilson2018"></div>
<p>[WRS+18] A. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht.
<a href="https://arxiv.org/abs/1705.08292">The marginal value of adaptive gradient methods in machine learning</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="wilson2019"></div>
<p>[WZL+19] R. Wilson, C. Zhang, W. Lam, D. Desfontaines, D. Simmons-Marengo, and B. Gipson.
<a href="https://arxiv.org/abs/1909.01917">Differentially private SQL with bounded user contribution</a>.
Nov. 2019.</p>
<div id="winder2020"></div>
<p>[Win20] P. Winder.
\MYhref{https://rl-book.com}{Reinforcement Learning: industrial applications of intelligent agents}.
<em>O'Reilly</em>, Nov. 2020.</p>
<div id="wright2019"></div>
<p>[Wri19] L. Wright.
<a href="https://medium.com/@lessw/new-deep-learning-optimizer-ranger-synergistic-combination-of-radam-lookahead-for-the-best-of-2dc83f79a48d">New deep learning optimizer, Ranger synergistic combination of RAdam + LookAhead for the best of both</a>.
Aug. 2019.</p>
<div id="wu2016"></div>
<p>[WZX+16] J. Wu, C. Zhang, T. Xue, W. Freeman, and J. Tenenbaum.
<a href="https://arxiv.org/abs/1610.07584">Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling</a>.
<em>NeurIPS</em>, Dec. 2016.</p>
<div id="wu2016-b"></div>
<p>[WSC+16] Y. Wu, M. Schuster, Z. Chen, et al.
<a href="https://arxiv.org/abs/1609.08144">Google's neural machine translation system: bridging the gap between human and machine translation</a>.
Sep. 2016.</p>
<div id="wu2017"></div>
<p>[WAB+17] C. Wu, A. Ahmed, A. Beutel, A. Smola, and H. Jing.
<a href="https://dl.acm.org/doi/10.1145/3018661.3018689">Recurrent recommender networks</a>.
<em>WSDM</em>, Feb. 2017.</p>
<div id="wu2017-b"></div>
<p>[WWF+17] S. Wu, J. Wieland, O. Farivar, and J. Schiller.
<a href="https://research.fb.com/publications/automatic-alt-text-computer-generated-image-descriptions-for-blind-users-on-a-social-network-service/">Automatic alt-text: computer-generated image descriptions for blind users on a social network service</a>.
<em>CSCW</em>, Feb. 2017.</p>
<div id="wu2018"></div>
<p>[WH18] Y. Wu and K. He.
<a href="https://arxiv.org/abs/1803.08494">Group normalization</a>.
<em>ECCV</em>, Mar. 2018.</p>
<div id="wu2019"></div>
<p>[WZZ+19] B. Wu, X. Zhou, S. Zhao, X. Yue, and K. Keutzer.
<a href="https://ieeexplore.ieee.org/abstract/document/8793495">SqueezeSegV.2:  improved model structure and unsupervised domain adaptation for road-object segmentation from a LiDAR point cloud</a>.
<em>ICRA</em>, May 2019.</p>
<div id="wu2019-b"></div>
<p>[WFB+19] F. Wu, A. Fan, A. Baevski, Y. Dauphin, and M. Auli.
<a href="https://arxiv.org/abs/1901.10430">Pay less attention with lightweight and dynamic convolutions</a>.
Jan. 2019.</p>
<div id="wu2019-c"></div>
<p>[WKM+19] Y. Wu, A. Kirillov, F. Massa, W. Lo, and R. Girshick.
<a href="https://github.com/facebookresearch/detectron2">Detectron2</a>.
2019.</p>
<div id="wu2019-d"></div>
<p>[WKM+19] Y. Wu, A. Kirillov, F. Massa, W. Lo, and R. Girshick.
<a href="https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/">Detectron.2: a PyTorch-based modular object detection library</a>.
2019.</p>
<div id="wu2019-e"></div>
<p>[Wu19] H. Wu.
<a href="https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9659-inference-at-reduced-precision-on-gpus.pdf">Low precision inference on GPU</a>.
<em>GTC</em>, Mar. 2019.</p>
<div id="wu2019-f"></div>
<p>[WDZ+19] B. Wu, X. Dai, P. Zhang, et al.
<a href="https://arxiv.org/abs/1812.03443">FBNet: hardware-aware efficient ConvNet design via differentiable neural architecture search</a>.
<em>CVPR</em>, May 2019.</p>
<div id="wulf1995"></div>
<p>[WM95] W. Wulf and S. McKee.
<a href="https://dl.acm.org/doi/10.1145/216585.216588">Hitting the memory wall: implications of the obvious</a>.
<em>SIGARCH</em>, Mar. 1995.</p>
<div id="xi2019"></div>
<p>[XYB+19] S. Xi, Y. Yao, K. Bhardwaj, P. Whatmough, G. Wei, and D. Brooks.
<a href="https://arxiv.org/abs/1912.04481">SMAUG: end-to-end full-stack simulation infrastructure for deep learning workloads</a>.
Dec. 2019.</p>
<div id="xiao2020"></div>
<p>[XZZ20] C. Xiao, P. Zhong, and C. Zheng.
<a href="https://openreview.net/forum?id=Skgvy64tvr">Enhancing adversarial defense by k-winners-take-all</a>.
<em>ICLR</em>, Feb. 2020.</p>
<div id="xie2017"></div>
<p>[XGD+17] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He.
<a href="https://arxiv.org/abs/1611.05431">Aggregated residual transformations for deep neural networks</a>.
<em>CVPR</em>, July 2017.</p>
<div id="xilinx-acap2019"></div>
<p>[Xil19] Xilinx.
<a href="https://www.xilinx.com/support/documentation/white_papers/wp505-versal-acap.pdf">Versal: the first adaptive compute acceleration platform (ACAP)</a>.
2019.</p>
<div id="xing2018"></div>
<p>[XAT+18] C. Xing, D. Arpit, C. Tsirigotis, and Y. Bengio.
<a href="https://arxiv.org/abs/1802.08770">A walk with SGD</a>.
May 2018.</p>
<div id="xu2017"></div>
<p>[XEQ17] W. Xu, D. Evans, and Y. Qi.
<a href="https://arxiv.org/abs/1704.01155">Feature squeezing: detecting adversarial examples in deep neural networks</a>.
Dec. 2017.</p>
<div id="xu2018"></div>
<p>[XLF+18] X. Xu, C. Liu, Q. Feng, H. Yin, L. Song, and D. Song.
<a href="https://arxiv.org/abs/1708.06525">Neural network-based graph embedding for cross-platform binary code similarity detection</a>.
<em>CCS</em>, July 2018.</p>
<div id="yamazaki2019"></div>
<p>[YKT+18] M. Yamazaki, A. Kasagi, A. Tabuchi, et al.
<a href="https://arxiv.org/abs/1903.12650">Yet another accelerated SGD: ResNet-50 training on ImageNet in 74.7 seconds</a>.
Mar. 2019.</p>
<div id="yampolskiy2012"></div>
<p>[Yam12] R. Yampolskiy.
<a href="http://cecs.louisville.edu/ry/TuringTestasaDefiningFeature04270003.pdf">Turing test as a defining feature of AI-Completeness</a>.
<em>SCI</em>, 2012.</p>
<div id="yang2017"></div>
<p>[YCS17] T. Yang, Y. Chen, and V. Sze.
<a href="https://arxiv.org/abs/1611.05128">Designing energy-efficient convolutional neural networks using energy-aware pruning</a>.
<em>CVPR</em>, Apr. 2017.</p>
<div id="yang2019"></div>
<p>[YDY+19] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. Le.
<a href="https://arxiv.org/abs/1906.08237">XLNet: generalized autoregressive pretraining for language understanding</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="yang2015"></div>
<p>[YHG+15] Z. Yang, X. He, J. Gao, L. Deng, and A. Smola.
<a href="https://arxiv.org/abs/1511.02274">Stacked attention networks for image question answering</a>.
<em>CVPR</em>, Nov. 2015.</p>
<div id="yao2018"></div>
<p>[YGL+18] Z. Yao, A. Gholami, Q. Lei, K. Keutzer, and M. Mahoney.
<a href="https://arxiv.org/abs/1802.08241">Hessian-based Analysis of large batch training and robustness to adversaries</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="yao2020"></div>
<p>[YGS+20] Z. Yao, A. Gholami, S. Shen, K. Keutzer, and M. Mahoney.
<a href="https://arxiv.org/abs/2006.00719">AdaHessian: an adaptive second order optimizer for machine learning</a>.
Jun. 2020.</p>
<div id="yin2020"></div>
<p>[YSE+20] J. Yin, S. Sethumurugan, Y. Eckert, N. Enright Jerger, et al.
<a href="https://ieeexplore.ieee.org/document/9065601">Experiences with ML-driven design: a NoC case study</a>.
<em>HPCA</em>, Feb. 2020.</p>
<div id="ying2018"></div>
<p>[YKC+18] C. Ying, S. Kumar, D. Chen, T. Wang, and Y. Cheng.
<a href="https://arxiv.org/abs/1811.06992">Image classification at supercomputer scale</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="you2017"></div>
<p>[YGG17] Y. You, I. Gitman, and B. Ginsburg.
<a href="https://arxiv.org/abs/1708.03888">Large batch training of convolutional networks</a>.
Sep. 2017.</p>
<div id="you2020"></div>
<p>[YLR+20] Y. You, J. Li, S. Reddi, et al.
<a href="https://arxiv.org/abs/1904.00962">Large batch optimization for deep learning: training BERT in 76 minutes</a>.
<em>ICLR</em>, Jan. 2020.</p>
<div id="you2018"></div>
<p>[YZH+18] Y. You, Z. Zhang, C. Hsieh, J. Demmel, and K. Keutzer.
<a href="https://arxiv.org/abs/1709.05011">ImageNet training in minutes</a>.
Jan. 2018.</p>
<div id="yu2018"></div>
<p>[YAB+18] Y. Yu, M. Abadi, P. Barham, et al.
<a href="https://arxiv.org/abs/1805.01772">Dynamic control flow in large-scale machine learning</a>.
<em>EUROSYS</em>, May 2018.</p>
<div id="yuan2019"></div>
<p>[YTL+19] L. Yuan, F. Tay, G. Li, T. Wang, and J. Feng.
<a href="https://arxiv.org/abs/1909.11723">Revisit knowledge distillation: a teacher-free framework</a>.
Sep. 2019.</p>
<div id="zagoruyko2015"></div>
<p>[ZK15] S. Zagoruyko and N. Komodakis.
<a href="https://arxiv.org/abs/1504.03641">Learning to compare image patches via convolutional neural networks</a>.
<em>CVPR</em>, June 2015.</p>
<div id="zeghidour2018"></div>
<p>[ZXL+18] N. Zeghidour, Q. Xu, V. Liptchinsky, N. Usunier, G. Synnaeve, and R. Collobert.
<a href="https://arxiv.org/abs/1812.06864">Fully convolutional speech recognition</a>.
Dec. 2018.</p>
<div id="zeiler2012"></div>
<p>[Zei12] M. Zeiler.
<a href="https://arxiv.org/abs/1212.5701">ADADELTA: an adaptive learning rate method</a>.
Dec. 2012.</p>
<div id="zeiler2013"></div>
<p>[ZF13] M. Zeiler and R. Fergus.
<a href="https://arxiv.org/abs/1311.2901">Visualizing and understanding convolutional networks</a>.
<em>ECCV</em>, Nov. 2013.</p>
<div id="zeiler2013-b"></div>
<p>[ZF13] M. Zeiler and R. Fergus.
<a href="https://arxiv.org/abs/1301.3557">Stochastic pooling for regularization of deep convolutional neural networks</a>.
<em>ICLR</em>, May 2013.</p>
<div id="zela2020"></div>
<p>[ZES+20] A. Zela, T. Elsken, T. Saikia, Y. Marrakchi, T. Brox, and F. Hutter.
<a href="https://arxiv.org/abs/1909.09656">Understanding and robustifying differentiable architecture search</a>.
<em>ICLR</em>, Jan. 2020.</p>
<div id="zerrell2019"></div>
<p>[ZB19] T. Zerrell and J. Bruestle.
<a href="https://arxiv.org/abs/1903.06498">Stripe: tensor compilation via the nested polyhedral model</a>.
Mar. 2019.</p>
<div id="zhang2019-b"></div>
<p>[ZDH19] B. Zhang, A. Davoodi, and Y. Hu.
<a href="https://arxiv.org/abs/1908.03266">Efficient inference of CNNs via channel pruning</a>.
Aug. 2019.</p>
<div id="zhang2018"></div>
<p>[ZYY18] J. Zhang, J. Yang, and H. Yuen.
<a href="https://research.fb.com/publications/training-with-low-precision-embedding-tables/">Training with low-precision embedding tables</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="zhang2018-b"></div>
<p>[ZRW+18] M. Zhang, S. Rajbhandari, W. Wang, and Y. He.
<a href="https://www.usenix.org/conference/atc18/presentation/zhang-minjia">DeepCPU: serving RNN-based deep learning models 10x faster</a>.
<em>ATC</em>, 2018.</p>
<div id="zhang2019"></div>
<p>[ZLH+19] M. Zhang, J. Lucas, G. Hinton, and J. Ba.
<a href="https://arxiv.org/abs/1907.08610">Lookahead optimizer: k steps forward, 1 step back</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="zhang2019-c"></div>
<p>[ZL19] W. Zhang and P. Li.
<a href="https://arxiv.org/abs/1908.06378">Spike-train level backpropagation for training deep recurrent spiking neural networks</a>.
<em>NeurIPS</em>, Dec. 2019.</p>
<div id="zhang2017"></div>
<p>[ZZL+17] X. Zhang, X. Zhou, M. Lin, and J. Sun.
<a href="https://arxiv.org/abs/1707.01083">ShuffleNet: an extremely efficient convolutional neural network for mobile devices</a>.
<em>CVPR</em>, July 2017.</p>
<div id="zhang2017-b"></div>
<p>[ZXH+17] Y. Zhang, T. Xiang, T. Hospedales, and H. Lu.
<a href="https://arxiv.org/abs/1706.00384">Deep mutual learning</a>.
<em>CVPR</em>, Jan. 2018.</p>
<div id="zhao2019-c"></div>
<p>[ZZZ+19] C. Zhao, S. Zhao, M. Zhao, Z. Chen, C. Gao, H. Li, and Y. Tan.
<a href="https://www.sciencedirect.com/science/article/pii/S0020025518308338">Secure multi-party computation: theory, practice and applications</a>.
<em>Inf. Sciences</em>, Feb. 2019.</p>
<div id="zhao2019"></div>
<p>[ZZX+19] W. Zhao, J. Zhang, D. Xie, Y. Qian, R. Jia, and P. Li.
<a href="https://dl.acm.org/doi/10.1145/3357384.3358045">AIBox: CTR prediction model training on a single node</a>.
<em>CIKM</em>, Nov. 2019.</p>
<div id="zhao2019-b"></div>
<p>[ZHW+19] Z. Zhao, L. Hong, L. Wei, et al.
<a href="https://dl.acm.org/doi/10.1145/3298689.3346997">Recommending what video to watch next: a multitask ranking system</a>.
<em>RecSys</em>, Sep. 2019.</p>
<div id="zheng2018"></div>
<p>[ZZZ+18] G. Zheng, F. Zhang, Z. Zheng, Y. Xiang, N. Yuan, X. Xie, and Z. Li.
<a href="https://dl.acm.org/doi/10.1145/3178876.3185994">DRN: a deep reinforcement learning framework for news recommendation</a>.
<em>IW3C2</em>, Apr. 2018.</p>
<div id="zhou2018"></div>
<p>[ZMF+18] G. Zhou, N. Mou, Y. Fan, Q. Pi, W. Bian, C. Zhou, X. Zhu, and K. Gai.
<a href="https://arxiv.org/abs/1809.03672">Deep interest evolution network for click-through rate prediction</a>.
<em>AAAI</em>, Nov. 2018.</p>
<div id="zhuang2018"></div>
<p>[ZTZ+18] Z. Zhuang, M. Tan, B. Zhuang, J. Liu, Y. Guo, Q. Wu, J. Huang, and J. Zhu.
<a href="https://arxiv.org/abs/1810.11809">Discrimination aware channel pruning for deep neural networks</a>.
<em>NeurIPS</em>, Dec. 2018.</p>
<div id="zhu2019"></div>
<p>[ZZY+19] R. Zhu, K. Zhao, H. Yang, W. Lin, C. Zhou, B. Ai, Y. Li, and J. Zhou.
<a href="https://arxiv.org/abs/1902.08730">AliGraph: a comprehensive graph neural network platform</a>.
<em>PVLDB</em>, Aug. 2019.</p>
<div id="zisserman2018"></div>
<p>[Zis18] A. Zisserman.
<a href="https://project.inria.fr/paiss/files/2018/07/zisserman-self-supervised.pdf">Self-supervised learning</a>.
July 2018.</p>
<div id="zoph2017"></div>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../bio/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../bio/" class="btn btn-xs btn-link">
        Author's Biography
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../ch10/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../ch10/" class="btn btn-xs btn-link">
        Chapter 10: Opportunities and Challenges
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>