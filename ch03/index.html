<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Chapter 3: Models and Applications - Deep Learning Systems: Algorithms, Compilers, and Processors for Large-Scale Production</title>
    <link href="../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../js/jquery-3.2.1.min.js"></script>
    <script src="../js/bootstrap-3.3.7.min.js"></script>
    <script src="../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Chapter 3: Models and Applications", url: "#_top", children: [
              {title: "3.1 Recommender Systems Topologies", url: "#31-recommender-systems-topologies" },
              {title: "3.2 Computer Vision Topologies", url: "#32-computer-vision-topologies" },
              {title: "3.3 Natural Language Processing Topologies", url: "#33-natural-language-processing-topologies" },
              {title: "3.4 Reinforcement Learning Algorithms", url: "#34-reinforcement-learning-algorithms" },
          ]},
        ];

    </script>
    <script src="../js/base.js"></script>
      <script src="../javascripts/config.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../ch04/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../ch04/" class="btn btn-xs btn-link">
        Chapter 4: Training a Model
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../ch02/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../ch02/" class="btn btn-xs btn-link">
        Chapter 2: Building Blocks
      </a>
    </div>
    
  </div>

    

    <div id="ch3"></div>
<h1 id="chapter-3-models-and-applications">Chapter 3: Models and Applications</h1>
<p>The main types of workloads where DL models are used in production are recommender systems, computer vision, and NLP.</p>
<p>Recommender systems are typically the most prevalent class of models at hyperscalers, given the significant revenue they generate. Neural (DL-based) recommenders usually have two main components: an embedding layer and a NN, typically an MLP. The embedding layer maps high-dimensional sparse data to low-dimensional dense data. For instance, a movie recommender may have a dataset with <span class="arithmatex">\(100\)</span> million users, each rating a few of the 10,000 available movies. This dataset is sparse since most users only watch a tiny subset of the movies. An embedding layer maps this sparse representation to a dense representation and then passes it to the MLP. Standard recommender models are Wide &amp; Deep (W&amp;D), Neural collaborative filtering (NCF), Deep &amp; Cross Network (DCN), Deep Interest Evolution Network (DIEN), and Deep Learning Recommender Model (DLRM), covered in Section <a href="../ch03/#ch03.sec1">3.1</a>.</p>
<p>Computer vision models have multiple convolutional layers, often followed by a normalization function, such as batch normalization, and a nonlinear activation function, such as ReLU. Standard computer vision models are Inception, ResNet, MobileNet, UNet, Mask-RCNN, SSD, YOLO, DCGAN, and StyleGAN, covered in Section <a href="../ch03/#ch03.sec2">3.2</a>.</p>
<p>NLP includes natural language understanding (NLU), speech recognition, speech generation, and speech-to-speech translation. Many of these workloads use an embedding layer to map tokens or words (represented as one-hot vectors) into a low-dimensional dense representation. A one-hot vector word representation is a high-dimensional vector of all zeros except a one at the corresponding word index.</p>
<p>There are two main types of NLP models: RNN-based (including LSTM-based and GRU-based) and transformer-based models. RNN-based models typically have lower latency compared to transformer-based models, which require significantly more computations but typically have higher statistical performance. Standard NLP models are sequence-to-sequence, Transformer-LT, BERT, Deep Speech 2, and Tacotron, covered in Section <a href="../ch03/#ch03.sec3">3.3</a>.</p>
<p>Some workloads use multimodal models, where a unified output combines the last layers from multiple models. For instance, an autonomous driving vehicle may use a multimodal model that combines radar, LiDAR, and visual input. A person ID model may combine a voice ID model and a face ID model. Other workloads may use models sequentially. For instance, a recommender may use a language model as a preprocessing step in a search query, or an image descriptor may use a computer vision model followed by a language model.</p>
<p>We discuss RL topologies popular in academia and with some industry adoption in Section <a href="../ch03/#ch03.sec4">3.4</a>. Several applications may benefit from RL, including hardware and software design (see Section <a href="../ch10/#ch10.sec1">10.1</a>). RL topologies fall nonexclusively under three types of algorithms: Q-learning, policy optimization, and model-based. The most well-known is the model-based <a href="https://www.nature.com/articles/nature16961">AlphaGo</a>, which <a href="https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol">beat</a> the Go world champion Lee Sedol [<a href="../biblio/#silver2016">SHM+16</a>].</p>
<p>In all these workloads, determining the best model that meets the accuracy requirements within a computational and memory budget requires experimentation. Also, each topology can require significant engineering effort to design. Recently, there are newer techniques known as <em>AutoML</em>, which includes neural architecture search (NAS), that automatically develop topologies for a particular task and hardware target, reducing the manual engineering effort at the expense of much higher computation. Section <a href="../ch10/#ch10.sec1">10.1</a> discusses NAS techniques.</p>
<div id="ch03.sec1"></div>
<h2 id="31-recommender-systems-topologies">3.1 Recommender Systems Topologies</h2>
<p>Recommender systems, also known as recommendation engines or recommenders, are the most important type of DL algorithms at hyperscalers. They can provide significant monetization opportunities; their utility and market impact cannot be overstated. Despite their importance, only 2% of DL publications focus on recommenders, and the published results are often not reproducible [<a href="../biblio/#dacrema2019">DCJ19</a>].</p>
<p>Recommender systems are categorized into two approaches, and a hybrid of these two is common.</p>
<ol>
<li>
<p>Content-based systems recommend items to a user based on their profile and their user-item interaction.</p>
</li>
<li>
<p>Collaborative filtering recommends items to a user based on the user-item interactions from similar users.</p>
</li>
</ol>
<p>The input can be structured data, such as databases, or unstructured data, such as text and images. CNNs and RNNs can be applied to images and text, respectively, to extract features to input into a recommender. The recommended items can be ads to click, merchandise to purchase, videos to watch, songs to listen, social contacts to add, and news and social media posts to read. Recommender systems recommend items based on user features, item features, user-item ratings, and contexts, such as the time, day, and season. User-item ratings can be explicit or implicit based on user-item interaction. Implicit feedback includes the amount of time spent reading a news article, listening to a song, or viewing a clip. Details on the recent advances in context-aware recommender systems (CARS) are available <a href="https://www.sciencedirect.com/science/article/pii/S1574013718302120">elsewhere</a> [<a href="../biblio/#raza2019">RD19</a>].</p>
<p>The total number of user-item combinations can reach quintillions, and adding context further increases that number. Netflix has <a href="https://en.wikipedia.org/wiki/Netflix">around</a> <span class="arithmatex">\(200\)</span> million users and <a href="https://cordcutting.com/blog/how-many-titles-are-available-on-netflix-in-your-country/">over</a> 13,000 titles [<a href="../biblio/#lovely2019">Lov19</a>]. Google Play has <a href="https://arxiv.org/abs/1606.07792">over</a> <span class="arithmatex">\(1\)</span> billion users and over <span class="arithmatex">\(1\)</span> million apps [<a href="../biblio/#cheng2016">CKH+16</a>]. eBay has <a href="https://tech.ebayinc.com/engineering/odm/">more</a> than <span class="arithmatex">\(180\)</span> million buyers and <a href="https://tech.ebayinc.com/research/building-a-product-catalog-ebays-university-machine-learning-competition/">over</a> <span class="arithmatex">\(1.3\)</span> billion listings [<a href="../biblio/#dong2019">Don19</a>; <a href="../biblio/#padmanabhan2019">Pad19</a>]. Alibaba <a href="https://www-nextplatform-com.cdn.ampproject.org/c/s/www.nextplatform.com/2019/12/19/ai-recommendation-systems-get-a-gpu-makeover/amp/">has</a> <span class="arithmatex">\(2\)</span> billion products and serves as many as <span class="arithmatex">\(500\)</span> million customers per day [<a href="../biblio/#feldman2019">Fel19</a>]. This huge catalog results in memory size bottlenecks on the hardware platform. If every combination requires one byte, then the total user-item combinations would require <span class="arithmatex">\(1\)</span> exabyte of memory, which is <span class="arithmatex">\(4\times\)</span> more than the total storage of the largest supercomputer <a href="https://en.wikipedia.org/wiki/Summit_(supercomputer)">Summit</a>. eBay clusters items into categories and <a href="https://tech.ebayinc.com/engineering/complementary-item-recommendations-at-ebay-scale/">utilizes</a> user-category (rather than user-item) to reduce the number of combinations [<a href="../biblio/#brovman2019">Bro19</a>].</p>
<p>Rather than ranking potentially billions of items, a large-scale recommender system breaks the process into <a href="https://tech.ebayinc.com/engineering/complementary-item-recommendations-at-ebay-scale/">two stages</a> to meet the latency requirement and reduce the number of computations. First, a recall (candidate generator) stage selects several items that may be of interest to the user. Second, a ranking stage scores each item and selects those shown to the user [<a href="../biblio/#brovman2019">Bro19</a>]. The recall step selects a set of items (for instance, <span class="arithmatex">\(1000\)</span>) that may be of interest to a particular user, each one represented as a vector. The dot products between the vector representing the user and each of the <span class="arithmatex">\(1000\)</span> item-vectors are computed. The items producing the highest dot products are then recommended to the user.</p>
<p>Despite using a two-stage approach, a significant challenge of large-scale recommender systems is the large memory required, in particular for the embedding tables to embed users and item features. Baidu's <em>Phoenix Nest</em> online advertising recommender models can <a href="https://dl.acm.org/citation.cfm?id=3358045">exceed</a> <span class="arithmatex">\(10\)</span> TB, dwarfing the capacity of a GPU. Therefore, the model is partitioned into embeddings on the CPU and the NN on the GPU on Baidu's AIBox [<a href="../biblio/#zhao2019">ZZX+19</a>].</p>
<p>Content-based recommenders use features, known as metadata, for each item (such as movie genres and IMDb ratings) and recommend items based on the similarities to other items the user has liked. A profile for each user is learned based on their likes and used to evaluate the recalled items to make a recommendation. Other features can include embedding representations (using RNN, CNN, or hand-engineered features) of written reviews, movie summaries, and still images. Content-based recommenders do not use information or ratings from other users.</p>
<p>Collaborative filtering (CF) recommends items based on user-item interactions across multiple users. Collaborative filtering uses no metadata or domain knowledge about the items; instead, it learns all the feature vectors. This eliminates the dependency of manually chosen features at the expense of requiring more data. A <em>rating matrix</em> <span class="arithmatex">\(\mathbf{R}\)</span>, also known as the utility matrix or user-interaction matrix, contains the ratings across various users and items. Collaborative filtering learns a user matrix <span class="arithmatex">\(\mathbf{U}\)</span> and an item matrix <span class="arithmatex">\(\mathbf{V}\)</span> composed of user and item feature vectors of equal dimension, respectively, such that the squared differences between <span class="arithmatex">\(\mathbf{R}\)</span> and the dense matrix <span class="arithmatex">\(\hat{\mathbf{R}} = \mathbf{U}\mathbf{V}^{T}\)</span> is minimized. This is known as matrix factorization. <span class="arithmatex">\(\hat{\mathbf{R}}\)</span> provides a metric of similarity between the items and users. In practice, for large rating matrices, only a subset of entries is used. The alternative least squares (ALS) algorithm can perform matrix factorization by alternating between holding constant one of the matrices and adjusting the other one to minimize the error. Singular Value Decomposition (SVD) is another commonly used matrix factorization algorithm.</p>
<p>Neural recommenders typically use a hybrid approach. They are trained with large datasets across multiple user and item pairs. Standard neural recommenders are Wide and Deep (W&amp;D), Neural collaborative filtering (NCF), Deep Interest Evolution Network (DIEN), and Deep Learning Recommender Model (DLRM). GNNs are also gaining adoption for recommenders. Other recommenders include <a href="https://arxiv.org/abs/1802.05814">autoencoders</a> to encode implicit ratings or feedback, <a href="https://arxiv.org/abs/1705.10513">GANs</a>, and deep RL to tackle dynamic changes in items and users' preferences [<a href="../biblio/#liang2018">LKH+18</a>; <a href="../biblio/#wang2017">WYZ+17</a>; <a href="../biblio/#zheng2018">ZZZ+18</a>].</p>
<p><strong><a href="https://arxiv.org/abs/1606.07792">Wide &amp; Deep</a> (W&amp;D)</strong> combines the output from a linear model (referred to as wide) and a deep model, as shown in Figure <a href="../ch03/#fig:wide-deep">3.1</a> [<a href="../biblio/#cheng2016">CKH+16</a>]. This was originally developed to improve Google Play's app recommendation. The probability the recommended app is chosen given the input vector is: </p>
<div class="arithmatex">\[P(v_i=1 | \mathbf{x} ) = \sigma \left( \mathbf{w}^T_{\mathit{wide}} \phi(\mathbf{x}) + f_{\mathit{deep}}\left(\mathbf{W}_{\mathit{deep}}, \mathbf{x} \right) + b \right),\]</div>
<p>where <span class="arithmatex">\(\sigma\)</span> is the logit function, <span class="arithmatex">\(b\)</span> is the bias term, <span class="arithmatex">\(\phi(\mathbf{x})\)</span> are the features on the linear model, <span class="arithmatex">\(f_{\mathit{deep}}(\cdot)\)</span> is the deep model, <span class="arithmatex">\(\mathbf{w}_{\mathit{wide}}\)</span> is the weight vector for the linear model, and <span class="arithmatex">\(\mathbf{W}_{\mathit{deep}}\)</span> is the set of weights for the deep model. The input vector <span class="arithmatex">\(\mathbf{x}\)</span> has user features (for instance, country, language, demographics), contextual features (for instance, device, hour of the day, and day of the week), and item features (for instance, app age, and historical statistics of an app). Sparse discrete high-dimensional categorical feature vectors are embedded into a low-dimensional dense representation and concatenated as one input vector into an MLP.</p>
<div id="fig:wide-deep"></div>
<p><img alt="" src="../figures/ch03-01.png" />
<em>Figure 3.1:</em> A (left) linear and (right) deep learning model. Based on [<a href="../biblio/#cheng2016">CKH+16</a>].</p>
<p>Similar models to W&amp;D are the MLP model <a href="https://research.google/pubs/pub45530/">used</a> for YouTube recommendations, which <a href="https://dl.acm.org/citation.cfm?id=3346997">incorporates</a> the mixture-of-experts ML technique, and the <a href="https://arxiv.org/pdf/1703.04247.pdf">DeepFM</a> model, which shares the input with its "wide" and "deep" parts [<a href="../biblio/#covington2016">CAS16</a>; <a href="../biblio/#zhao2019-b">ZHW+19</a>; <a href="../biblio/#guo2017">GTY+17</a>]. Another similar model is <a href="https://arxiv.org/abs/1708.05123">Deep &amp; Cross Network</a> (DCN) used for ad click prediction. It applies feature crossing and, unlike W&amp;D, does not require manually selecting the features to cross.</p>
<p><strong><a href="https://arxiv.org/pdf/1708.05031.pdf">Neural Collaborative Filtering</a> (NCF)</strong> is a CF-based recommender that generalizes the popular matrix factorization algorithm [<a href="../biblio/#he2017-b">HLZ+17</a>]. A one-layer linear NN can represent matrix factorization. NCF augments this linear NN with multiple layers, as shown in Figure <a href="../ch03/#fig:ncf">3.2</a>, to model complex nonlinearities in the data, which improves the learned features and recommendations.</p>
<div id="fig:ncf"></div>
<p><img alt="" src="../figures/ch03-02.png" />
<em>Figure 3.2:</em> A neural collaborative filtering (NCF) model with one embedding layer for the user and one for the items, and an MLP model. Based on [<a href="../biblio/#he2017-b">HLZ+17</a>].</p>
<p><strong><a href="https://arxiv.org/abs/1809.03672">Deep Interest Evolution Network</a> (DIEN)</strong> and <strong><a href="https://arxiv.org/abs/1905.06874">Behavior Sequence Transformer</a> (BST)</strong> are used in production at Alibaba's Taobao to recommend advertisements [<a href="../biblio/#zhou2018">ZMF+18</a>; <a href="../biblio/#chen2019-c">CZL+19</a>]. They use a GRU- and a transformer-based topology, respectively, to model user behavior through time. A similar model, <a href="https://dl.acm.org/citation.cfm?id=3018689">Recurrent Recommender Network</a> (RRN), uses LSTM units [<a href="../biblio/#wu2017">WAB+17</a>].</p>
<p><strong><a href="https://arxiv.org/pdf/1906.00091.pdf">Deep Learning Recommendation Model</a> (DLRM)</strong> is a class of models used by Facebook. DLRM improves the handling of categorical features [<a href="../biblio/#naumov2019">NMS+19</a>]. The dot product of pairs of embedding vectors and processed dense features are post-processed through another MLP, as shown in Figure <a href="../ch03/#fig:dlrm">3.3</a>, to predict event probabilities. Because the embedding tables are enormous, model parallelism, discussed in Chapter <a href="../ch06/#ch06">6</a>, can be used to mitigate memory constraints. Facebook also <a href="https://arxiv.org/abs/1811.05922">proposed</a> using nonvolatile memory (NVM) as the primary storage medium and DRAM as a cache for commonly used embedding vectors [<a href="../biblio/#eisenman2018">ENG+18</a>].</p>
<div id="fig:dlrm"></div>
<p><img alt="" src="../figures/ch03-03.png" />
<em>Figure 3.3:</em> A Deep Learning Recommendation Model (DLRM) with a dense feature, multiple embedding layers for sparse features, and multiple MLP topologies. Based on [<a href="../biblio/#naumov2019">NMS+19</a>].</p>
<p><strong>Graph Neural Networks (GNNs)</strong>, introduced in Section <a href="../ch01/#ch01.sec5.sub5">1.5.5</a>, are gaining traction for large-scaled recommender systems. Industry platforms include Pinterest's <a href="https://arxiv.org/abs/1806.01973">PinSage</a>, Alibaba's <a href="https://arxiv.org/abs/1902.08730">AliGraph</a>, Microsoft's <a href="https://www.microsoft.com/en-us/research/publication/neugraph-parallel-deep-neural-network-computation-on-large-graphs/">NeuGraph</a>, and Amazon's <a href="https://arxiv.org/abs/1909.01315">Deep Graph Library</a> (DGL) [<a href="../biblio/#ying2018">YKC+18</a>; <a href="../biblio/#zhu2019">ZZY+19</a>; <a href="../biblio/#ma2019">MYM+19</a>; <a href="../biblio/#wang2019">WVP+19</a>; <a href="../biblio/#fey2019">FL19</a>; <a href="../biblio/#fey2020">Fey20</a>].</p>
<div id="ch03.sec2"></div>
<h2 id="32-computer-vision-topologies">3.2 Computer Vision Topologies</h2>
<p>Computer vision topologies are the most widely adopted types of topologies across enterprise businesses. Commercial applications include image/video tagging, facial identification, autonomous navigation, video summarization, medical image analysis, and automatic target recognition with electro-optical, synthetic aperture radar (SAR), and hyperspectral imagery.</p>
<p>Before adopting DL, computer vision practitioners spent significant efforts in engineering features to extract from the data samples. In particular, for image classification, practitioners developed (do not worry if you are unfamiliar with these algorithms) local binary pattern (<a href="https://dl.acm.org/citation.cfm?id=628808">LBP</a>), histogram of oriented gradients (<a href="https://ieeexplore.ieee.org/document/1467360">HOG</a>), and speeded up robust features (<a href="https://www.vision.ee.ethz.ch/~surf/eccv06.pdf">SURF</a>) [<a href="../biblio/#ojala2002">OPM02</a>; <a href="../biblio/#bay2006">BTV06</a>]. Similar to <a href="https://digital-library.theiet.org/content/journals/10.1049/ji-3-2.1946.0074">Gabor</a> filters, these features attempt to capture the gradient information around a pixel or set of pixels, essentially acting as edge detectors [<a href="../biblio/#gabor1946">Gab46</a>]. The features are then passed to a classifier, such as an SVM. Extracting features results in better performance than feeding the shallow classifier the raw pixels.</p>
<p>In 2012, Krizhevsky et al. developed the now-called <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a> topology and decisively won the ImageNet <a href="http://www.image-net.org/challenges/LSVRC/">challenge</a>, which grabbed the attention of the computer vision and other communities [<a href="../biblio/#russakovsky2015">RDS+15</a>; <a href="../biblio/#krizhevsky2012">KSH12</a>]. The computer vision community rapidly adopted DL techniques resulting in a lower image classification error every year (see Figure <a href="../ch03/#fig:imagenet-error">3.4</a> and note the jump from 2011-2012 when AlexNet was introduced). Commercial applications no longer use AlexNet given the newer improved models, but it is mentioned given its historical significance.</p>
<p>CNN models learn to detect features with increased hierarchical complexity along each consecutive layer. Interestingly, the weights in the first layer of computer vision models learned similar features to those developed over decades of research, which are also similar to the features used by the mammal primary visual cortex. That is, the weights in the first layer usually become edge detectors after training the model. The second figure in Zeiler and Fergus' 2013 <a href="https://arxiv.org/pdf/1311.2901.pdf">paper</a> (not replicated here) shows what some of the feature maps across various layers specialized to detect [<a href="../biblio/#zeiler2013">ZF13</a>]. One difference (of many) between CNN models and the mammal visual cortex is that CNNs <a href="https://arxiv.org/abs/1811.12231">rely</a> more on texture features than shape features [<a href="../biblio/#geirhos2018">GRM+18</a>]. Augmenting the training dataset by perturbing each image's texture increases the dependency on shape features and improves the model's performance.</p>
<p>Computer vision tasks detailed in this section are classification, object detection, semantic segmentation, verification, and image generations. Additional tasks not discussed include action recognition, image denoising, super-resolution, and style transfer.</p>
<div id="ch03.sec2.sub1"></div>
<h3 id="321-image-classification">3.2.1 Image Classification</h3>
<p>The task of image classification is to predict the class label of an image. A common preprocessing technique is to resize all the training and testing images to be the same size and square. Two common metrics to measure accuracy are Top-<span class="arithmatex">\(1\)</span> and Top-<span class="arithmatex">\(5\)</span>. Top-<span class="arithmatex">\(1\)</span> accuracy requires that the model's top guess (corresponding to the highest logit) is the correct one. Top-<span class="arithmatex">\(5\)</span> requires that one of the model's top <span class="arithmatex">\(5\)</span> guesses is the correct one. Top-<span class="arithmatex">\(5\)</span> helps to account for uncertainty in the labels. Figure <a href="../ch03/#fig:imagenet-error">3.4</a> illustrates the Top-<span class="arithmatex">\(5\)</span> classification error from 2010 to 2017 on the ImageNet 1,000 classes (<span class="arithmatex">\(i1k\)</span>) dataset. Since 2012 all the top results have used DL.</p>
<div id="fig:imagenet-error"></div>
<p><img alt="" src="../figures/ch03-04.png" />
<em>Figure 3.4:</em> Top-<span class="arithmatex">\(5\)</span> classification error from 2010-2017 on the ImageNet-1K dataset. Since 2012 all the top results have used DL. Based on [<a href="../biblio/#zisserman2018">Zis18</a>].</p>
<p>Key neural image classifiers include AlexNet, VGG, Inception, ResNet, DenseNet, Xception, MobileNet, ResNeXt, and NAS. These families of topologies introduce new layers discussed below, such as inception, residual, group convolution, and depthwise separable convolutional layers, and introduce new techniques, such as factorization.</p>
<p><strong>AlexNet</strong>, shown in Figure <a href="../ch03/#fig:alexnet">3.5</a>, is similar to the 1998 <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"><strong>LeNet-5</strong></a> topology used for digit recognition but with more layers and units. Also, AlexNet uses ReLU rather than the logit activation functions, and max pooling rather than average pooling [<a href="../biblio/#lecun1998">LBB+98</a>].</p>
<div id="fig:alexnet"></div>
<p><img alt="" src="../figures/ch03-05.png" />
<em>Figure 3.5:</em> All the layers of the AlexNet topology. Based on [<a href="../biblio/#hassan2018">Has18</a>].</p>
<p><strong><a href="https://arxiv.org/abs/1409.1556">VGG</a></strong> is a family of topologies similar to AlexNet but with more layers and only uses <span class="arithmatex">\(3\times 3\)</span> convolution filters [<a href="../biblio/#simonyan2014">SZ14</a>]. VGG factorizes a <span class="arithmatex">\(5\times 5\)</span> into two consecutive <span class="arithmatex">\(3\times 3\)</span> layers to reduce the number of parameters, as shown in Figure <a href="../ch03/#fig:factorization3x3">3.6</a>. Factorization maintains the same receptive field coverage. The reduced number of parameters mitigates overfitting, which facilitates using topologies with more layers.</p>
<div id="fig:factorization3x3"></div>
<p><img alt="" src="../figures/ch03-06.png" />
<em>Figure 3.6:</em> The factorization of a <span class="arithmatex">\(5\times 5\)</span> filter into two consecutive <span class="arithmatex">\(3\times 3\)</span> filters maintains the same receptive field.</p>
<p><strong><a href="https://arxiv.org/abs/1409.4842">Inception-v1</a></strong>, also known as <strong>GoogleNet</strong>, introduced the inception module, which is composed of multiple filters of different sizes that process the same input, as shown in Figure <a href="../ch03/#fig:inception">3.7</a> [<a href="../biblio/#szegedy2015">SVI+15</a>; <a href="../biblio/#szegedy2014">SLJ+14</a>]. These filters extract multilevel features, and their outputs are concatenated. The inception module popularized the usage of <span class="arithmatex">\(1\times 1\)</span> filters, which modifies the number of channels. Inception replaces the fully-connected layers at the end of the topology with a global average pooling across the 2D feature map, which notably reduces the total number of parameters.</p>
<div id="fig:inception"></div>
<p><img alt="" src="../figures/ch03-07.png" />
<em>Figure 3.7:</em> The Inception-v1 module. Different filter sizes are applied to the input tensor and the outputs are concatenated.</p>
<p><strong>Inception-v3</strong> introduces the factorization of an <span class="arithmatex">\(n\times n\)</span> convolutional filter into a <span class="arithmatex">\(1\times n\)</span> followed by an <span class="arithmatex">\(n\times 1\)</span> filter, as shown in Figure <a href="../ch03/#fig:factorization">3.8</a>. This factorization maintains the same receptive field and reduces the number of weights from <span class="arithmatex">\(n^2\)</span> to <span class="arithmatex">\(2n\)</span>. Inception-v3 also adds a regularization known as <em>label smoothing</em> to the one-hot label vectors by replacing the zeros with a small epsilon value. Inception-v3, like <a href="https://arxiv.org/abs/1502.03167">Inception-v2</a> (also known as Batch-Normalization Inception), uses batch normalization [<a href="../biblio/#ioffe2015">IS15</a>].</p>
<div id="fig:factorization"></div>
<p><img alt="" src="../figures/ch03-08.png" />
<em>Figure 3.8:</em> The factorization of a <span class="arithmatex">\(5\times 5\)</span> filter into a <span class="arithmatex">\(5\times 1\)</span> and <span class="arithmatex">\(1\times 5\)</span> filters maintains the same receptive field.</p>
<p>Another technique introduced in VGG and improved in Inception-v3 is doubling the number of channels and halving the feature maps' length and width in consecutive layers. This pattern is made in one of three ways. First, convolution followed by pooling at the expense of a convolution with a larger tensor input. Second, pooling followed by convolution at the expense of a less-expressive layer. Third (recommended), two parallel blocks: (1) a convolution block with a stride of <span class="arithmatex">\(2\)</span> that maintains the same number of channels; and (2) a pooling layer, as shown in Figure <a href="../ch03/#fig:grid-reduction">3.9</a>.</p>
<div id="fig:grid-reduction"></div>
<p><img alt="" src="../figures/ch03-09.png" />
<em>Figure 3.9:</em> Efficient grid size reduction. The number of channels doubles and the height and width are halved.</p>
<p><strong><a href="https://arxiv.org/abs/1512.03385">ResNet</a></strong> is a family of models that popularized layers with skip connections, also known as residual layers. Skip connections bypass other layers, as shown in Figure <a href="../ch03/#fig:residual">3.10</a> [<a href="../biblio/#he2015">HZR+15</a>]. The motivation is that rather than learning the direct mapping from <span class="arithmatex">\(x\)</span> to <span class="arithmatex">\(\mathcal{H}(x)\)</span> it is easier to learn <span class="arithmatex">\(\mathcal{F}(x)\)</span>, which is the difference or the <em>residual</em> between <span class="arithmatex">\(x\)</span> and <span class="arithmatex">\(\mathcal{H}(x)\)</span>. Then <span class="arithmatex">\(\mathcal{H}(x)\)</span> can be computed by adding this residual. Using residual layers, together with batch normalization, allows the training of overly deep models with over <span class="arithmatex">\(1000\)</span> layers. The gradient can backpropagate via shortcut connections; thus, mitigating the vanishing gradient problem introduced in Section <a href="../ch02/#ch02.sec1">2.1</a>. Deep ResNets use a bottleneck unit to reduce the number of computations, as shown on the right of Figure <a href="../ch03/#fig:residual">3.10</a>.</p>
<div id="fig:residual"></div>
<p><img alt="" src="../figures/ch03-10.png" />
<em>Figure 3.10:</em> Residual layers have skip connections that bypass certain layers. (left) A residual layer with two convolutional layers. (right) A residual module reduces the tensor to <span class="arithmatex">\(64\)</span> channels (from <span class="arithmatex">\(256\)</span> channels) to reduce the number of <span class="arithmatex">\(3\times 3\)</span> convolutions and then expands the output back to <span class="arithmatex">\(256\)</span> channels.</p>
<p><strong><a href="https://arxiv.org/abs/1608.06993">DenseNet</a></strong> connects each layer to every other layer [<a href="../biblio/#huang2016">HLv+16</a>]. Each layer's inputs are the concatenation of all feature maps from all the previous layers, which have a large memory footprint. On the flip side, DenseNet requires fewer weights than other similarly performing models.</p>
<p><strong>Extreme Inception (<a href="https://arxiv.org/abs/1610.02357">Xception</a>)</strong> combines design principles from VGG, Inception, and ResNet, and introduces <em>depthwise separable convolutions</em>, shown in Figure <a href="../ch03/#fig:conv-separable">3.11</a>. In depthwise separable convolutions, the cross-channel correlations and spatial correlations are mapped separately [<a href="../biblio/#chollet2016">Cho16</a>]. That is, every input channel is convolved with a different filter and the results are aggregated using a <span class="arithmatex">\(1\times 1\)</span> filter called a <em>pointwise convolution</em>.</p>
<div id="fig:conv-separable"></div>
<p><img alt="" src="../figures/ch03-11.png" />
<em>Figure 3.11:</em> Depthwise separable convolution is a depthwise convolution, where every input channel is convolved with a different filter, followed by a pointwise convolution.</p>
<p><strong><a href="https://arxiv.org/abs/1704.04861">MobileNet</a></strong>, <strong><a href="https://arxiv.org/abs/1801.04381">MobileNet-v2</a></strong>, and <strong><a href="https://arxiv.org/abs/1905.02244">MobileNet-v3</a></strong> target hardware with limited power, compute, and memory, such as mobile phones. These models use depthwise separable convolution blocks with no pooling layers in between. MobileNet-v2 uses residual connections and adds a channel expansion convolution layer prior to the depthwise separable convolutions, as shown in Figure <a href="../ch03/#fig:mobilenet">3.12</a> for stride 1 [<a href="../biblio/#howard2017">HZC+17</a>]. MobileNet-v3 uses AutoML, a technique discussed in Section <a href="../ch10/#ch10.sec1">10.1</a>. These models are served not just in mobile phones but also in data centers.</p>
<div id="fig:mobilenet"></div>
<p><img alt="" src="../figures/ch03-12.png" />
<em>Figure 3.12:</em> The MobileNet modules with arbitrary input tensor sizes using stride 1 for (left) v1 and (right) v2.</p>
<p><strong><a href="https://arxiv.org/abs/1611.05431">ResNeXt</a></strong> reintroduced group convolutions (initially used by AlexNet to distribute the model into two GPUs) [<a href="../biblio/#xie2017">XGD+17</a>]. In group convolution, the filters separate into groups, and each group operates on specific channels of the input tensor. The group convolution tensors are typically represented as a 5D tensor with the group id as the additional dimension. Depthwise separable convolution is a particular case of group convolution where the number of groups equals the number of channels of the input tensor.</p>
<p>ResNeXt replaces residual convolution blocks with residual group convolutions, shown in Figure <a href="../ch03/#fig:resnext">3.13</a>, and every path of the group contains the same topology. These convolutions facilitate training and serving across multiple devices since each convolution in the group can be done independently of the other ones. ResNeXt is or has been <a href="https://arxiv.org/pdf/1811.09886.pdf">used</a> at Facebook [<a href="../biblio/#park2018">PNB+18</a>].</p>
<div id="fig:resnext"></div>
<p><img alt="" src="../figures/ch03-13.png" />
<em>Figure 3.13:</em> ResNeXt module with equivalent representations. ResNeXt uses residual group convolutions which are easier to parallelize across compute units. Based on [<a href="../biblio/#xie2017">XGD+17</a>].</p>
<p><strong>NAS</strong> is a family of algorithms that learn both the topology and the weights targeting a particular hardware target, such as NASNet and <a href="https://arxiv.org/abs/1905.11946">EfficientNet</a> [<a href="../biblio/#tan2019-b">TL19</a>]. EfficientNet was initially used on TPUs, but can be used with other hardware. Given their long training times and the diverse hardware fleet in data centers (multiple generations of CPUs and GPUs), the adoption of NAS-based models in the industry is still limited.</p>
<div id="ch03.sec2.sub2"></div>
<h3 id="322-object-detection">3.2.2 Object Detection</h3>
<p>Object detection involves finding or localizing objects of interest in an image and assigning a class label to them. Traditionally, object detection required a two-step approach: a region proposal step and a classification step. The input image is scaled up and down, known as an <em>image pyramid</em>, to detect objects of various sizes. New NNs can do these steps simultaneously, starting with the widely adopted Single-Shot Detector (SSD) and You Only Look Once (YOLO) models. Despite being relatively old, these models are still used in production because of their plug-and-play nature, where the object detector can use the latest CNN classifier as the base network.</p>
<p>Object detection models use a unified weighted cost function that accounts for the localization and the classification tasks. Also, object detectors generate several bounding boxes for a given object, and remove most of them using <a href="https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c">non-maximum suppression</a> (NMS).</p>
<p>The most common metric to measure the detection accuracy is the mean Average Precision (<a href="https://towardsdatascience.com/breaking-down-mean-average-precision-map-ae462f623a52">mAP</a>). The average precision (AP) is the area under the precision-recall curve and ranges from <span class="arithmatex">\(0\)</span> to <span class="arithmatex">\(1\)</span>, with <span class="arithmatex">\(1\)</span> being perfect detection for one class. The mAP is the mean AP across all the classes.</p>
<p>Key neural object detectors include Faster-RCNN, YOLO, SSD, RetinaNet, and EfficientDet.</p>
<p><strong><a href="https://arxiv.org/abs/1506.01497">Faster-RCNN</a></strong> uses a two-step approach with a region proposal network (RPN) and a classification network [<a href="../biblio/#ren2015">RHG+15</a>]. In Faster-RCNN, these two networks share a base CNN network or backbone architecture, which reduces the number of redundant computations. The base CNN model extracts feature maps from the image, which are passed to the RPN to generate and refine candidate bounding boxes, as shown in Figure <a href="../ch03/#fig:faster-rcnn">3.14</a>. All the bounding boxes are then reshaped to be the same size and passed to the classifier. The <a href="https://arxiv.org/abs/1612.03144">Feature Pyramid Network</a> (FPN) improved this topology; the predictions happen on high- and low-resolution feature maps [<a href="../biblio/#lin2016">LGH+16</a>].</p>
<div id="fig:faster-rcnn"></div>
<p><img alt="" src="../figures/ch03-14.png" />
<em>Figure 3.14:</em> (left) The Faster-RCNN topology generates regions of interest in a feature map and jointly processes them. (right) The Feature Pyramid Network (FPN) topology can be used with the Faster-RCNN topology or other models to better detect objects at different scales.</p>
<p><strong><a href="https://arxiv.org/pdf/1506.02640.pdf">YOLO</a></strong> divides the image into a <span class="arithmatex">\(7\times 7\)</span> grid [<a href="../biblio/#redmon2016">RDG+16</a>]. Each grid cell is responsible for <span class="arithmatex">\(2\)</span> bounding boxes. Each bounding box is composed of the <span class="arithmatex">\((x,y)\)</span> center coordinate of an object, and the width, height, and confidence. That is, each bounding box has <span class="arithmatex">\(5\)</span> values. The output of each grid cell is <span class="arithmatex">\(5\)</span> values times <span class="arithmatex">\(2\)</span> bounding boxes plus the probability of each class label given the input. If there are <span class="arithmatex">\(20\)</span> classes, then each grid cell has an output vector of <span class="arithmatex">\(5\times 2 + 20 = 30\)</span>, and given the <span class="arithmatex">\(7\times 7\)</span> cells, then the total number of output values for an input image is <span class="arithmatex">\(7\times 7\times 30=1470\)</span>, as shown in Figure <a href="../ch03/#fig:yolo">3.15</a>. In practice, the number of grid cells and bounding boxes are hyperparameters. The input image maps to the output via a CNN pretrained on an image classification task, such as the ImageNet dataset. YOLOv2 and <a href="https://arxiv.org/abs/1804.02767">YOLOv3</a> improves by detecting at three scales, using a deeper CNN topology, and having a class score for each bounding box [<a href="../biblio/#redmon2018">RF18</a>].</p>
<div id="fig:yolo"></div>
<p><img alt="" src="../figures/ch03-15.png" />
<em>Figure 3.15:</em> A YOLO model can map an input image to a <span class="arithmatex">\(7\times 7\times 30\)</span> grid output. Based on [<a href="../biblio/#tsang2018">Tsa18</a>].</p>
<p><strong>Single-shot detector <a href="https://arxiv.org/abs/1512.02325">(SSD)</a></strong> uses an image classification model, such as VGG or MobileNet, as the base network and appends additional layers to the model [<a href="../biblio/#liu2015">LAE+15</a>]. Bounding boxes start from predefined anchor boxes. In each of the appended layers, the model refines or predict the bounding box coordinates, each with a respective score. Most of the computations are in the base network.</p>
<p><strong><a href="https://arxiv.org/abs/1708.02002">RetinaNet</a></strong> is the first one-stage detector model that outperforms the two-stage detection approach. The primary reason for previous one-stage detectors trailing in accuracy is the extreme class imbalance (many more background class samples). RetinaNet uses the <em>focal loss</em> function to mitigate this class imbalance [<a href="../biblio/#lin2017">LGG+17</a>]. The focal loss reduces the loss to well-classified examples.</p>
<p><strong><a href="https://arxiv.org/abs/1911.09070">EfficientDet</a></strong> is a scalable family of detectors based on EfficientNet. It uses a pyramid network for multiscale detection [<a href="../biblio/#tan2019">TPL19</a>].</p>
<div id="ch03.sec2.sub3"></div>
<h3 id="323-segmentation">3.2.3 Segmentation</h3>
<p>Segmentation is a generalized and more challenging form of object detection, where every pixel in an image has a corresponding class label. Widely adopted models in the industry include Mask-RCNN and DeepLabv3 and in biomedical applications: U-Net, 3D-UNet, and V-Net.</p>
<p><strong><a href="https://arxiv.org/abs/1703.06870">Mask R-CNN</a></strong> extends Faster-RCNN by adding a separate output branch to predict the masks for all the classes [<a href="../biblio/#he2017">HGD+17</a>]. This branch is in parallel to the bounding box predictor branch. Similar to Faster-RCNN, the choice for the base network is flexible.</p>
<p><strong><a href="https://arxiv.org/abs/1706.05587">DeepLabv3</a></strong> uses <em>atrous convolution</em>, also known as dilated convolution, hole algorithm, or up-conv to increase the size of the feature maps by upsampling the weight filter, that is, inserting one or more zeros between each weight in the filters [<a href="../biblio/#chen2017">CPS+17</a>]. Atrous convolution, combined with Spatial Pyramid Pooling (SPP), is known as Atrous SPP (ASPP) and shown in Figure <a href="../ch03/#fig:aspp">3.16</a>. ASPP can account for different object scales in the image.</p>
<div id="fig:aspp"></div>
<p><img alt="" src="../figures/ch03-16.png" />
<em>Figure 3.16:</em> Atrous or dilated convolutions can maintain or increase the size of the feature maps. Based on [<a href="../biblio/#chen2017">CPS+17</a>].</p>
<p><strong><a href="https://arxiv.org/abs/1505.04597">U-Net</a></strong> is an encoder-decoder CNN [<a href="../biblio/#ronneberger2015">RFB15</a>]. It uses convolutions to reduce the size of the receptive field, followed by transposed convolutions (or upsampling) to increase the size. U-Net also has skip connections between mirrored layers in the encoder and decoder stacks. This type of model is known as a fully-convolutional network (<a href="https://arxiv.org/abs/1411.4038">FCN</a>) [<a href="../biblio/#long2014">LSD14</a>]. U-Net can be trained with few images using data augmentation with multiple shifts and rotations.</p>
<p><strong><a href="https://arxiv.org/abs/1606.06650">3D U-Net</a></strong> and <strong><a href="https://arxiv.org/abs/1606.04797">V-Net</a></strong> are 3D convolutional networks designed for voxel (3D pixels) segmentation from volumetric data [<a href="../biblio/#cicek2016">CAL+16</a>; <a href="../biblio/#milletari2016">MNA16</a>]. These models generally required the immense memory only available on server CPUs for training due to the large activations. Model parallelism techniques (discussed in Section <a href="../ch05/#ch05.sec2">5.2</a>) can be applied to train on GPUs and accelerators.</p>
<p><strong><a href="https://github.com/facebookresearch/detectron2">Detectron</a></strong> is a popular open-source platform developed by Facebook [<a href="../biblio/#wu2019-c">WKM+19</a>]. Detectron is implemented in PyTorch and <a href="https://ai.facebook.com/blog/-detectron2-a-pytorch-based-modular-object-detection-library-/">contains</a> implementations of various object detection and segmentation algorithms, which facilitates community adoption [<a href="../biblio/#wu2019-d">WKM+19</a>].</p>
<div id="ch03.sec2.sub4"></div>
<h3 id="324-verification">3.2.4 Verification</h3>
<p>The task of verification is to determine whether a sample belongs to a particular set. The set size may be one, for instance, in the set of people with access to a personal bank account, or many, for instance, in the set of people with access to a building. Siamese networks are designed for this task.</p>
<p><strong><a href="https://dl.acm.org/citation.cfm?id=2987282">Siamese networks</a></strong> learn a similarity function between two input images [<a href="../biblio/#bromley1993">BGL+93</a>]. They can be trained by comparing an anchor image to a positive and negative image or, more precisely, comparing a metric of the distance between the feature vectors extracted from the images. The objective is to simultaneously minimize the distance between the anchor and positive image features and maximize the distance between the anchor and negative image features.</p>
<p>While Siamese networks are decades old, they can use modern techniques to improve their performance. For instance, CNN models can be a component of a Siamese network. The CNN models are <a href="https://arxiv.org/abs/1504.03641">trained</a> across a variety of image appearances and used to extract features from the images [<a href="../biblio/#zagoruyko2015">ZK15</a>].</p>
<div id="ch03.sec2.sub5"></div>
<h3 id="325-image-generation">3.2.5 Image Generation</h3>
<p>The task of image generation requires modeling the distribution of images for a relative domain, such as written digits (the ultimate aspiration is modeling the distribution of all the natural images). Image generation is primarily an unsupervised learning task used in academia with some industry adoption for image deblurring, compression, and completion. The main types of algorithms used for image generation include auto-regressive and GAN models, specifically, PixelRNN, PixelCNN, DCGAN, 3D GAN, StackedGAN, StarGAN, SyleGAN, and Pix2pix.</p>
<p><strong><a href="https://arxiv.org/abs/1601.06759">PixelRNN</a></strong> and <strong><a href="https://arxiv.org/abs/1606.05328">PixelCNN</a></strong> are auto-regressive models that predict the pixels along both axes using recurrent and convolutional layers, respectively. These models generate a conditional distribution over the <span class="arithmatex">\(256\)</span> possible values for each RGB image channel at each pixel location [<a href="../biblio/#vandenoord2016">vKK+16</a>].</p>
<p><strong><a href="https://arxiv.org/abs/1511.06434">DCGAN</a></strong> and <strong><a href="https://arxiv.org/abs/1610.07584">3D GAN</a></strong> combine CNNs and GANs to generate 3D objects, as shown in Figure <a href="../ch03/#fig:3dgan">3.17</a> [<a href="../biblio/#radford2015">RMC15</a>; <a href="../biblio/#wu2016">WZX+16</a>]. These GANs learn to generate high-quality objects by sampling from a low-dimensional space and passing those samples to the generator. <strong>Stacked GANs</strong> trains across multiple stacks of GANs, which results in higher quality image generation [<a href="../biblio/#huang2017">HLP+17</a>].</p>
<div id="fig:3dgan"></div>
<p><img alt="" src="../figures/ch03-17.png" />
<em>Figure 3.17:</em> A 3D-GAN generator takes a random vector <span class="arithmatex">\(\mathbf{z}\)</span> and generates a 3D image. Based on [<a href="../biblio/#wu2016">WZX+16</a>].</p>
<p><strong><a href="https://arxiv.org/abs/1711.09020">StarGAN</a></strong> and <strong><a href="https://arxiv.org/abs/1812.04948">StyleGAN</a></strong> generate photorealistic images. For instance, they can generate human faces adjusting latent factors, such as freckles, hair color, gender, eyeglasses, and facial shape, when trained on a face dataset [<a href="../biblio/#choi2017">CCK+17</a>; <a href="../biblio/#karras2019">KLA19</a>].</p>
<p><strong><a href="https://arxiv.org/abs/1611.07004">Pix2pix</a></strong> is an adversarial network that learns a mapping from an input image to an output image and also learns a cost function to train this mapping. It can generate realistic images from labeled maps, colorize gray images, fill gaps in images, remove image backgrounds, and generate images from sketches [<a href="../biblio/#isola2016">IZZ+16</a>].</p>
<p><strong>Other computer vision topologies</strong> that have been influential in the field, but do not currently have extensive commercial adoption are <a href="https://arxiv.org/abs/1503.03832">FaceNet</a> for face recognition and verification, <a href="https://arxiv.org/abs/1602.07360">SqueezeNet</a> and <a href="https://arxiv.org/abs/1707.01083">ShuffleNet</a> for image classification on edge devices, <a href="https://arxiv.org/abs/1709.01507">SENet</a> for high accuracy image classification, <a href="https://arxiv.org/abs/1609.04802">SRGAN</a> for image super-resolution, and <a href="https://ieeexplore.ieee.org/abstract/document/8793495">SqueezeSegV2</a> for road-object segmentation from LiDAR point cloud [<a href="../biblio/#schroff2015">SKP15</a>; <a href="../biblio/#cao2018">HSW+18</a>; <a href="../biblio/#iandola2016">IHM+16</a>; <a href="../biblio/#zhang2017">ZZL+17</a>; <a href="../biblio/#hu2019-b">HSA+19</a>; <a href="../biblio/#ledig2016">LTH+16</a>; <a href="../biblio/#wu2019">WZZ+19</a>]. <a href="https://arxiv.org/abs/1812.08008">OpenPose</a> is used for pose estimation and has some adoption; Wrnch.AI <a href="https://builders.intel.com/ai/blog/track-athletes-3d-wrnch-intel-dl-boost">uses</a> a modified proprietary model to <a href="https://www.youtube.com/watch?v=S7Ntp2hpqJ8">detect kinematics</a> from 2D video.</p>
<div id="ch03.sec3"></div>
<h2 id="33-natural-language-processing-topologies">3.3 Natural Language Processing Topologies</h2>
<p>NLP has been considered an <a href="http://cecs.louisville.edu/ry/TuringTestasaDefiningFeature04270003.pdf">AI-complete</a> problem (requiring human-level intelligence to solve) given the complexity required to understand language. NLP is a required step toward automatic reasoning, that is, using stored knowledge to generate additional knowledge [<a href="../biblio/#yampolskiy2012">Yam12</a>]. Academia and industry have made tremendous progress in recent years.</p>
<p>Traditional NLP systems often use a hidden Markov model (HMM) (do not worry if you are not familiar with HMM). An HMM requires language experts to encode grammatical and semantic rules, provide a detailed <a href="https://wordnet.princeton.edu/">ontology</a>, parse the data, tag words with the appropriate part-of-speech, and iteratively align inputs and outputs. Neural NLP models can learn a particular task using a lexicon or language vocabulary and a massive data corpus and without explicitly programming the language rules. A popular benchmark to assess the performance of NLP models is the General Language Understanding Evaluation (<a href="https://arxiv.org/abs/1804.07461">GLUE</a>) benchmark [<a href="../biblio/#wang2018">SMH+18</a>].</p>
<p>Hyperscalers use NLP algorithms for NLU, speech recognition, speech generation, and speech-to-speech translation tasks. NLU tasks include language translation, sentiment analysis, automatic document summarization, image captioning, document clustering, and question &amp; answering. Speech recognition and speech synthesis are used as part of an NLP system by AI assistants, such as Apple Siri, Amazon Alexa, Google Assistant, Microsoft Cortana, and Baidu DuerOS. Speech-to-speech translation is used to interpret speech between different languages either as three separate stages (speech-to-text, text translation, and text-to-speech) or as a combined model. NLP algorithms facilitate human-machine interactions, enhancing a machine's ability to understand human language, and improve human-human communication, enabling communication between people without a common language.</p>
<div id="ch03.sec3.sub1"></div>
<h3 id="331-natural-language-understanding">3.3.1 Natural Language Understanding</h3>
<p>NLU deals with machine reading comprehension. Neural machine translation (NMT) is the NLU task of mapping written text from a source to a target language using a NN. NMT topologies and techniques are also used for other NLU tasks with minor to no adaptations, such as sentiment analysis to categorize the sentiment of written product reviews, for question &amp; answering systems, for document summarization, and for image captioning to "translate" a vector representation of the image into a description or caption.</p>
<p>Neural NLU models can be RNN-based, CNN-based, and transformer-based. They consist of an encoder that takes the source sentence and a decoder that outputs the target sentence. The decoder's task is to predict the next output word (or subword) given the previous outputs and the inputs. During the decode stage, a target output word can be greedily chosen from the softmax output or using a <em>beam search</em> approach as follows: The top <span class="arithmatex">\(n\)</span> candidate words from the <span class="arithmatex">\(M\)</span> softmax outputs at time <span class="arithmatex">\(t\)</span> are selected and used as inputs into the next timestep. This results in <span class="arithmatex">\(nM\)</span> output candidates at <span class="arithmatex">\(t+1\)</span>. The top <span class="arithmatex">\(n\)</span> are selected from this <span class="arithmatex">\(\mathit{nM}\)</span> group, and the process iteratively repeats in each subsequent timestep. At the last timestep, one output is selected from the <span class="arithmatex">\(\mathit{nM}\)</span> choices. A common choice is <span class="arithmatex">\(n=10\)</span> to provide a compromise between speed and accuracy. Figure <a href="../ch03/#fig:beam">3.18</a> depicts a beam search of <span class="arithmatex">\(n=2\)</span>.</p>
<div id="fig:beam"></div>
<p><img alt="" src="../figures/ch03-18.png" />
<em>Figure 3.18:</em> Beam search using a beam size of <span class="arithmatex">\(2\)</span>. Except for the initial decoder input at decoding timestep <span class="arithmatex">\(0\)</span>, every timestep uses the <span class="arithmatex">\(2\)</span> most probable outputs (underlined) from the previous timesteps as inputs. At time <span class="arithmatex">\(t=4\)</span>, beam search results in the sentences: "I am playing the piano with ..." and "I am playing the piano &lt;eos> ...," where &lt;eos> is the end of sentence token.</p>
<p>The quality of the target sentence in machine translation is typically reported using the <a href="https://www.aclweb.org/anthology/J18-3002/">BLEU</a> score, a measure of similarity between the machine's translation and a professional human translation normalized by the sequence length. Other quality metrics have been <a href="https://arxiv.org/abs/1707.06875">proposed</a> [<a href="../biblio/#novikova2017">NDC+17</a>].</p>
<p>One implementation challenge during training is the variable sequence length. Using a constant sequence length to batch the sequences and either pad short sequences or truncate long sequences to the predetermined length so that each sample in a batch is of the same length mitigates this challenge.</p>
<p>The inputs to the NN are known as tokens. While earlier NLU topologies used words as tokens, most newer topologies use learned <a href="https://arxiv.org/abs/1508.07909">subwords</a> [<a href="../biblio/#sennrich2015">SHB15</a>]. An algorithm segments words constrained to a fixed vocabulary size (the maximum number of subwords). These subwords are often interpretable, and the model can generalize to new words not seen during training using these subwords. Subwords are crucial for low-resource languages, that is, languages where the data corpus is small. The downside of using subwords rather than words is that a sequence has more tokens, which requires more computations.</p>
<p>Multi-language NMT involves learning a model used across multiple language pairs. They are particularly helpful for low-resource languages. Some care is <a href="https://arxiv.org/abs/1907.05019">needed</a> to use them over simpler pairwise language models without sacrificing the performance of the translations from the high-resource language pairs [<a href="../biblio/#arivazhagan2019">ABF+19</a>]. Jointly learning the subwords across the combined languages has been <a href="https://arxiv.org/abs/1508.07909">shown</a> to be beneficial [<a href="../biblio/#sennrich2015">SHB15</a>]. Google <a href="https://arxiv.org/abs/1907.05019">uses</a> a multi-language NMT transformer-based model to support translation across <span class="arithmatex">\(100\)</span> languages [<a href="../biblio/#arivazhagan2019">ABF+19</a>].</p>
<h4 id="rnn-based">RNN-Based</h4>
<p>During serving, RNN-based models are challenging to parallelize due to their sequential nature. A server CPU with fewer but more powerful cores than a GPU <a href="https://www.usenix.org/conference/atc18/presentation/zhang-minjia">works well</a> for RNN-based inference [<a href="../biblio/#zhang2018-b">ZRW+18</a>]. These models are typically memory bandwidth bound, leaving much computational capacity unused. Some <a href="https://on-demand-gtc.gputechconf.com/gtcnew/sessionview.php?sessionName=s8822-optimizing+nmt+with+tensorrt">work</a> demonstrates that their implementation can be modified to be more compute bound [<a href="../biblio/#villmow2018">Vil18</a>]. <a href="https://papers.nips.cc/paper/9451-shallow-rnn-accurate-time-series-classification-on-resource-constrained-devices.pdf">ShaRNN</a> provides an example of an RNN model with a small memory footprint, which is useful for edge deployments [<a href="../biblio/#dennis2019">DAM+19</a>].</p>
<p>Despite the adoption of transformer-based models in commercial applications, RNN-based models <a href="https://arxiv.org/abs/1911.11423">continue to be used</a> commercially due to their adequate statistical performance and low latency, and due to the larger memory and computational requirements of transformer-based models [<a href="../biblio/#merity2019">Mer19</a>].</p>
<p><strong>Sequence-to-sequence (<a href="https://arxiv.org/abs/1409.3215">S2S</a>)</strong> was the first widely adopted NMT model, and provides the foundation for similar models still used in production [<a href="../biblio/#sutskever2014">SVL14</a>]. The encoder LSTM units take as input (1) the state of the previous LSTM cell, (2) the output of the previous LSTM cell, and (3) the current token, as shown in Figure <a href="../ch03/#fig:encoder-decoder">3.19</a>. The <em>thought vector</em> is the concatenated state vector and output vector of the last LSTM encoder unit. This thought vector is an encoding of the source sentence. The decoder takes the thought vector as input to the first decoder LSTM unit and produces a target word. Each subsequent unit takes the output from the previous unit as its input. This cycle continues until an LSTM outputs an end-of-sentence token. In practice, generating the target sentence in reverse order typically results in better quality translations.</p>
<div id="fig:encoder-decoder"></div>
<p><img alt="" src="../figures/ch03-19.png" />
<em>Figure 3.19:</em> Encoder and decoder LSTM units for a question-and-answer system. The input sentence is represented by the <em>thought vector</em>.</p>
<p>Variants of the original S2S topology include models with multiple stacked bidirectional LSTM layers and <a href="https://arxiv.org/abs/1611.01603">bidirectional attention</a> [<a href="../biblio/#seo2016">SKF+16</a>]. The term <em>NMT</em> is sometimes incorrectly used as a synonym <a href="https://github.com/tensorflow/nmt">for S2S</a> or <a href="https://cloud.google.com/blog/products/ai-machine-learning/mlperf-benchmark-establishes-that-google-cloud-offers-the-most-accessible-scale-for-machine-learning-training">for GNMT</a>.</p>
<p><strong>Google's Neural Machine Translation (<a href="https://arxiv.org/abs/1609.08144">GNMT</a>)</strong> is the most popular RNN-based model [<a href="../biblio/#wu2016-b">WSC+16</a>]. GNMT learns a better thought vector by simultaneously training across multiple languages and incorporates an <a href="https://arxiv.org/abs/1508.04025">attention</a> module to cope with long sentences [<a href="../biblio/#luong2015">LPM15</a>]. The main idea of GNMT is that the thought vector should be the same regardless of the source and target language since it captures a meaning, which should be independent of the language.</p>
<h4 id="cnn-based">CNN-Based</h4>
<p>Using CNNs may have a computational advantage over RNNs, given they are more easily parallelizable and have a higher operational intensity (discussed in Section <a href="../ch07/#ch07.sec3">7.3</a>). Another advantage is they extract features hierarchically and may better capture complex relationships in the data.</p>
<p>Bai et al. <a href="https://arxiv.org/abs/1803.01271">demonstrated</a> that CNN-based models outperform RNN-based models on various NLP long-sequence tasks [<a href="../biblio/#bai2018">BKK18</a>]. Similarly, Facebook <a href="https://arxiv.org/abs/1705.03122">demonstrated</a> that CNN-based models had a computational advantage over GNMT at a similar statistical performance (both on CPUs and GPUs) [<a href="../biblio/#gehring2017">GAG+17</a>]. When trained on models of the same size, the CNN-based models outperform GNMT.</p>
<p>CNN models have also been <a href="https://arxiv.org/abs/1411.4555">used</a> as a preprocessing step to image captioning by extracting relevant features [<a href="../biblio/#vinyals2014">VTB+14</a>]. In particular, the second-to-last activation output in a CNN model is often used as the feature vector. This vector is an encoding of the image and passed to an NLU decoder to generate a caption. Attention can <a href="https://arxiv.org/abs/1511.02274">improve</a> the captions by focusing the decoder on certain parts of the input image [<a href="../biblio/#yang2015">YHG+15</a>].</p>
<h4 id="transformer-based">Transformer-Based</h4>
<p>Transformer-based models use attention modules without any RNN units. The first transformer-based model, Transformer-LT, was introduced by Google in the 2017 <a href="https://arxiv.org/abs/1706.03762">paper</a> <em>Attention is All You Need</em> and has been <a href="https://arxiv.org/abs/1909.06317">shown</a> to statistically outperform RNN-based methods on various NLP tasks [<a href="../biblio/#vaswani2017">VSP+17</a>; <a href="../biblio/#karita2019">KCH+19</a>]. These models are more easily parallelizable than RNNs, can learn longer-term dependencies, and have higher arithmetic intensity.</p>
<p>A transformer primarily consists of a set of encoder and decoder blocks with the same structure but different weight values and with skip connections, as shown in Figure <a href="../ch03/#fig:transformer">3.20</a>. Each encoder block consists of two main layers: a self-attention and a feedforward layer, where the self-attention block helps account for context in the input sentence. Each decoder block consists of three main layers: a self-attention, an encoder-decoder attention, and a feedforward layer. In the decoder, the encoder-decoder attention allows the decoder to focus on the crucial parts of the encoder representation. Words (or subwords, in practice) get embedded into vectors. A stack of encoders processes these vectors, and a stack of decoders processes their output. The architecture has skip-connections added and normalized after each layer. The target output word is chosen from the softmax output using a beam search approach.</p>
<div id="fig:transformer"></div>
<p><img alt="" src="../figures/ch03-20.png" />
<em>Figure 3.20:</em>  (a) A transformer is composed of several encoder and decoder blocks; (b) each block has an attention layer (the decoder has two) and a feedforward layer; and (c) the entire transformer model with <span class="arithmatex">\(N\times\)</span> blocks is depicted. Based on [<a href="../biblio/#alammar2018">Ala18</a>; <a href="../biblio/#vaswani2017">VSP+17</a>].</p>
<p><strong>Bidirectional Encoder Representations from Transformers (<a href="https://arxiv.org/abs/1810.04805">BERT</a>)</strong> is a bidirectional transformer model developed by Google, and widely adopted across hyperscalers [<a href="../biblio/#devlin2018">DCL+18</a>]. BERT achieved state-of-the-art results on multiple NLP tasks using a massive corpus of unannotated text crawled from the web, rather than a corpus labeled for a specific task. The standard embedding models before BERT, such as word2vec or GloVe (discussed in Section <a href="../ch02/#ch02.sec7">2.7</a>), learned context-free word embeddings, whereas BERT uses context to learn better embeddings. BERT is <a href="https://www.blog.google/products/search/search-language-understanding-bert/">used</a> by Google Search to better understand long search queries to improve the quality of the results [<a href="../biblio/#nayak2019">Nay19</a>].</p>
<p>BERT is trained using two self-supervised learning tasks. In one task, the model predicts a randomly masked-out word based on the context of the words before and after it. In the second task, the model predicts whether the second sentence follows the first sentence in the original paragraph.</p>
<p>BERT and other transformer-based models are shown in Figure <a href="../ch03/#fig:transformers-time">3.21</a> and the most prominent are highlighted in Table <a href="../ch03/#tab:transformers">3.1</a>. Typically, newer models better capture the <a href="https://arxiv.org/abs/1906.08237">dependencies</a> between tokens [<a href="../biblio/#yang2019">YDY+19</a>].</p>
<div id="tab:transformers"></div>
<p><em>Table 3.1:</em> Prominent transformer-based models.
<img alt="" src="../figures/ta03-01.png" /></p>
<div id="fig:transformers-time"></div>
<p><img alt="" src="../figures/ch03-21.png" />
<em>Figure 3.21:</em> Multiple transformer-based models and their respective number of parameters across time. Based on [<a href="../biblio/#sanh2019-b">San19</a>].</p>
<p>Large transformer-based models require considerable power and compute to train and deploy. While hyperscalers widely use them, they are less common at companies without WSCs due to the training costs. Also, larger transformer-based models may not meet the stringent low latency inference requirements in some applications.</p>
<p>The Hugging Face <a href="https://github.com/huggingface/transformers">Transformers</a>, Facebook <a href="https://github.com/pytorch/fairseq">Fairseq</a>, and AWS <a href="https://arxiv.org/abs/2008.04885">Sockeye 2</a> libraries contain several transformer-based models to facilitate wider adoption [<a href="../biblio/#domhan2020">DDV+20</a>]. Future models are likely to compromise between prodigious costly models and smaller efficient models, trained and adopted by medium-size companies and universities, with smaller serving latencies. These include smaller BERT-like models, such as <a href="https://arxiv.org/abs/1909.11942">ALBERT</a> by Google, <a href="https://arxiv.org/abs/1910.01108">DistilBERT</a> by Hugging Face, and <a href="https://arxiv.org/abs/1909.05840">Q-BERT</a> by UC Berkeley. Other solutions are <a href="https://arxiv.org/abs/1901.10430">replacing</a> computationally expensive layers with light convolutions, <a href="https://arxiv.org/abs/1905.07799">adapting</a> the number of attention layers, or <a href="https://arxiv.org/abs/1905.10650">removing</a> most attention layers during inference to reduce serving latency [<a href="../biblio/#lan2019">LCG+19</a>; <a href="../biblio/#sanh2019">SDC+19</a>; <a href="../biblio/#shen2019">SDY+19</a>; <a href="../biblio/#wu2019-b">WFB+19</a>; <a href="../biblio/#sukhbaatar2019">SGB+19</a>; <a href="../biblio/#michel2019">MLN19</a>].</p>
<div id="ch03.sec3.sub2"></div>
<h3 id="332-speech-recognition">3.3.2 Speech Recognition</h3>
<p>Automatic speech recognition (ASR) is the task of converting acoustic sound waves into written text. ASR differs from voice recognition, where the task is to identify a person based on their voice. One of the main ASR challenges is the combinatory space of various aspects of speech, such as pace, accent, pitch, volume, and background noise. Also, serving an ASR model requires decoding acoustic signals in real-time with reliable accuracy. Neural ASR approaches have successfully overcome these challenges with large datasets without pronunciation models, HMMs, or other components of traditional ASR systems. Nassif et al. <a href="https://ieeexplore.ieee.org/document/8632885">provide</a> a systematic review of various neural ASR systems [<a href="../biblio/#nassif2019">NSA+19</a>].</p>
<p>ASR systems and other speech-related systems often transform the acoustic sound waves into a spectrogram or Mel-spectrogram representation. A spectrogram is a 2D frequency-time representation of the acoustic signal that uses frequencies across short-time intervals, as shown in Figure <a href="../ch03/#fig:spectrogram">3.22</a>. In the figure, the color represents the amplitude of a particular frequency at a specific time interval. The Mel-spectrogram is a spectrogram where the frequencies are scaled using the mel-scale to better match the frequency resolution of the human auditory system.</p>
<div id="fig:spectrogram"></div>
<p><img alt="" src="../figures/ch03-22.png" />
<em>Figure 3.22:</em> A spectrogram is a 2D frequency-time representation of the acoustic signal where the frequencies for short time intervals are captured. Source: [<a href="../biblio/#wikimedia2018">Wik18</a>] (CC BY-SA 4.0).</p>
<p><strong>Deep Speech 2 (<a href="https://arxiv.org/abs/1512.02595">DS2</a>)</strong> was developed by Baidu and is the first major neural ASR. It provides a baseline for other models. DS2 uses a spectrogram as the input to a series of CNN and RNN layers [<a href="../biblio/#amodei2015">AAB+15</a>]. The CNN layers treat the spectrogram input as an image.</p>
<p><strong>Listen, Attend, and Spell (<a href="https://ieeexplore.ieee.org/document/7472621">LAS</a>)</strong> was developed by Google. This model uses <a href="https://arxiv.org/abs/1904.08779">SpecAugment</a> for data augmentation. SpecAugment uses image augmentation techniques on the spectrogram [<a href="../biblio/#chan2016">CJL+16</a>; <a href="../biblio/#park2019">PCZ+19</a>]. The LAS system has an encoder and decoder. The encoder is a pyramid RNN. The decoder is an attention-based RNN that emits each character conditioned on all previous characters and the entire acoustic sequence.</p>
<p><strong>RNN-Transducer (RNN-T)</strong> processes the input samples and streams alphabetical character outputs. It does not use attention. For mobile devices, Google <a href="https://arxiv.org/abs/1811.06621">developed</a> a quantized RNN-T model that <a href="https://ai.googleblog.com/2019/03/an-all-neural-on-device-speech.html">runs</a> in real-time on a Google Pixel device and is deployed with the Gboard app with <span class="arithmatex">\(80\)</span> MB memory footprint [<a href="../biblio/#he2019">HSP+19</a>; <a href="../biblio/#schalkwyk2019">Sch19</a>].</p>
<p><strong><a href="https://arxiv.org/abs/1812.07625">Wav2letter++</a></strong> is an open-source neural ASR framework developed by Facebook; it uses the fully convolutional model <a href="https://arxiv.org/abs/1812.06864">ConvLM</a> [<a href="../biblio/#pratap2018">PHX+18</a>; <a href="../biblio/#zeghidour2018">ZXL+18</a>]. Facebook also <a href="https://arxiv.org/abs/1910.09799">demonstrated</a> the use of transformers for ASR [<a href="../biblio/#wang2019-c">WML+19</a>].</p>
<div id="ch03.sec3.sub3"></div>
<h3 id="333-text-to-speech">3.3.3 Text-to-Speech</h3>
<p>Text-to-speech (TTS) is the task of synthesizing speech from text. The most well-known TTS system is probably the one used by the late Prof. Stephen Hawking. A TTS system is typically composed of three stages: (1) a text-analysis model, (2) an acoustic model, and (3) an audio synthesis module known as a vocoder. Traditionally, audio synthesis modules combined short-speech fragments collected from a user to form complete utterances. Using these fragments makes it difficult to modify the tone or voice characteristics and results in a robotic-like synthesis.</p>
<p>Neural TTS systems are now able to generate human-like speech as measured by the MOS (Mean Opinion Score), a human evaluation of the quality of voice. A neural TTS model can learn to generate voices with different characteristics. They can also be adapted to generate music and speech from an image. Facebook <a href="https://research.fb.com/publications/automatic-alt-text-computer-generated-image-descriptions-for-blind-users-on-a-social-network-service/">uses</a> automatic captioning to help visually impaired users browse their News Feed and hear a machine-generated caption of each image [<a href="../biblio/#wu2017-b">WWF+17</a>]. Google <a href="https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html">Duplex</a> uses neural TTS models on Pixel phones, for example, to contact restaurants to make reservations [<a href="../biblio/#leviathan2018">LM18</a>].</p>
<p>The primary neural speech synthesis systems deployed in production are WaveNet, Parallel WaveNet, and WaveRNN and require a text-to-linguistic features preprocessing step. Tacotron 2 provides a full end-to-end text-to-speech generator. Deep Voice 3 and ClariNet are speech synthesizers (not end-to-end TTS) developed by Baidu that have been influential and may be used in production. GAN-based TTS <a href="https://openreview.net/forum?id=r1gfQgSFDr">is starting</a> to gain traction in academia despite the <a href="https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/">earlier unknowns</a> of how to use GANs with discrete values [<a href="../biblio/#binkowski2020">BDD+20</a>].</p>
<p><strong><a href="https://arxiv.org/abs/1609.03499">WaveNet</a></strong> by Google is a vocoder autoregressive model based on the PixelCNN model [<a href="../biblio/#vandenoord2016-b">vDZ+16</a>]. It predicts a distribution for each audio sample conditioned on all previous audio samples and the input linguistic features. These features are derived from the input text and contain phoneme, syllable, and word information. To deal with long-range temporal dependencies needed for raw audio generation, WaveNet uses a stack of dilated causal convolutions to allow their receptive fields to grow exponentially with depth.</p>
<p>WaveNet suffers from high serving latency due to the sequential generation of audio samples. WaveNet uses an <span class="arithmatex">\(8\)</span>-bit integer value timestep (rather than a 16-bit, as is typical in audio) to reduce the latency and make the softmax output more tractable.</p>
<p><strong><a href="https://arxiv.org/abs/1711.10433">Parallel WaveNet</a></strong> by Google uses <a href="https://arxiv.org/abs/1503.02531">knowledge distillation</a> to train a feedforward network with WaveNet [<a href="../biblio/#vandenoord2017">vLB+17</a>; <a href="../biblio/#hinton2015">HVD15</a>]. Knowledge distillation (detailed in Section <a href="../ch06/#ch06.sec4">6.4</a>) uses a teacher model to train a smaller, more efficient student model. The FFNN is easily parallelizable and generates speech samples in real-time with minimal accuracy loss compared to WaveNet. Google Assistant uses Parallel WaveNet.</p>
<p><strong><a href="https://arxiv.org/abs/1712.05884">Tacotron 2</a></strong> by Google is a generative end-to-end model trained with audio and text pairs that synthesizes speech directly from characters and combines the methodologies of the popular WaveNet and <a href="https://arxiv.org/abs/1703.10135">Tacotron</a> to generate human-like speech [<a href="../biblio/#shen2017">SPW+17</a>; <a href="../biblio/#wang2017-c">WSS+17</a>]. Specifically, Tacotron 2 uses CNN and LSTM layers to encode character embeddings into Mel-spectrograms, capturing audio with various intonations. This Mel-spectrogram is then converted to waveforms using a WaveNet model as a vocoder. This system can be <a href="https://arxiv.org/abs/1806.04558">adapted</a> to generate speech audio in the voice of different speakers [<a href="../biblio/#jia2018-b">JZW+18</a>]. A speaker encoder network can generate a vector representation for a given speaker using seconds of reference speech from a target speaker. The Tacotron 2 network is adapted to generate speech conditioned on this vector representation.</p>
<p><strong><a href="https://arxiv.org/abs/1802.08435">WaveRNN</a></strong> by Google uses a dual softmax layer to predict 16-bit audio samples efficiently; each softmax layer predicts 8 bits. For real-time inference in mobile CPUs, the small model weights are pruned (removed or forced to zero) [<a href="../biblio/#kalchbrenner2018">KES+18</a>]. <a href="https://arxiv.org/abs/1810.11846">LPCNet</a> is a WaveRNN variant that achieves higher quality by combining linear prediction with the RNN [<a href="../biblio/#valin2019">VS19</a>].</p>
<p><strong>Deep Voice 3 (<a href="https://arxiv.org/abs/1710.07654">DV3</a>)</strong> by Baidu is a generative end-to-end model synthesizer, similar to Tacotron 2 [<a href="../biblio/#ping2017">PPG+17</a>]. The primary difference is that Tacotron 2 uses a fully convolutional topology to map the character embeddings to a Mel-spectrogram. This convolutional topology improves the computational efficiency and reduces the training time.</p>
<p><strong><a href="https://arxiv.org/abs/1807.07281">ClariNet</a></strong> by Baidu extends DV3 as a text-to-wave topology and uses a similar WaveNet distillation approach [<a href="../biblio/#ping2018">PPC18</a>].</p>
<div id="ch03.sec3.sub4"></div>
<h3 id="334-speech-to-speech-translation">3.3.4 Speech-to-Speech Translation</h3>
<p>Speech-to-speech translation is the task of generating interpreted speech between language pairs. This task can be done in three separate stages: ASR, MT, and TTS. Combining some or all of these stages avoids compounding errors and may result in lowered inference latency, but it is challenging due to the limited data. Google <a href="https://arxiv.org/abs/1811.02050">developed</a> a data augmentation process to improve the performance of a speech-to-translated-text (ST) system [<a href="../biblio/#jia2018">JJM+18</a>]. Google later developed <a href="https://arxiv.org/abs/1904.06037">Translatotron</a>, an end-to-end direct speech-to-speech translation atttention-based sequence-to-sequence model [<a href="../biblio/#jia2019">JWB+19</a>].</p>
<p>In the near future, persons without a common language may communicate in real-time using neural speech-to-speech interpreters. The generated voice may maintain the same voice characteristics as the input voice or some <a href="https://arxiv.org/abs/1806.04558">other</a> choice of voice [<a href="../biblio/#jia2018-b">JZW+18</a>].</p>
<div id="ch03.sec4"></div>
<h2 id="34-reinforcement-learning-algorithms">3.4 Reinforcement Learning Algorithms</h2>
<p>RL is used to teach an agent to perform certain actions based on rewards. The agent's goal is to take the required actions that maximize the cumulative reward over time. A simple task is the cart-pole balancing, depicted in Figure <a href="../ch03/#fig:cart">3.23</a>, where the reward depends on the height of the pole. The agent controls the cart and learns to balance the pole to maximize the reward.</p>
<div id="fig:cart"></div>
<p><img alt="" src="../figures/ch03-23.png" />
<em>Figure 3.23:</em> Reinforcement learning can be used to learn to balance the pole. Source: [<a href="../biblio/#wikimedia2012">Wik12</a>] (CC BY-SA 1.0).</p>
<p>The primary academic use of RL is in gaming, where the monetary cost of a mistake is minimal to none. RL beats human experts in <a href="https://arxiv.org/abs/1312.5602">Atari games</a>, Doom, <a href="https://arxiv.org/abs/1702.06230">Super Smash Bros</a>, <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">Starcraft 2</a>, Dota, Dota 2, chess, and Go [<a href="../biblio/#mnih2015">MKS+15</a>; <a href="../biblio/#firoiu2017">FWT11</a>; <a href="../biblio/#vinyals2019">VBC+19</a>]. The OpenAI Five team <a href="https://arxiv.org/abs/1912.06680">defeated</a> the professional human champions at DOTA 2 and demonstrated the use of RL for complex tasks, team cooperation, and strategizing in simulated environments [<a href="../biblio/#berner2019">BBC+19</a>].</p>
<p>In production, <a href="https://covariant.ai/">Covariant</a> uses RL in robots to accelerate warehouse operations; it is one of the few examples of successfully using RL commercially with physical agents. JPMorgan's internal RL system <a href="https://medium.com/@ranko.mosic/reinforcement-learning-based-trading-application-at-jp-morgan-chase-f829b8ec54f2">LOXM</a> is used <a href="https://arxiv.org/abs/1811.09549">to train</a> trading agents [<a href="../biblio/#mosic2017">Mos17</a>; <a href="../biblio/#bacoyannis2018">BGJ+18</a>]. Facebook <a href="https://arxiv.org/abs/1811.00260">uses</a> the open-source ReAgent (formerly called RL Horizon) platform for personalized notifications and recommendations [<a href="../biblio/#gauci2019">GCL+19</a>]. Microsoft acquired the <a href="https://www.bons.ai/">Bonsai</a> platform, designed to build autonomous industrial systems. Intel developed the <a href="https://github.com/NervanaSystems/coach">Coach</a> platform, which supports multiple RL algorithms and integrated environments, and is <a href="https://aws.amazon.com/blogs/machine-learning/custom-deep-reinforcement-learning-and-multi-track-training-for-aws-deepracer-with-amazon-sagemaker-rl-notebook/">integrated</a> into Amazon SageMaker RL. DeepMind built the <a href="https://github.com/deepmind/trfl/blob/master/docs/index.md">TRFL</a> platform and Google built the <a href="https://github.com/google/dopamine">Dopamine</a> platform (both on top of TensorFlow), and UC Berkeley released <a href="https://arxiv.org/abs/1712.05889">Ray</a> with the <a href="https://ray.readthedocs.io/en/latest/rllib.html">RLlib</a> reinforcement library to accelerate RL research [<a href="../biblio/#castro2018">CMG+18</a>; <a href="../biblio/#moritz2018">MNW+18</a>]. A comparison of various platforms is found <a href="https://winderresearch.com/a-comparison-of-reinforcement-learning-frameworks-dopamine-rllib-keras-rl-coach-trfl-tensorforce-coach-and-more/">elsewhere</a> [<a href="../biblio/#winder2020">Win20</a>]. Hardware targeting deep RL <a href="https://ieeexplore.ieee.org/document/8662447">has been developed</a> for edge applications [<a href="../biblio/#kim2019">KKS+19</a>].</p>
<p>It is impractical to train a physical agent in the physical world by allowing it to commit millions of errors. Rather, physics simulation engines simulate a real-world environment to train an agent. These simulators include <a href="https://arxiv.org/abs/1801.00690">DeepMind Control Suite</a> environments, MuJoCo locomotion environments, and OpenAI Gym, which standardized the simulation environment APIs and had a significant influence on the field [<a href="../biblio/#tassa2018">TYD+18</a>]. Other dynamic simulations <a href="https://ieeexplore.ieee.org/document/7139807">include</a> Bullet, Havoc, ODE, <a href="https://developer.nvidia.com/flex">FleX</a>, and PhysX [<a href="../biblio/#erez2015">ETT15</a>]. However, more realistic simulators are needed to transfer the learnings to physical agents. These agents are then fine-tuned in the physical world. Alternatively, interleaving simulation with some real-world rollouts <a href="https://arxiv.org/abs/1810.05687">works</a> for simple tasks [<a href="../biblio/#chebotar2019">CHM+19</a>].</p>
<p>Other challenges with RL are debugging and reward selection. For some tasks, <a href="https://openai.com/blog/faulty-reward-functions/">care is needed</a> to ensure the reward <a href="https://arxiv.org/abs/1808.04355">is aligned</a> with the programmer's end goal for the agent [<a href="../biblio/#amodei2016">AC16</a>; <a href="../biblio/#burda2018">BEP+18</a>]. RL can be difficult to debug because the lack of learning may be due to many factors, such as a suboptimal reward, a large exploration space with sparse rewards, or an issue with code. As a general guideline, it is best to start with a simple algorithm and incrementally increase the complexity. Simpler Monte Carlo Tree Search or Genetic Algorithms can tackle simple tasks.</p>
<p>RL algorithms often run multiple agents on CPUs; <a href="https://github.com/NervanaSystems/coach/tree/master/benchmarks/a3c">one per core</a> [<a href="../biblio/#caspi2017">CLN+17</a>]. Recent <a href="https://arxiv.org/abs/1803.02811">work</a>, such as the OpenAI Rapid system, shows that leveraging both CPUs and GPUs can improve performance [<a href="../biblio/#stooke2019">SA19</a>].</p>
<p>The three families of RL algorithms, shown in Figure <a href="../ch03/#fig:rl">3.24</a>, are Q-learning, policy optimization, and model-based.</p>
<div id="fig:rl"></div>
<p><img alt="" src="../figures/ch03-24.png" />
<em>Figure 3.24:</em> Families of deep RL algorithms. Based on [<a href="../biblio/#openai2018">Ope18</a>].</p>
<p><strong>Q-learning</strong>, also known as <strong>value-based</strong>, learns the quality of the agent's state and action. DeepMind popularized Q-learning in 2013 with the Deep Q-network (DQN) algorithm <a href="https://arxiv.org/abs/1312.5602">showing</a> superior performance than previous RL methods across various Atari games (soon after, Google acquired DeepMind, and is now a sibling company to Google under Alphabet) [<a href="../biblio/#mnih2013">KSe+13</a>]. Using a variety of Q-learning models <a href="https://arxiv.org/abs/1710.02298">achieves</a> better performance over any single Q-learning model [<a href="../biblio/#hessel2017">HMv+17</a>].</p>
<p><strong>Policy optimization</strong>, also known as <strong>on-policy</strong>, learns the policy function and selects the output action stochastically. A policy is the agent's strategy. A policy function maps the input state to a distribution of actions, and a DL model can represent this function.</p>
<p>Policy optimization was popularized by the Policy Gradient (PG) algorithm that <a href="https://www.nature.com/articles/nature14236">showed</a> superior performance over DQN [<a href="../biblio/#mnih2015">MKS+15</a>]. The space is explored initially through random actions. Actions that lead to a positive reward are more likely to be retaken.</p>
<p>A primary challenge is the sparse delayed rewards, formally known as the credit assignment problem. The agent receives a reward after taking several actions. The reward can be positive or negative. Depending on the reward, all the actions taken are considered good or bad, even if only some of them were critical to receiving the reward. Given the sparse rewards, policy optimization requires lots of training samples. Alternatively, manually shaping the rewards for a pasticular task can guide the learning behavior. Trust Region Policy Optimization (<a href="https://arxiv.org/abs/1502.05477">TRPO</a>) is typically used over vanilla PG as it guarantees monotonic policy improvements [<a href="../biblio/#schulman2017">SLM+17</a>]. A comparison of TRPO to DDPG and other PG-based algorithms, such as Proximal Policy Optimization (PPO) and Actor-Critic using Kronecker-Factored Trust Region (ACKTR) can be found <a href="https://arxiv.org/abs/1709.06560">elsewhere</a> [<a href="../biblio/#henderson2019">HIB+19</a>].</p>
<p>Various algorithms combine Q-learning and policy optimization methodologies. The most popular ones are <a href="https://arxiv.org/abs/1602.01783">A3C</a> and <a href="https://arxiv.org/abs/1509.02971">DDPG</a> [<a href="../biblio/#mnih2016">MBM+16</a>; <a href="../biblio/#lillicrap2019">LHP+19</a>]. Asynchronous Actor-Critic Agents (A3C) uses a policy-based actor and a value-based critic to measure how good is the chosen action. Deep Deterministic Policy Gradients (DDPG) uses continuous (rather than discrete) actions. While TRPO, DDPG, and A3C are typically good algorithms to use, experimentation is required to determine the most suitable for a particular task.</p>
<p><strong>Model-based</strong> algorithms use a model with the rules of their environment. The agent uses the model to infer the outcomes of various sets of actions and chooses the set with the maximum reward. Model-based algorithms are used in games like chess and Go, where the rules of the game are known. DeepMind's <a href="https://www.nature.com/articles/nature16961">AlphaGo</a>, <a href="https://www.nature.com/articles/nature24270.epdf">AlphaGo Zero</a>, <a href="https://science.sciencemag.org/content/362/6419/1140.full">AlphaZero</a>, and <a href="https://arxiv.org/abs/1911.08265">MuZero</a> use model-based algorithms [<a href="../biblio/#silver2016">SHM+16</a>; <a href="../biblio/#silver2017">SSS+17</a>; <a href="../biblio/#silver2018">SSS+18</a>; <a href="../biblio/#schrittwieser2020">SAH+20</a>]. Learning a model through trial and error introduces biases, and errors in the inferred outcome compound over the prediction horizon. Model-based policy optimization (<a href="https://arxiv.org/abs/1906.08253">MBPO</a>) uses a model with policy optimization to mitigate the compounding errors [<a href="../biblio/#janner2019">JFZ+19</a>].</p>
<p>In this chapter, we detailed the types of workloads that typically use DL models at hyperscalers: recommenders, computer vision, and NLP. We discussed the common topologies used in each of these workloads. Despite having the smallest adoption in academia, top hyperscalers widely use recommender models. We highlighted popular academic trends in RL that may soon transition to commercial applications. In the next chapter, we review how to train a topology, including how a data scientist may use an existing topology to guide the topology design for a related application.</p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../ch04/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../ch04/" class="btn btn-xs btn-link">
        Chapter 4: Training a Model
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../ch02/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../ch02/" class="btn btn-xs btn-link">
        Chapter 2: Building Blocks
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>