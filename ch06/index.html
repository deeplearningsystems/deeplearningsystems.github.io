<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    
    
    <link rel="shortcut icon" href="../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Chapter 6: Reducing the Model Size - Deep Learning Systems: Algorithms, Compilers, and Processors for Large-Scale Production</title>
    <link href="../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../css/highlight.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../js/jquery-3.2.1.min.js"></script>
    <script src="../js/bootstrap-3.3.7.min.js"></script>
    <script src="../js/highlight.pack.js"></script>
    
    <base target="_top">
    <script>
      var base_url = '..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "Chapter 6: Reducing the Model Size", url: "#_top", children: [
              {title: "6.1 Numerical Formats", url: "#61-numerical-formats" },
              {title: "6.2 Quantization Methodology", url: "#62-quantization-methodology" },
              {title: "6.3 Pruning and Compression", url: "#63-pruning-and-compression" },
              {title: "6.4 Knowledge Distillation", url: "#64-knowledge-distillation" },
          ]},
        ];

    </script>
    <script src="../js/base.js"></script>
      <script src="../javascripts/config.js"></script>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
  <a name="_top"></a>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../ch07/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../ch07/" class="btn btn-xs btn-link">
        Chapter 7: Hardware
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../ch05/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../ch05/" class="btn btn-xs btn-link">
        Chapter 5: Distributed Training
      </a>
    </div>
    
  </div>

    

    <div id="ch6"></div>
<h1 id="chapter-6-reducing-the-model-size">Chapter 6: Reducing the Model Size</h1>
<p>Computers represent real numerical values as a set of binary digits or bits, usually with 8, 16, 32, or 64 bits. The more bits used, the higher the numerical range and precision or representation of the numerical value. The numerical format of a model can impact its computational and statistical performance. Using a smaller numerical representation can increase the number of operations per cycle and reduce memory, memory bandwidth, network bandwidth, and power consumption. In particular, if a workload is memory bandwidth bound (bottlenecked by the memory bandwidth), reducing the numerical representation alleviates such bottleneck and improves the computational performance. If it is compute bound (bottlenecked by the compute available), hardware designers can pack more smaller numerical format multipliers into a given die area to improve the computational performance. However, using a smaller numerical representation may result in lower statistical performance for some models.</p>
<p>Figure <a href="../ch01/#fig:precisions">1.17</a> shows various numerical formats with the respective number of sign, exponent, and mantissa bits. The exponent bits determine the range, and the mantissa bits determine the precision. For instance, <span class="arithmatex">\(fp32\)</span> and <span class="arithmatex">\(bf16\)</span> have the same range factor, but <span class="arithmatex">\(fp32\)</span> provides higher precision.</p>
<p>There are four main techniques used to reduce the model size:</p>
<ol>
<li>
<p>reducing the numerical representation;</p>
</li>
<li>
<p>pruning (trimming) parts of the model and compressing the pruned model;</p>
</li>
<li>
<p>distilling the knowledge to a smaller model; and</p>
</li>
<li>
<p>using NAS that rewards small models.</p>
</li>
</ol>
<p>While most commercial applications use <span class="arithmatex">\(fp32\)</span> for training and inference workloads, lower numerical formats are rapidly gaining adoption. Specifically, half-precision floating-point (<span class="arithmatex">\(fp16\)</span>) and bfloat16 (<span class="arithmatex">\(bf16\)</span>) for training and inference and, for a subset of workloads, <span class="arithmatex">\(\mathit{int8}\)</span> for inference, all with <span class="arithmatex">\(32\)</span> bits accumulation for MAC operations. Using <span class="arithmatex">\(bf16\)</span> or <span class="arithmatex">\(fp16\)</span> multipliers with <span class="arithmatex">\(fp32\)</span> accumulators has insignificant to no loss in the accuracy for training and inference. Using <span class="arithmatex">\(\mathit{int8}\)</span> multipliers with <span class="arithmatex">\(\mathit{int32}\)</span> accumulators has some to minimal loss in the accuracy for some inference workloads. Note that storing the activations in a <span class="arithmatex">\(16\)</span>-bit format reduces memory and bandwidth consumption by almost <span class="arithmatex">\(2\times\)</span>, <a href="https://arxiv.org/abs/1705.01991">even</a> if the hardware does not support <span class="arithmatex">\(16\)</span>-bit multiplies [<a href="../biblio/#devlin2017">Dev17</a>].</p>
<p>Training requires a larger numerical representation than inference, in particular, to capture the dynamic range of the gradients and weight updates. Figure <a href="../ch06/#fig:log-dist">6.1</a> shows the histogram of log-base <span class="arithmatex">\(2\)</span> absolute values from ResNet-110 tensors across two separate training epochs and illustrates the larger range of the weight update values.</p>
<div id="fig:log-dist"></div>
<p><img alt="" src="../figures/ch06-01.png" />
<em>Figure 6.1:</em> Distributions of the ResNet-110 weights, activations, and weight updates at two separate training epochs using the CIFAR dataset. Adapted from [<a href="../biblio/#koster2017">KWW+17</a>] with the authors' permission.</p>
<p>An active research area is to develop numerical representations that better represent the values with <span class="arithmatex">\(8\)</span> bits and <span class="arithmatex">\(4\)</span> bits and are simple to implement in silicon. Using a smaller numerical representation can improve training and inference even if the hardware does not support higher peak operations per cycle at the smaller representation because the memory bandwidth savings accelerate memory bandwidth bound layers, which are common.</p>
<p>Models are typically overparameterized, which facilitates training and provides opportunities to reduce the model size post-training. Trained models typically have several small weights. Forcing them to zero can have computational advantages with minimal to no statistical impact. This process is called <em>pruning</em> and results in a sparse model. There are two types of model sparsity, discussed in Section <a href="../ch06/#ch06.sec3">6.3</a>, structured and unstructured.</p>
<p>A key benefit of sparse models is improved compression. Compression reduces the memory footprint and memory bandwidth consumption at the expense of some additional computations for decompression. The time for this additional decompression is usually less than the additional time to transmit the uncompressed data; therefore, compression is advantageous.</p>
<p>A small model can be trained to produce the output of a large trained model. The knowledge of the larger trained model (the teacher model) is distilled to the smaller model (the student model). This method is known as knowledge distillation.</p>
<p>In Section <a href="../ch06/#ch06.sec1">6.1</a>, we review the various <span class="arithmatex">\(16\)</span>-bit and <span class="arithmatex">\(8\)</span>-bit numerical formats adopted in production, as well as other promising formats. In Section <a href="../ch06/#ch06.sec2">6.2</a>, we discuss techniques to quantize a model from <span class="arithmatex">\(fp32\)</span> to <span class="arithmatex">\(\mathit{int8}\)</span>. In Section <a href="../ch06/#ch06.sec3">6.3</a>, we review pruning and compression techniques. In Section <a href="../ch06/#ch06.sec4">6.4</a>, we explain knowledge distillation in more detail.</p>
<div id="ch06.sec1"></div>
<h2 id="61-numerical-formats">6.1 Numerical Formats</h2>
<p>The most popular and widely adopted format is <span class="arithmatex">\(fp32\)</span> for both training and inference. The industry is moving toward <span class="arithmatex">\(fp16\)</span> and <span class="arithmatex">\(bf16\)</span> for training and inference, and for a subset of workloads, <span class="arithmatex">\(\mathit{int8}\)</span> for inference. Nvidia introduced a nonstandard <span class="arithmatex">\(fp19\)</span> format (sometimes referred to as bfloat19) for matrix multiplications, which combines the range of <span class="arithmatex">\(bf16\)</span> and the precision of <span class="arithmatex">\(fp16\)</span>. Intel and IBM explored nonstandard <span class="arithmatex">\(fp8\)</span> formats. Figure <a href="../ch01/#fig:precisions">1.17</a> shows various numerical formats with the respective number of sign, exponent, and mantissa bits. The mantissa is also known as the <em>significand</em> and should not be confused with the term <em>mantissa</em> used in the logarithmic literature to refer to the <a href="https://en.wikipedia.org/wiki/Common_logarithm">fractional part</a> of a logarithm.</p>
<p>Looking ahead for different hardware usages, the numerical formats that are or can be used across various types of development stages are:</p>
<ul>
<li>
<p>topology research and topology design: <span class="arithmatex">\(fp32\)</span>;</p>
</li>
<li>
<p>training production models in data centers: <span class="arithmatex">\(fp32\)</span>, <span class="arithmatex">\(bf16\)</span>, <span class="arithmatex">\(fp16\)</span>, and <span class="arithmatex">\(fp19\)</span>; limited <span class="arithmatex">\(fp8\)</span>;</p>
</li>
<li>
<p>serving production models in data centers: <span class="arithmatex">\(fp16\)</span>, <span class="arithmatex">\(bf16\)</span>, and <span class="arithmatex">\(fp8\)</span>; some <span class="arithmatex">\(\mathit{int8}\)</span>; extremely limited <span class="arithmatex">\(\mathit{int4}\)</span>; and</p>
</li>
<li>
<p>serving production models in edge devices: <span class="arithmatex">\(fp16\)</span> (depending on power constraints), <span class="arithmatex">\(\mathit{int8}\)</span>, and <span class="arithmatex">\(fp8\)</span>; some <span class="arithmatex">\(\mathit{int4}\)</span>.</p>
</li>
</ul>
<p>DL libraries, such as TensorFlow, PyTorch, MXNet, OpenVINO, and TensorRT, support <span class="arithmatex">\(\mathit{int8}\)</span>, <span class="arithmatex">\(fp16\)</span>, <span class="arithmatex">\(bf16\)</span>, and <span class="arithmatex">\(fp32\)</span>. For other formats to gain adoption, hardware and framework support is needed.</p>
<p>Table <a href="../ch06/#tab:precision">6.1</a> shows the range, the minimum and maximum positive values for the floating-point numbers, and the maximum numerical error across various numerical formats. <span class="arithmatex">\(fp8\)</span>-<span class="arithmatex">\(\mathit{ibm}\)</span> refers to an <span class="arithmatex">\(8\)</span>-bit floating-point format introduced by IBM and discussed below. <span class="arithmatex">\(u\{4,8\}\)</span> represents a <span class="arithmatex">\(\{4,8\}\)</span>-bit unsigned integer, <span class="arithmatex">\(s\{4,8,16,32\}\)</span> represents a <span class="arithmatex">\(\{4,8,16,32\}\)</span>-bit signed integer, and (<span class="arithmatex">\(n_S,n_E,n_M\)</span>) indicates the number of sign, exponent, and mantissa bits, respectively, of the floating-point formats. Thus, (<span class="arithmatex">\(1,8,23\)</span>) indicates a format with a sign bit, <span class="arithmatex">\(8\)</span> exponent bits, and <span class="arithmatex">\(23\)</span> mantissa bits, which corresponds to <span class="arithmatex">\(fp32\)</span>. The exponent bits determine the range and the mantissa bits the precision. The maximum numerical error of a given floating-point representation is the floating-point number multiplied by </p>
<div class="arithmatex">\[1/2^{(n_M + 1)}\]</div>
<p>or <span class="arithmatex">\(0.5\)</span> for the integer representations.</p>
<div id="tab:precision"></div>
<p><em>Table 6.1:</em> A comparison of different numerical formats. The maximum numerical error of a given floating-point representation is the floating-point number multiplied by <em>Maximun Error</em>.
<img alt="" src="../figures/ta06-01.png" /></p>
<p>Training a model with 16 bits (specifically <span class="arithmatex">\(bf16\)</span> or <span class="arithmatex">\(fp16\)</span>) usually requires the following:</p>
<ul>
<li>
<p>MAC operators with <span class="arithmatex">\(16\)</span>-bit operands accumulated to <span class="arithmatex">\(fp32\)</span>, and the accumulation is converted to <span class="arithmatex">\(16\)</span>-bit after totalling the running sum (note that the hardware logic may accumulate to less-bits registers, such as (<span class="arithmatex">\(1,8,21\)</span>) to reduce cost);</p>
</li>
<li>
<p>reductions (sums) accumulated to <span class="arithmatex">\(fp32\)</span> and the result converted to <span class="arithmatex">\(16\)</span>-bit;</p>
</li>
<li>
<p>activation functions at either <span class="arithmatex">\(fp32\)</span> or <span class="arithmatex">\(16\)</span>-bit;</p>
</li>
<li>
<p>activations stored in <span class="arithmatex">\(16\)</span>-bit;</p>
</li>
<li>
<p>a copy of <span class="arithmatex">\(fp32\)</span> weights used for the weight update (the updates use <span class="arithmatex">\(16\)</span>-bit gradients); and</p>
</li>
<li>
<p>a copy of the updated weights converted to <span class="arithmatex">\(16\)</span>-bit for the next iteration.</p>
</li>
</ul>
<p>The first three bullets also apply to inference with a <span class="arithmatex">\(16\)</span>-bit or <span class="arithmatex">\(8\)</span>-bit format. In both cases, accumulation to a larger numerical format is recommended to avoid numerical overflow (notation: MAC source <span class="arithmatex">\(\rightarrow\)</span> MAC destination): <span class="arithmatex">\(\{fp16, bf16\}\rightarrow fp32\)</span>, and <span class="arithmatex">\(\mathit{int8} \rightarrow s32\)</span> (signed <span class="arithmatex">\(\mathit{int32}\)</span>).</p>
<p><strong>Floating-point <span class="arithmatex">\(\mathbf{16}\)</span>-bit bfloat</strong> (<span class="arithmatex">\(bf16\)</span>) was introduced by Google as brain floating-point. Models are robust to additive noise, and, in fact, it is a common practice to add noise when training a model in the form of weight decay regularization, as discussed in Section <a href="../ch04/#ch04.sec1">4.1</a>. Reducing the mantissa bits from <span class="arithmatex">\(23\)</span> in <span class="arithmatex">\(fp32\)</span> to <span class="arithmatex">\(7\)</span> in <span class="arithmatex">\(bf16\)</span> can be interpreted as injecting noise into the model. <span class="arithmatex">\(bf16\)</span> maintains the same range factor as <span class="arithmatex">\(fp32\)</span> and is particularly useful to support the range in the gradients. Experiments <a href="https://arxiv.org/abs/1905.12322">demonstrate</a> that models trained with <span class="arithmatex">\(bf16\)</span> have virtually the same accuracy as those trained with <span class="arithmatex">\(fp32\)</span> with the same number of iterations, without changing any hyperparameter, and without scaling the objective function cost [<a href="../biblio/#kalamkar2019">KMM+19</a>]. However, there may be outlier models where these observations are not valid. Also, when the number of classes is greater than <span class="arithmatex">\(2^{n_M}\)</span> or <span class="arithmatex">\(127\)</span>, <span class="arithmatex">\(fp32\)</span> should be used for the cost function. Moreover, while softmax alone can use <span class="arithmatex">\(bf16\)</span>, various implementations combine the softmax function and the cost function. Those implementations should use <span class="arithmatex">\(fp32\)</span>.</p>
<p>While <span class="arithmatex">\(bf16\)</span> was primarily designed for training (the large exponent to represent the gradients), it is also used for inference with similar computational gains over <span class="arithmatex">\(fp32\)</span>. Google TPU v2-4, the Habana Gaudi AI processor, the 3rd-generation Intel Xeon Scalable processor (codename Cooper Lake), the Arm-based Neoverse N2 "Zeus" CPU, and the Nvidia A100 GPU have <span class="arithmatex">\(bf16\)</span> multipliers.</p>
<p><strong>Floating-point <span class="arithmatex">\(\mathbf{16}\)</span>-bit half-precision</strong> (<span class="arithmatex">\(fp16\)</span>) is used for inference and training, the latter often requiring a technique known as <em>loss-scaling</em>. During training, particularly during the early stages, the magnitude of many activation gradients often falls below the supported range of <span class="arithmatex">\(fp16\)</span> and gets truncated to zero and the upper range of <span class="arithmatex">\(fp16\)</span> is unutilized. Scaling the loss (more precisely, the cost or objective function), mitigates this inability to represent very small values and enables the use of the higher range. Specifically, the cost is scaled by a value <span class="arithmatex">\(\gg 1\)</span> without overflowing the activation gradients past the upper <span class="arithmatex">\(fp16\)</span> range. Then, unscaling the weight gradients by the same factor before the weight update. In addition, normalizing <span class="arithmatex">\(0\)</span>-<span class="arithmatex">\(255\)</span> RGB input image value to <span class="arithmatex">\(0\)</span>-<span class="arithmatex">\(1\)</span> and adding batch normalization to the activation <a href="https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9659-inference-at-reduced-precision-on-gpus.pdf">reduces</a> overflow risks [<a href="../biblio/#wu2019-e">Wu19</a>]. Nvidia GPUs, AMD Radeon GPUs, Huawei Atlas and Ascend processors, and Graphcore Colossus have <span class="arithmatex">\(fp16\)</span> multipliers.</p>
<p>The primary advantage of <span class="arithmatex">\(bf16\)</span> over <span class="arithmatex">\(fp16\)</span> is avoiding the need to implement loss-scaling, which requires empirical tuning. This advantage is particularly significant for models requiring dynamic loss scaling (and dynamic tuning) <a href="https://arxiv.org/abs/1905.12334">such as</a> GNMT and Transformer, given the large variations in gradient distribution throughout training, which increases the software complexity [<a href="../biblio/#mellempudi2019">MSD+19</a>]. Some tools, such as <a href="https://arxiv.org/abs/1805.10387">OpenSeq2Seq</a>, can automate dynamic loss scaling for some models [<a href="../biblio/#kuchaiev2018">KGG+18</a>].</p>
<p>A disadvantage of <span class="arithmatex">\(bf16\)</span> over <span class="arithmatex">\(fp16\)</span> is the <span class="arithmatex">\(3\)</span> fewer mantissa bits; there may be some precision-sensitive workloads that benefit from those bits. The upper range values of <span class="arithmatex">\(bf16\)</span> are not used, bringing to question the need for <span class="arithmatex">\(8\)</span> exponent bits for most training workloads. Facebook, for instance, uses <span class="arithmatex">\(fp16\)</span> (rather than <span class="arithmatex">\(bf16\)</span>) <a href="https://research.fb.com/publications/training-with-low-precision-embedding-tables/">to store the embedding layers</a> (not for MAC operators) in DLRM training (the MAC operators of the embedding layers happen in <span class="arithmatex">\(fp32\)</span>) [<a href="../biblio/#zhang2018">ZYY18</a>]. In designing a training processor, it is recommended to support both <span class="arithmatex">\(fp16\)</span> and <span class="arithmatex">\(bf16\)</span> (using a <span class="arithmatex">\(19\)</span>-bit (<span class="arithmatex">\(1,8,10\)</span>) <span class="arithmatex">\(fp19\)</span> floating-point circuitry unit) to facilitate transitioning from existing hardware that only support one format (<span class="arithmatex">\(fp16\)</span> or <span class="arithmatex">\(bf16\)</span>).</p>
<p><strong>TensorFloat-32 with <span class="arithmatex">\(\mathbf{19}\)</span>-bit floats</strong> (<span class="arithmatex">\(tf32\)</span>) was introduced by Nvidia starting in the Ampere architecture. TensorFloat-32 uses <span class="arithmatex">\(fp19\)</span> MACs with <span class="arithmatex">\(fp32\)</span> accumulation. All the operations and storage happen in <span class="arithmatex">\(fp32\)</span> except for the MAC operations used in matrix multiplications. Those <span class="arithmatex">\(fp32\)</span> MACs are replaced with <span class="arithmatex">\(fp19\)</span> MACs and accelerated with specialized tensor cores. This replacement can be hidden to the framework end-user, where everything seems to run in <span class="arithmatex">\(fp32\)</span>. The <span class="arithmatex">\(fp32\)</span> to <span class="arithmatex">\(fp19\)</span> conversions (truncating the last <span class="arithmatex">\(13\)</span> mantissa bits) and the <span class="arithmatex">\(fp19\)</span> MACs are managed by the CUDA compiler and hidden by low-level libraries, such as cuDNN and cuBLAS. The accuracy of <span class="arithmatex">\(fp19\)</span> MACs is not guaranteed to be the same as <span class="arithmatex">\(fp32\)</span> MACs. However, empirical evidence using <span class="arithmatex">\(bf16\)</span> (which carries to <span class="arithmatex">\(fp19\)</span>) <a href="https://arxiv.org/abs/1905.12322">suggests</a> that for DL workloads, the accuracy difference is insignificant; although unknown outliers may exist [<a href="../biblio/#kalamkar2019">KMM+19</a>].</p>
<p>The primary advantage of <span class="arithmatex">\(tf32\)</span> is the ease-of-adoption. It requires no changes in the DL libraries (except for an enablement flag) and works out-of-the-box. The disadvantage is the lack of memory or bandwidth savings compared to <span class="arithmatex">\(16\)</span>-bit formats, which is often the bigger bottleneck.</p>
<p><strong>Integer-<span class="arithmatex">\(\mathbf{16}\)</span></strong> (<span class="arithmatex">\(\mathit{int16}\)</span>) training has been <a href="https://arxiv.org/abs/1711.02213">demonstrated</a> on <a href="https://arxiv.org/abs/1802.00930">some</a> models with no hyperparameters tuning [<a href="../biblio/#koster2017">KWW+17</a>; <a href="../biblio/#das2018">DMM+18</a>]. The distribution of the weights, activations, weight gradients, and activation gradients in a tensor can be represented using <span class="arithmatex">\(\mathit{int16}\)</span> and one shared scalar for the entire tensor. This scalar is dynamically adjusted to maximize range and minimize overflow. The weight and activation distributions do not change rapidly in consecutive training iterations. The gradient distribution changes more rapidly. A program can monitor the distributions and adjust the exponents for each tensor as needed.</p>
<p>For training, <span class="arithmatex">\(\mathit{int16}\)</span> is not used in production; <span class="arithmatex">\(bf16\)</span> and <span class="arithmatex">\(fp16\)</span> are preferred over <span class="arithmatex">\(\mathit{int16}\)</span> given the added complexity to manage the shared exponent with <span class="arithmatex">\(\mathit{int16}\)</span>, particularly for the gradient tensors. For inference, <span class="arithmatex">\(\mathit{int16}\)</span> has some adoption. Habana Goya <a href="https://habana.ai/wp-content/uploads/pdf/habana_labs_goya_whitepaper.pdf">uses</a> <span class="arithmatex">\(\mathit{int16}\)</span> for workloads that required more precision than <span class="arithmatex">\(\mathit{int8}\)</span> (Habana Goya also <a href="https://habana.ai/wp-content/uploads/pdf/habana_labs_goya_whitepaper.pdf">supports</a> other formats) [<a href="../biblio/#habana2019">Hab19</a>].</p>
<p><strong>Integer-<span class="arithmatex">\(\mathbf{8}\)</span></strong> (<span class="arithmatex">\(\mathit{int8}\)</span>) is rapidly gaining adoption for <em>some</em> inference workloads. Using <span class="arithmatex">\(\mathit{int8}\)</span> often reduces the statistical performance due to the information loss quantizing from <span class="arithmatex">\(32\)</span>-bit to <span class="arithmatex">\(8\)</span>-bit. For some applications, a small drop in statistical performance is unacceptable, as it can have a negative monetary impact. In particular, less relevant product recommendation results in reduced purchases. There are techniques to reduce the statistical loss discussed in Section <a href="../ch06/#ch06.sec2">6.2</a>. Note that training with <span class="arithmatex">\(\mathit{int8}\)</span> is limited to academic research on a few simple models not relevant in industry.</p>
<p>There are two main challenges with most <span class="arithmatex">\(\mathit{int8}\)</span> quantization techniques. First, the uniform distribution of <span class="arithmatex">\(\mathit{int8}\)</span> does not allow finer-granularity to better represent values in high-density regions where most of the information exists. A better approach is to use a nonuniform numerical format with high granularity in high-density regions and low granularity in low-density regions. This reduces the <span class="arithmatex">\(32\)</span>- to <span class="arithmatex">\(8\)</span>-bit information loss. Some proposals, such as <span class="arithmatex">\(fp8\)</span>, are discussed below.</p>
<p>Second, precomputing the activations' quantization factors is needed to maximize the computational benefits of <span class="arithmatex">\(\mathit{int8}\)</span> but requires additional effort for the developer. The distribution of the activation values with production data can be estimated using data samples with similar characteristics as the production data. This requires that a developer quantizing a model has access to production-like data samples.</p>
<p>Despite these challenges, <span class="arithmatex">\(\mathit{int8}\)</span> is supported by all prevalent hardware marketed for inference. Google <a href="https://arxiv.org/abs/1704.04760">uses</a> <span class="arithmatex">\(\mathit{int8}\)</span> in production on TPUs for some MLP-, CNN-, and LSTM-based models, and on the Google Pixel phone <a href="https://arxiv.org/abs/1811.06621">for speech recognition</a> with RNN models. Facebook (as well as many other companies) also uses <span class="arithmatex">\(\mathit{int8}\)</span> across <a href="https://arxiv.org/abs/1811.09886">various workloads</a> [<a href="../biblio/#jouppi2017">JYP+17</a>; <a href="../biblio/#he2019">HSP+19</a>; <a href="../biblio/#park2018">PNB+18</a>]. Facebook also <a href="https://arxiv.org/abs/1911.02079">demonstrated</a> quantization to <span class="arithmatex">\(4\)</span> bits on the embedding layers for serving recommendations without affecting statistical performance.</p>
<p>In particular <span class="arithmatex">\(\mathit{int8}\)</span> inference has been <a href="http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">shown</a> to work across various CNN models [<a href="../biblio/#guan2019">GMY+19</a>]. However, even some CNN models like MobileNet and ResNeXt, and various non-CNNs such as <a href="https://arxiv.org/abs/1909.05840">BERT</a>, are more susceptible to information loss from quantization and require <a href="https://arxiv.org/abs/1909.05840">additional effort</a> to achieve acceptable statistical performance [<a href="../biblio/#shen2019">SDY+19</a>]. While the acceptable degradation varies, for most companies degradation over <span class="arithmatex">\(1\%\)</span> is unacceptable, under <span class="arithmatex">\(0.5\%\)</span> is acceptable, and in between depends on the application. Recommenders have a stricter threshold in the order of 0.01% due to the monetization impact.</p>
<p><strong>Floating-point <span class="arithmatex">\(8\)</span>-bit</strong> (<span class="arithmatex">\(fp8\)</span>) is used by <a href="https://ieeexplore.ieee.org/document/8344479">Microsoft in FPGAs</a> (Microsoft also uses <span class="arithmatex">\(fp9\)</span>) using either <span class="arithmatex">\(2\)</span> or <span class="arithmatex">\(3\)</span> mantissa bits. <span class="arithmatex">\(fp8\)</span> is implemented by researchers in some ASICs, such as the <a href="https://ieeexplore.ieee.org/document/8662302">deep-learning neural processing unit</a> (LNPU) to demonstrate <em>training</em> models on mobile devices (LNPU uses <span class="arithmatex">\(fp8\)</span> and <span class="arithmatex">\(fp16\)</span> mixed precision training) [<a href="../biblio/#chung2018">CFO+18</a>; <a href="../biblio/#lee2019">LLH+19</a>]. <a href="https://openreview.net/forum?id=HJe88xBKPr">Intel</a> and <a href="https://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks">IBM</a> demonstrate that <span class="arithmatex">\(fp8\)</span> multiplies (accumulated to <span class="arithmatex">\(fp32\)</span> and <span class="arithmatex">\(fp16\)</span>, respectively) can be used for training and inference with insignificant loss in performance for various workloads [<a href="../biblio/#cambier2020">CBG+20</a>; <a href="../biblio/#mellempudi2019">MSD+19</a>; <a href="../biblio/#sun2019">SCC+19</a>].</p>
<p>There is no standardized <span class="arithmatex">\(fp8\)</span> format. The most common formats are (<span class="arithmatex">\(1,5,2\)</span>) and (<span class="arithmatex">\(1,4,3\)</span>). The (<span class="arithmatex">\(1,5,2\)</span>) format better represents the dynamic range of the gradients. A particular challenge in training with an <span class="arithmatex">\(8\)</span>-bit format is in RNNs and models without normalization layers, as they are more susceptible to errors. The gradient errors can quickly increase in RNNs, and the typical lack of normalization can result in irregular tensor value distributions.</p>
<p>IBM <a href="https://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks">proposed</a> a hybrid (<span class="arithmatex">\(1,4,3\)</span>) and (<span class="arithmatex">\(1,5,2\)</span>) approach for the forward and backpropagation, respectively, using loss-scaling and stochastic rounding, and keeping the input and last layers at <span class="arithmatex">\(fp16\)</span> [<a href="../biblio/#sun2019">SCC+19</a>]. The (<span class="arithmatex">\(1,4,3\)</span>) format is modified using a <span class="arithmatex">\(-4\)</span> fixed exponent bias to shift the coverage range by <span class="arithmatex">\(2^{-4}\)</span> to better align with the distribution of the weights and activations. This format is referred to as <span class="arithmatex">\(fp8\)</span>-<span class="arithmatex">\(\mathit{ibm}\)</span> in Table <a href="../ch06/#tab:precision">6.1</a>. There are two primary challenges to this format. First, some models, <a href="https://arxiv.org/abs/1905.12334">such as</a> GNMT and Transfomer, require dynamic loss to <a href="https://arxiv.org/abs/1805.10387">properly converge</a>, which increases the software complexity. Second, the more limited representation of small values, compared to <span class="arithmatex">\(fp16\)</span> (the smallest positive values are <span class="arithmatex">\(1.5\times 10^{-5}\)</span> in (<span class="arithmatex">\(1,5,2\)</span>) vs. <span class="arithmatex">\(6.0\times 10^{-8}\)</span> in (<span class="arithmatex">\(1,5,10\)</span>), often results in underflow.</p>
<p>Intel has proposed two methods, both using the (<span class="arithmatex">\(1,5,2\)</span>) format. One <a href="https://openreview.net/forum?id=Bkxe2AVtPS">method</a> uses a shift and scale (shifted and squeezed FP8 (S2FP8)) parameter per tensor to represent a broad set of values. S2FP8 alleviates the need for loss-scaling, stochastic rounding, and <span class="arithmatex">\(fp32\)</span> for the first and last layer. The main weights and accumulations are in <span class="arithmatex">\(fp32\)</span> [<a href="../biblio/#cambier2020">CBG+20</a>]. However, S2FP8 requires tracking the statistics in the tensor distribution (similar to <span class="arithmatex">\(\mathit{int16}\)</span> training) and updating the shift and scale parameters which increases the software complexity.</p>
<p>The other <a href="https://arxiv.org/abs/1905.12334">method</a> uses enhanced loss scaling to improve the range of values and reduce the common underflow observed with <span class="arithmatex">\(fp8\)</span> training. This method uses loss scaling with a dynamically increasing minimum threshold for the scaling factor. Using a minimum threshold ignores spurious overflows in order to maintain a higher loss scale value. However, this method <a href="https://openreview.net/forum?id=HJe88xBKPr&amp;noteId=Hyx3iPknoH">requires</a> observing the training cost to determine when to adjust this threshold value.</p>
<p>A significant advantage of <span class="arithmatex">\(fp8\)</span> over <span class="arithmatex">\(\mathit{int8}\)</span> inference is circumventing the complexities of quantization. The current disadvantage is the limited hardware and software supporting <span class="arithmatex">\(fp8\)</span> formats. A minor disadvantage is that NaNs are overrepresented and consume <span class="arithmatex">\(6\)</span> out of <span class="arithmatex">\(256\)</span> (<span class="arithmatex">\(2\%\)</span>) and <span class="arithmatex">\(14\)</span> out of <span class="arithmatex">\(256\)</span> (<span class="arithmatex">\(6\%\)</span>) values in the (<span class="arithmatex">\(1,5,2\)</span>) and (<span class="arithmatex">\(1,4,3\)</span>) formats, respectively.</p>
<p>The published <span class="arithmatex">\(fp8\)</span> empirical results suggest that for the backpropagation (<span class="arithmatex">\(1,5,2\)</span>) is preferred over (<span class="arithmatex">\(1,4,3\)</span>). For inference (forward propagation), IBM demonstrated superior statistical performance using (<span class="arithmatex">\(1,4,3\)</span>) with the exponent shift, albeit the results are primarily targeting convolutional models. Intel demonstrated (<span class="arithmatex">\(1,5,2\)</span>) for both forward and backpropagation across ResNet, GNMT, Transformer, and NCF. The published results suggest that CNN models can benefit more from the additional mantissa bit in (<span class="arithmatex">\(1,4,3\)</span>), and non-CNN models can benefit more from the additional exponent bit in (<span class="arithmatex">\(1,5,2\)</span>). Nevertheless, the number of models in these studies is relatively small, and making solid conclusions requires further work.</p>
<p><strong>Integer-<span class="arithmatex">\(\mathbf{4}\)</span></strong> (<span class="arithmatex">\(\mathit{int4}\)</span>) support is available in recent Nvidia GPUs. <span class="arithmatex">\(\mathit{int4}\)</span> inference adoption on some CNN models may slowly grow on edge devices, such as in mobile phones, where power and memory are limited. The adoption in data centers may likely be none to very limited for workloads tolerant to extremely low range and precision and limited to representing activations from ReLU functions with unsigned <span class="arithmatex">\(\mathit{int4}\)</span> (the weights kept at <span class="arithmatex">\(\mathit{int8}\)</span>). There is ongoing research toward improving <span class="arithmatex">\(\mathit{int4}\)</span> quantization [<a href="../biblio/#choi2018">CWV+18</a>; <a href="../biblio/#dong2019">Don19</a>; <a href="../biblio/#guan2019">GMY+19</a>].</p>
<p><strong>Floating-point <span class="arithmatex">\(\mathbf{24}\)</span>-bit</strong> (<span class="arithmatex">\(fp24\)</span>) (<span class="arithmatex">\(1,8,15\)</span>) is used by Alibaba Neural Processing Unit (NPU) for CNN models for the element-wise and reduction operators (the matrix-wise operators use <span class="arithmatex">\(\mathit{int8}\rightarrow \mathit{int16}\)</span>) [<a href="../biblio/#jiao2020">JHJ+20</a>].</p>
<p><strong>Posit</strong> is a relatively new <a href="https://posithub.org/docs/Posits4.pdf">format</a> different from the IEEE floating standard. This format requires less power and die area than the IEEE floating-point counterpart [<a href="../biblio/#gustafson2017">Gus17</a>; <a href="../biblio/#johnson2018">Joh18</a>]. It does not overrepresent NaNs and provides <a href="https://dl.acm.org/doi/10.1145/3316279.3316285">other</a> benefits and drawbacks [<a href="../biblio/#dinechin2019">dDF+19</a>]. However, this format has minimal adoption in academia and none in industry.</p>
<p><strong>Log-domain</strong> is another form of <a href="https://arxiv.org/abs/1603.01025">nonlinear</a> quantization that <a href="https://ieeexplore.ieee.org/document/7953288">has been shown</a> to maintain statistical performance with smaller numerical formats [<a href="../biblio/#lee2017">LMC+17</a>]. This format has limited adoption in academia and none in industry.</p>
<p><strong>Binary</strong> (<a href="https://arxiv.org/abs/1603.05279"><span class="arithmatex">\(1\)</span> bit</a>) and <strong>ternary</strong> (<a href="https://ieeexplore.ieee.org/abstract/document/6986082"><span class="arithmatex">\(2\)</span> bits</a> to represent <span class="arithmatex">\(-1\)</span>, <span class="arithmatex">\(0\)</span>, and <span class="arithmatex">\(1\)</span>) have been used in research, in particular, to represent the weights in a forward propagation passes [<a href="../biblio/#rastegari2016">ROR+16</a>; <a href="../biblio/#hwang2014">HS14</a>].</p>
<div id="ch06.sec1.sub1"></div>
<h3 id="611-die-cost">6.1.1 Die Cost</h3>
<p>The die cost to build a multiplier, and the power cost to use the multiplier both exhibit quadratic growth with the number of mantissa bits and increase linearly with the number of exponent bits. Therefore, a <span class="arithmatex">\(bf16\)</span> multiplier is less expensive than a <span class="arithmatex">\(fp16\)</span> multiplier. However, area costs continue to decrease rapidly, and therefore this difference should not be a major factor in the DL hardware design decisions. Usability and software development costs are much more critical factors.</p>
<p>To facilitate transitioning from hardware that only support one format (<span class="arithmatex">\(fp16\)</span> or <span class="arithmatex">\(bf16\)</span>), we recommend designing hardware that supports both <span class="arithmatex">\(bf16\)</span> and <span class="arithmatex">\(fp16\)</span> formats using a <span class="arithmatex">\(19\)</span>-bit (<span class="arithmatex">\(1,8,10\)</span>) floating-point unit (FPU). Similarly, we recommend supporting both (<span class="arithmatex">\(1,5,2\)</span>) and (<span class="arithmatex">\(1,4,3\)</span>) <span class="arithmatex">\(fp8\)</span> formats using a 9-bit (<span class="arithmatex">\(1,5,3\)</span>) FPU. According to IBM, supporting both formats only <a href="https://papers.nips.cc/paper/8736-hybrid-8-bit-floating-point-hfp8-training-and-inference-for-deep-neural-networks">requires</a> a <span class="arithmatex">\(5\%\)</span> larger unit than supporting one format [<a href="../biblio/#sun2019">SCC+19</a>].</p>
<div id="ch06.sec2"></div>
<h2 id="62-quantization-methodology">6.2 Quantization Methodology</h2>
<p>Using <span class="arithmatex">\(\mathit{int8}\)</span> can improve the computational performance at the expense of some (1) additional development and (2) loss in statistical performance. In this section, we explain the quantization methodology and share techniques that can mitigate loss in statistical performance and reduce the development process.</p>
<p>Assuming an <span class="arithmatex">\(fp32\)</span>, <span class="arithmatex">\(fp16\)</span>, or <span class="arithmatex">\(bf16\)</span>, trained model a simple technique to quantize to <span class="arithmatex">\(\mathit{int8}\)</span> is as follows: For each weight tensor, the maximum absolute value is mapped to <span class="arithmatex">\(\pm 127\)</span>. For the activation tensors, a representative sample of the production data, called the <em>calibration dataset</em>, is used to collect activations statistics to find the distribution of activation values in each tensor across the samples. The quantization factor is: </p>
<div class="arithmatex">\[Q_{\mathbf{a}, \mathbf{w}} = \frac{127}{\max(\mathit{abs} (T_{\mathbf{a}, \mathbf{w}}))},\]</div>
<p>where <span class="arithmatex">\(T_{\mathbf{a}, \mathbf{w}}\)</span> is a tensor corresponding to either the weights <span class="arithmatex">\(\mathbf{w}\)</span> or the activations <span class="arithmatex">\(\mathbf{a}\)</span> (recall that the inputs to the NN can be considered the activations of Layer <span class="arithmatex">\(0\)</span>). The quantized values are:</p>
<p>where the function <span class="arithmatex">\(\Phi(\cdot)\)</span> rounds to the nearest integer.</p>
<p>The following techniques can improve <span class="arithmatex">\(\mathit{int8}\)</span> inference accuracy. Note that even with these techniques, the loss over <span class="arithmatex">\(fp32\)</span> accuracy may still be unacceptable for some applications.</p>
<p><strong>Asymmetric quantization</strong> uses a scalar and a shift factor, which can improve the quantization of the <em>activations</em>. Note that the weights are typically approximately zero-mean and should use symmetric quantization. The minimum activation value gets <a href="https://www.tensorflow.org/lite/performance/quantization_spec">mapped</a> to <span class="arithmatex">\(-128\)</span> and the maximum value to <span class="arithmatex">\(127\)</span>.</p>
<p><strong>Threshold calibration</strong> requires deployment-like data (unlabeled data is OK) and no additional backpropagation. Mapping the largest absolute value to <span class="arithmatex">\(\pm 127\)</span> (or in asymmetric quantization the minimum and maximum value to <span class="arithmatex">\(-128\)</span> and <span class="arithmatex">\(127\)</span>, respectively) may result in poor utilization of the available <span class="arithmatex">\(256\)</span> <span class="arithmatex">\(\mathit{int8}\)</span> values when an outlier number is much larger than the other numbers. To illustrate, suppose the largest number is <span class="arithmatex">\(10\times\)</span> larger than the next largest value. That one number gets mapped to <span class="arithmatex">\(127\)</span>, and the rest of the values can only map to <span class="arithmatex">\([-13,13]\)</span>. It is better to ignore outliers and find a threshold that minimizes the reconstruction error back to <span class="arithmatex">\(fp32\)</span>. Another <a href="https://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf">approach</a> that works for some CNN models is to truncate outliers to minimize the information loss measured by the KL-divergence between the larger numerical representation tensor distribution and the quantized tensor distribution [<a href="../biblio/#migacz2017">Mig17</a>]. Note that KL-divergence minimizes a metric of error in a layer, which may not minimize the accuracy error in the entire model. In practice, just using a threshold that captures <span class="arithmatex">\(99\%\)</span> or <span class="arithmatex">\(99.9\%\)</span> of the values results in superior performance accuracy.</p>
<p><strong>Quantization aware training (QAT)</strong> requires labeled data (training data) and backpropagation. QAT (as opposed to post-training quantization) fine-tunes a model while enforcing quantization, and <a href="https://www.tensorflow.org/lite/performance/model_optimization#latency_and_accuracy_results">has been shown</a> to improve accuracy. At each training iteration, the weights and activations of the layers targeted for quantization are fake-quantized to mimic <span class="arithmatex">\(\mathit{int8}\)</span> values. The cost used in the backpropagation is based on the quantized values. The gradients and the weights updates are computed in single-precision. Another advantage is that QAT eliminates the need for the threshold calibration step as QAT minimizes the reconstruction error of the quantized values.</p>
<p><strong>Selective quantization</strong> requires labeled data but no backpropagation. Some layers, such as softmax, tanh, sigmoid, depthwise-separable convolution, GELU, and the input and output layers, are more sensitive to quantization and should be kept at the larger numerical format to reduce the accuracy loss [<a href="../biblio/#wu2019-e">Wu19</a>]. The sensitivity of softmax can be slightly reduced by accumulating the logits in the larger numerical format and <a href="https://academic.oup.com/imajna/advance-article/doi/10.1093/imanum/draa038/5893596">subtracting</a> the max value before quantizing [<a href="../biblio/#blanchard2020">BHH20</a>]. The activation output of GELU can be clipped, for instance, to <span class="arithmatex">\(10\)</span>, in order to allow some <span class="arithmatex">\(\mathit{int8}\)</span> value to represent the GELU negative activation values.</p>
<p>Analyzing an approximation of the Hessian matrix's trace is recommended to <a href="https://arxiv.org/abs/1911.03852">assess</a> the sensitivity of a layer. This technique can be used to reduce the numerical format to <span class="arithmatex">\(4\)</span> bits for some layers with minimal accuracy loss [<a href="../biblio/#dong2019-b">DYC+19</a>]. Other less reliable but faster-to-compute metrics to assess sensitivity are the KL-divergence, and the root mean squared error (RMSE) with the reconstructed <span class="arithmatex">\(fp32\)</span> model. RL can facilitate designing a quantized model optimized for latency, energy, and accuracy for a particular hardware target. A possible algorithm for selective quantization follows:</p>
<p>Quantize all the layers and approximate the average Hessian trace for each layer [<a href="../biblio/#dong2019-b">DYC+19</a>]\ Set the maximum acceptable accuracy error <span class="arithmatex">\(E\)</span>\ [[alg:quantization]]{#alg:quantization label="alg:quantization"}</p>
<p>This algorithm determines the layers that can be quantized. Note that one challenge is that interleaving layers with large and small numerical formats may result in higher computational cost from the overhead of the many conversions.</p>
<p><strong>Cross-layer range equalization</strong> is a data-free quantization (requires no data and no backpropagation). The range of weights across the layers is <a href="https://arxiv.org/abs/1906.04721">equalized</a>, and the range of activations are constraint under the assumption that a piece-wise linear activation function (such as ReLU) is used between the layers [<a href="../biblio/#nagel2019">NvB+19</a>]. This constraint is satisfied by many CNN models but not by non-CNN models. This technique <a href="https://www.qualcomm.com/news/onq/2019/07/30/new-research-quantization-could-revolutionize-power-efficient-ai">is used</a> in the Qualcomm Neural Processing SDK.</p>
<p><strong>Channel-wise quantization</strong> uses a quantization factor for each channel rather than one factor for the entire tensor.</p>
<p><strong>Stochastic rounding</strong> (rather than nearest-value rounding) after multiplying by the quantization factor can <a href="https://arxiv.org/abs/1812.08011">improve performance</a> [<a href="../biblio/#wang2018-b">WCB+18</a>]. To illustrate, rather than rounding the number <span class="arithmatex">\(1.2\)</span> to the number <span class="arithmatex">\(1\)</span>, it is rounded to <span class="arithmatex">\(1\)</span> with <span class="arithmatex">\(80\%\)</span> probability and to <span class="arithmatex">\(2\)</span> with <span class="arithmatex">\(20\%\)</span> probability.</p>
<p><strong>Unsigned <span class="arithmatex">\(\mathit{int8}\)</span> ReLU activations</strong> uses the unsigned <span class="arithmatex">\(\mathit{int8}\)</span> representation, rather than signed <span class="arithmatex">\(\mathit{int8}\)</span>, for the activations of the ReLU functions. Using signed <span class="arithmatex">\(\mathit{int8}\)</span> wastes half of the values since all the activations are nonnegative.</p>
<p>The techniques QAT, selective quantization, channel-wise quantization, and stochastic rounding also benefit <span class="arithmatex">\(fp8\)</span> [<a href="../biblio/#cambier2020">CBG+20</a>].</p>
<div id="ch06.sec3"></div>
<h2 id="63-pruning-and-compression">6.3 Pruning and Compression</h2>
<p>Trained models typically have several weights that are approximately zero. Pruning them, that is, forcing all the weights less than some small <span class="arithmatex">\(\epsilon\)</span> value to zero results in a sparse model. Selecting a good value for <span class="arithmatex">\(\epsilon\)</span> requires experimentation. Pruning has been used for several decades <a href="https://ieeexplore.ieee.org/document/4308320">to reduce</a> the size of models. An interesting (but likely just coincidental) side note is that pruning biological neurons is important for <a href="https://www.nature.com/articles/d41586-018-02053-7">healthy development</a> [<a href="../biblio/#ivakhnenko1971">Iva71</a>; <a href="../biblio/#lecun1989">LDS89</a>; <a href="../biblio/#johnson2018-b">JS18</a>; <a href="../biblio/#walsh2013">Wal13</a>]. While pruning can reduce the number of operations using sparse operators, the primary benefit of pruning is to reduce the memory footprint via compression and alleviate memory bandwidth constraints. Note that AutoML, discussed in Section <a href="../ch10/#ch10.sec1">10.1</a>, can be used <a href="https://arxiv.org/abs/1802.03494">to learn</a> a compact topology [<a href="../biblio/#he2019-b">HLL+19</a>].</p>
<p>Doing some pruning usually has minimal impact on statistical performance, depending on the amount of pruning. In some cases, it may improve performance as pruning is a form of regularization. The ability to prune a model without affecting the statistical performance means the model is overparameterized. A hypothesis is that overparameterized models are needed to better explore the solution space and find a flatter minimum. After training the model, many of those parameters are no longer needed. A related hypothesis is the <a href="https://arxiv.org/abs/1803.03635">Lottery Ticket</a>: within a large model there exist smaller models (lottery winners) that have the same or better performance as the larger model [<a href="../biblio/#frankle2019">FC19</a>].</p>
<p>There are two types of model sparsity: structured and unstructured. Structured sparsity learning (<a href="https://arxiv.org/abs/1705.08922">SSL</a>) prunes an entire vector, array, or tensor. SSL reduces the overall number of parameters and computations; for instance, by removing a convolutional filter [<a href="../biblio/#mao2017">MHP+17</a>]. Various SSL techniques <a href="https://arxiv.org/abs/1608.03665">have been developed</a> [<a href="../biblio/#wen2016">WWW+16</a>; <a href="../biblio/#he2017">HGD+17</a>; <a href="../biblio/#zhuang2018">ZTZ+18</a>; <a href="../biblio/#zhang2019-b">ZDH19</a>; <a href="../biblio/#hua2019">HZS+19</a>; <a href="../biblio/#liu2019-d">LSZ+19</a>]. On CPUs and GPUs, structured sparsity (unlike unstructured sparsity) can reduce the number of operations.</p>
<p>Unstructured sparsity prunes values throughout a tensor without affecting the overall structure of the tensor, as shown in Figure <a href="../ch06/#fig:pruning">6.2</a>. The unstructured sparse pruned model can take advantage of BLAS functions in the Nvidia cuSPARSE and Intel oneMKL libraries when the sparsity is greater than <span class="arithmatex">\(90\%\)</span>. However, most sparse models have insufficient sparsity to significantly benefit from the sparse GEMM functions in these libraries. Alternatively, Google, DeepMind, and Stanford <a href="https://arxiv.org/abs/2006.10901">developed</a> techniques that achieve <span class="arithmatex">\(1.2\times-2.1\times\)</span> speedups and up to <span class="arithmatex">\(12.8\times\)</span> memory savings on Nvidia V100 GPUs without sacrificing accuracy on moderately sparse Transformer and MobileNet models [<a href="../biblio/#gale2020">GZY+20</a>].</p>
<p>Most production hardware are designed for dense matrix operations. Hardware with support for sparse operands is limited; one example is the <a href="https://ieeexplore.ieee.org/document/8662302">LNPU</a> device [<a href="../biblio/#lee2019">LLH+19</a>]. Nvidia A100 GPUs have support for <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf">fine-grained structure sparsity</a> with <span class="arithmatex">\(2\times\)</span> more compute.</p>
<div id="fig:pruning"></div>
<p><img alt="" src="../figures/ch06-02.png" />
<em>Figure 6.2:</em> Pruning a model by removing the weights (links) closed to zero.</p>
<p>The techniques for pruning are:</p>
<ul>
<li>
<p>train with larger weight decay to force more weights near zero;</p>
</li>
<li>
<p>fine-tune the pruned model (requires labeled data and backpropagation) [<a href="../biblio/#han2017">HPN+17</a>]; and</p>
</li>
<li>
<p>prune <a href="https://dl.acm.org/doi/10.1145/3295500.3356156">throughout</a> the training process: set the small weights to zero at each training iteration [<a href="../biblio/#lym2019">LCZ+19</a>].</p>
</li>
</ul>
<p>For power-constrained edge devices, <a href="https://arxiv.org/abs/1611.05128">energy aware pruning</a> may be required; that is, pruning the layers that consume the most energy [<a href="../biblio/#yang2017">YCS17</a>].</p>
<p>Pruned models are less robust to adversarial attacks. An adversarial attack occurs when the input to the NN is meticulously altered so that a human would not detect the change, but the model produces a very different output. For instance, the model <a href="https://research.google/pubs/pub42503/">predicts</a> with high confidence that the imperceivable altered image of a bus is an ostrich. Adversarially Trained Model Compression (<a href="https://arxiv.org/abs/1902.03538">ATMC</a>) and <a href="https://arxiv.org/abs/1904.08444">Defensive Quantization</a> are techniques that provide a balance between pruning and ensuring robustness to these attacks [<a href="../biblio/#gui2019">GWY+19</a>; <a href="../biblio/#lin2019-b">LGH19</a>].</p>
<p>Model compression reduces memory and bandwidth requirements at the expense of some additional computations for decompression. The time for these additional computations is often small relative to the time saved from the reduced bandwidth constraints. Therefore, compressing is usually advantageous. Note that an uncompressed unstructured sparse model and a dense model have the same memory footprint because storing an uncompressed zero-value requires the same number of bits as any other value. Compression algorithms, such as <a href="https://en.wikipedia.org/wiki/Huffman_coding">Huffman coding</a>, use <span class="arithmatex">\(1\)</span> bit to encode common values, such as the zero value. Another <a href="https://arxiv.org/abs/1510.00149">technique</a> is to cluster similar values and to quantize them to few bits, each group having a quantization factor [<a href="../biblio/#han2016">HKK16</a>].</p>
<p>Models with ReLU functions have sparse activations, and that sparsity grows for activations deeper into the model. During the forward propagation training stage, compressing the sparse activations before storing them (to use for the backpropagation stage) alleviates bandwidth bottlenecks.</p>
<div id="ch06.sec4"></div>
<h2 id="64-knowledge-distillation">6.4 Knowledge Distillation</h2>
<p><a href="https://arxiv.org/abs/1503.02531">Knowledge distillation</a> (KD) is a model compression technique that builds on the work by <a href="https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf">Bucila et al.</a> and is gaining rapid adoption [<a href="../biblio/#hinton2015">HVD15</a>; <a href="../biblio/#bucila2006">BCN06</a>]. KD reduces the memory and computational requirements for a particular task and does not require a decompression step. KD is related to transfer learning. The knowledge from a complex model (the teacher model) is distilled to a simpler model (the student model). The student model is trained using a smaller dataset and a larger LR than was used on the teacher model.</p>
<p>The trained teacher model generates softened probability outputs on the student's training dataset. The student model is trained to produce similar outputs as the teacher's softened probability output, as illustrated in Figure <a href="../ch06/#fig:kd">6.3</a>. A softened softmax, also called a softmax temperature, first divides the logits by some value <span class="arithmatex">\(T&gt;1\)</span> (called the temperature) before normalizing them. The output is a softened probability distribution that better captures class similarities. To illustrate, the softened output in digit classification for an input image with the number <span class="arithmatex">\(7\)</span> should have the highest value for <span class="arithmatex">\(7\)</span> and also a relatively high value for digits that look like <span class="arithmatex">\(7\)</span>, such as the handwritten digit <span class="arithmatex">\(1\)</span> and <span class="arithmatex">\(9\)</span>. The student model is trained to learn (1) the softened output using a softmax temperature and (2) the one-hot ground truth vector using the regular softmax. The softmax temperature also <a href="https://arxiv.org/abs/1909.11723">provides regularization</a> to the model [<a href="../biblio/#yuan2019">YTL+19</a>].</p>
<div id="fig:kd"></div>
<p><img alt="" src="../figures/ch06-03.png" />
<em>Figure 6.3:</em> Knowledge distillation. A large teacher model distills the knowledge to a smaller student model. The student model learns using both the regular softmax and a softened softmax from the teacher model. Based on [<a href="../biblio/#intel2018">Int18</a>].</p>
<p>The intuition behind KD is that the teacher model requires a more complex model to learn the relationships between the various classes. The ground truth one-hot vector does not encode class similarities and treats each class as entirely independent. The teacher model provides the class relations to the student model. Thus, the student model does not need to learn them from scratch and can use a simpler topology.</p>
<p>Extensions to this work are the <a href="https://arxiv.org/abs/1706.00384">deep mutual learning</a> (DML) where an ensemble of students collaboratively learn and teach others by sharing their softmax outputs, and the <a href="https://arxiv.org/abs/1902.03393">teacher assistant</a> (TA) to distill the knowledge from the larger-size teacher model to an intermediate-size TA model to a smaller-size student model [<a href="../biblio/#zhang2017-b">ZXH+17</a>; <a href="../biblio/#mirzadeh2019">MFL+19</a>].</p>
<p>In this chapter, we detailed the various numerical formats used in production and those in exploration by researchers as well as compression techniques to reduce the memory footprint of models. Using a smaller numerical representation can increase the number of operations per cycle, and reduce the memory, memory bandwidth, network bandwidth, and power consumption. However, it may also result in lower statistical performance, particularly for some <span class="arithmatex">\(\mathit{int8}\)</span> models. We discussed advances in quantization techniques to mitigate this accuracy loss and find Hessian-based analysis as a promising path to determine which layers are quantizable. Hardware support across numerical formats is one of the vital hardware design decisions. We recommend that training processors primarily support both <span class="arithmatex">\(bf16\)</span> and <span class="arithmatex">\(fp16\)</span> given the small die cost over supporting just one, and some <span class="arithmatex">\(fp32\)</span>, and inference processors primarily support <span class="arithmatex">\(fp16\)</span>, <span class="arithmatex">\(bf16\)</span> for compatibility with the training format, <span class="arithmatex">\(\mathit{int8}\)</span> and <span class="arithmatex">\(fp8\)</span> and some <span class="arithmatex">\(fp32\)</span>. In the next chapter, we review the basics of computer architecture, and discuss the various DL hardware designs.</p>

  <br>
    

    
    
      
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../ch07/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../ch07/" class="btn btn-xs btn-link">
        Chapter 7: Hardware
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../ch05/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../ch05/" class="btn btn-xs btn-link">
        Chapter 5: Distributed Training
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="container-fluid wm-page-content">
  <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a> using <a href="https://github.com/gristlabs/mkdocs-windmill">Windmill</a> theme by Grist Labs.</p>
</footer>

</body>
</html>